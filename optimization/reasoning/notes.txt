GPT-4 results:
{'bert_recall': 0.20514322817325592, 'bert_precision': 0.13337865471839905, 'bert_fmeasure': 0.166858971118927, 'rouge1_fmeasure': 0.22919429386519022, 'rouge2_fmeasure': 0.04057938245185126, 'rougeLsum_fmeasure': 0.19303369875006648, 'score_bart': -4.440537522236506, 'unieval_coherence': 0.9207369241227149, 'unieval_consistency': 0.838578721084772, 'unieval_fluency': 0.9388420135884724, 'unieval_relevance': 0.8072637249378665, 'unieval_overall': 0.8763553459334564}
scores zs generation -0.19270657963222929 scores conv generation 0.29802761661509675
scores zs ground truths -0.21340340092068624 scores conv ground truth 0.33408020436763763


LLaMA31-70B-Instruct results:
{'bert_recall': 0.17938821017742157, 'bert_precision': 0.20184679329395294, 'bert_fmeasure': 0.18723277747631073, 'rouge1_fmeasure': 0.22961935408784928, 'rouge2_fmeasure': 0.04489656940694876, 'rougeLsum_fmeasure': 0.1727085431265675, 'score_bart': -4.488025635480881, 'unieval_coherence': 0.9418705501512279, 'unieval_consistency': 0.9440489636814396, 'unieval_fluency': 0.9484615261694666, 'unieval_relevance': 0.7270671333979969, 'unieval_overall': 0.8903620433500329}
scores zs generation -0.24492899576822916 scores conv generation 0.2677627485245466
scores zs ground truths -0.21340340092068624 scores conv ground truth 0.33408020436763763
