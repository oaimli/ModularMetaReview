GPT-4 results:
{
    "bert_recall": -0.18761496245861053,
    "bert_precision": -0.2680947184562683,
    "bert_fmeasure": -0.2271289825439453,
    "rouge1_fmeasure": 0.15533982231357893,
    "rouge2_fmeasure": 0.024477579740875208,
    "rougeLsum_fmeasure": 0.12425707209125145,
    "score_bart": -5.043527301152547,
    "unieval_coherence": 0.9377660845416567,
    "unieval_consistency": 0.8557981200143085,
    "unieval_fluency": 0.9293241212297954,
    "unieval_relevance": 0.7591575478502509,
    "unieval_overall": 0.8705114684090026
}

scores zs ground truths -0.1559311803181966 scores conv ground truth 0.2973101298014323

scores zs generation -0.1236272690788148 scores conv generation 0.295846796532472


LLaMA3-70B-Instruct results:
