You are good at understanding documents with scientific review opinions.
Below is a scientific review for an academic manuscript, please extract fragments that make comments on Advancement of the research work.

Definition of Advancement:
Importance of the manuscript to discipline, significance of the contributions of the manuscript, and its potential impact to the field.


Example input review:

Thank you for the feedback and we appreciate that the reviewer finds that our MD method is suitable for NN quantization.

In this reply, we clarify the novelty and significance of our MD method compared to ProxQuant (PQ) [1]. Meanwhile, responses to other comments will be provided in a subsequent reply.

# Summary
- Our main contribution of the paper is to show that MD is a suitable framework for NN quantization and introduce a numerically stable MD algorithm for NN quantization with superior performance compared to directly comparable baselines.
- In this regard, we find the statement that our MD method is a “natural extension of PQ” (ie, proximal gradient method or in general gradient descent where the $L_2$ norm is used) to be misleading and the differences are as follows.

# MD vs PQ
- The main and important difference between our MD method and PQ is that MD allows gradient descent to be performed on a more general non-Euclidean space (refer to Sec. 2) whereas PQ does not. To see this, we first give the update equations of PQ and MD below:
- PQ: $\tilde{x}^{k+1} \gets x^k - \eta g^k$ where $x^k = \text{prox}(\tilde{x}^k)$ and $g^k = \nabla f(x)|_{x = x^k}$. Here, $x^k, \tilde{x}^k \in R$. (refer to Alg. 1 in [1])
- MD: $\tilde{x}^{k+1} \gets \tilde{x}^k - \eta g^k$ where $x^k = P(\tilde{x}^k)$ and $g^k = \nabla f(x)|_{x = x^k}$. Here, $x^k \in B$ and $\tilde{x}^k \in B^*$, where $B^*$ is the dual space of $B$. (refer to Eq. 22 in the paper)
- Notice that, PQ assumes the point $x^k$ and gradient $g^k$ are in the same space. Then only the formula $x^k - \eta g^k$ is valid. This would only be true for the Euclidean space [2]. However, MD allows gradient descent to be performed on a more general non-Euclidean space by first mapping a primal point $x^k\in B$ to a point $\tilde{x}^k \in B^*$ in the dual space via the mirror map. Such an ability is extremely beneficial in many problems (eg, simple constrained optimization) and it enabled theoretical and practical research on MD for the past three decades. Therefore, as mentioned in the paper (page 7) PQ is not based on MD.
- Furthermore, it is clear from our experiments that MD significantly outperforms PQ (up to 20% in some cases when fully-quantized, refer to Table 1) demonstrating the importance of optimizing on a non-Euclidean space based on our MD framework.
- Even though PQ hinted at the connection to the dual averaging version of MD and STE, it does not analyze the conditions on the projections under which corresponding valid mirror maps exist. This is important to show STE as a numerically stable implementation method for MD and such a link was previously lacking in the literature.
- We have added this discussion in the revised version of the paper (page 7) to improve clarity.

[1] Bai, Yu, Yu-Xiang Wang, and Edo Liberty. "Proxquant: Quantized neural networks via proximal operators." ICLR (2019).
[2] Bubeck, Sébastien. "Convex optimization: Algorithms and complexity." Foundations and Trends® in Machine Learning (2015).

Example output fragments in JSON Lines:

{"extracted_fragment": "# Summary\n- Our main contribution of the paper is to show that MD is a suitable framework for NN quantization and introduce a numerically stable MD algorithm for NN quantization with superior performance compared to directly comparable baselines.\n"}


Target input review:

{{input_document}}

Final extracted fragments (follow the format above in JSON Lines and if no resulted fragments just output "No fragments"):