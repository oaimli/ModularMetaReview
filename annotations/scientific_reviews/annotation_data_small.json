{
    "iaqgio-pOv": {
        "paper_id": "iclr_2022_iaqgio-pOv",
        "paper_title": "Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners",
        "paper_abstract": "Post-hoc explanations for black box models have been studied extensively in classification and regression settings. However, explanations for models that output similarity between two inputs have received comparatively lesser attention. In this paper, we provide model agnostic local explanations for similarity learners applicable to tabular and text data. We first propose a method that provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. We then propose analogies as a new form of explanation in machine learning. Here the goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model's prediction. The selection of analogies can optionally leverage feature attributions, thus connecting the two forms of explanation while still maintaining complementarity. We prove that our analogy objective function is submodular, making the search for good-quality analogies efficient. We apply the proposed approaches to explain similarities between sentences as predicted by a state-of-the-art sentence encoder, and between patients in a healthcare utilization application. Efficacy is measured through quantitative evaluations, a careful user study, and examples of explanations.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper presents two novel approaches to provide explanations for the similarity between two samples based on 1) the importance measure of individual features and 2) some of the other pairs of examples used as analogies.  The proposed approach to explain similarity prediction is a relatively less explored area, which makes the problem addressed and the proposed method unique. However, reviewers expressed concerns about evaluation methods and there were some concerns about the design choices that were not well motivated. The major issue is, as pointed out by the majority of the reviewers, the evaluation methods. Given the paper, reviews, and responses of the authors and the reviewers, it appears that there is certainly room for improvement for more convincing evaluation methodologies to convince a cross-section of machine learning researchers that the proposed approach advances the field. Overall, this is a good paper, but appears to be borderline to marginally below the threshold for the acceptance.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "5lgTObKOx-k",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Any more clarifications needed?",
                "comment": " Thanks to all reviewers for their critical reviews and also for engaging with us. We believe we have addressed most of your concerns. Please let us know if any more clarifications or explanations are required. Thank you.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DsgzDmXfRPV",
                "writer": "author",
                "reply_to": "Vzs9a1uxaxy",
                "title": "Thank you for raising your score and the interesting reference. More clarifications...",
                "comment": " 1) **Evaluating different explanation techniques:** We would like to stress that the methods used for quantitative evaluations are a *superset* of what we evaluated in the user study and not some arbitrary intersection as the reviewer initially thought (e.g. ProtoDash not present in quantitative evaluations, which it is as indicated by us). The reasons for including the additional techniques in the quantitative evaluations was to show that straightforward application of popular methods such as LIME and methods that are not obvious baselines because of the settings they are used in (search and query) such as JSLIME underperform in our setting even from a quantitative standpoint.  Not to mention that interpretation of these methods for our setup (i.e. explaining pairwise similarity for tabular and text data) is not straightforward. This is also the reason for not including them in the user study, where the most natural competitors are compared with. We believe this makes our findings generalizable.\n\n2) **Regarding Hummel et. al (2014):** First, thank you so much for sharing the interesting reference. We will refer to this in the final version. Although, the paper brings out the importance of analogies as explanations (which further motivates our work) it also states that analogies are *not sufficient* for a good explanation and additional aspects such as providing causal factors and integrating information from diverse sources is essential. This key observation in (Hummel et. al, 2014), we believe, further corroborates our results that feature based explanations using a powerful method such as FbFull could provide complementary information (viz. uncovering robust/causal factors) that leads to users performing reasonably well on them. We thus believe that our results do *not* violate the surmise made in the shared reference, but rather support it.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Vzs9a1uxaxy",
                "writer": "official_reviewer",
                "reply_to": "BOg0Uyf9rLc",
                "title": "Thank you",
                "comment": " * Evaluating different explanation techniques in human and systematic evaluation is not advisable. One can include fewer techniques in both studies but the conclusions should be generalizable. \n\n* See Hummel et. al (2014) with regards to my statement about analogies\n\nHummel, J. E., Licato, J., & Bringsjord, S. (2014). Analogy, explanation, and proof. Frontiers in human neuroscience, 8, 867.\n\nNote: I upgraded my score to 6 because of the inclusion of the ablation study and the analysis.  ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Nqv1cfm4OMs",
                "writer": "author",
                "reply_to": "i0x1QMIkYSI",
                "title": "Checking in...",
                "comment": " Thank you for being so responsive. We hope the above clarifications address your concerns. We would be happy to answer more questions in case you have any. Thanks again.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZCDYNZBlUZ3",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Official Review of Paper720 by Reviewer QDef",
                "comment": "The paper proposes a new technique for explaining models that predict the similarities of an input pair. The authors propose two forms of explanations for such models: feature and analogy-based. Feature-based explanations highlight the important features of a predicted similarity for an input pair. For the explained pair, analogy-based explanations provide a new input pair that has a similar relationship to one another. The proposed technique outperforms other similar techniques in human and functionally grounded empirical experiments.\n I have summarized the main review into the following pros and cons: \n\nPros: \n\n* The proposed technique is flexible as it can provide two forms of explanations: feature and analogy-based. Moreover, explanations in the form of analogies are intuitive for human users. \n* The study includes human and functionally grounded evaluation experiments to show the usefulness of the proposed explanation technique.\n\nCons:\n\n* Many important design choices behind the proposed method in sections 4.1 and 4.2 are not well motivated.\n* Some of the methods in functionally-grounded evaluation are not included in the human grounded evaluation experiments and vice versa. This makes it difficult to draw a general conclusion in favor of the proposed approach across both types of evaluation methods. \n Overall, I vote for rejecting the paper. Although the proposed technique performs well in both human and functionally grounded evaluation experiments, many important design choices are not well motivated. Overall, I believe that the study needs some further refinements before it can be accepted to ICLR 2022. \n\n\nI have divided my detailed feedback into two categories: \u201cmajor concerns\u201d and \u201cminor improvements\u201d. I am willing to improve my current score in case the authors can address points raised in the major concerns section.\n\nMajor Concern\n\n* What are the reasons that LIME and JSLIME are performing relatively similar in comparison to the proposed FBFull and FBDiag methods on MEPS dataset (Table 1)? Does that mean that the problem at hand can be solved with LIME and JSLIME formulation as well? If so, what are the benefits and limitations of the proposed explanation techniques in this paper? \n\n* How can the usefulness of the analogy-based explanations be argued for when the result of user studies show that users can get nearly similar accuracies using AbE or FBFull (Figure 3)? \n\n* Can authors provide explanations on the effect of each of five additive components in Equation 2? \n\n* What are the reasons for not performing the human and functionally grounded evaluations on the same set of techniques? In addition, how can this affect the generalized statements about which explanation techniques perform best across both evaluation experiments? (For example, LIME and JSLIME are missing in human studies in Figure 3 whereas PDash is missing in the functionally grounded evaluation in Table 1)\n\n* Why lambdas and alphas are not tuned per example and what is the effect of this on the fidelity of \u201clocal\u201d explanations (section 5.1 - AbE hyper-parameters)? \n\nMinor Improvement\n\n* Can authors provide a more detailed explanation for the problems that hinder the extension or use the work of [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021] for the problem at hand? \n\n* I see a potential problem in the additive definition of w_{x_i, y_i} (section 4.1). In the current definition, the loss cannot differentiate between these two cases:  perturbations x_i s are close to x and many y_i points are further away from y and vice versa. This can be problematic since removing and adding terms to the explained pair of instances changes the Mahalanobis distance asymmetrically (see Example 1-3 in Figure 2). Can authors confirm this and provide an analysis on the possible effect this can have on the quality of explanations?\n",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "BOg0Uyf9rLc",
                "writer": "author",
                "reply_to": "tujvNHHBhcZ",
                "title": "Thank you for responding. Hope you are feeling better. Further clarifications... [2/2]",
                "comment": " We see that using the full objective, we are able to obtain analogies that have all the three desired properties - high fidelity to black-box, meaningful analogousness, and sufficient diversity. However, as we turn off the black-box fidelity term, the chosen pairs seem to have no fidelity in terms of $\\delta_{BB}$ values, and this also qualitatively leads to choosing analogies that are quite dissimilar such as in the second pair, given that the input pair had high similarity (low $\\delta_{BB}(x,y)$). Without the analogy closeness term, the essential sense of analogousness in the input pair (people performing some activity) is lost in the second chosen pair. Finally without the diversity term, the second and third pairs chosen are the same, just with the order flipped. This example clearly demonstrates the usefulness of each term in the objective. We will include more such examples in the final paper.\n\n\n3) **Doing human evaluation of LIME analogous to how functional evaluation was done:** We do not agree that LIME could be used in the same manner in the human evaluation. The latter requires methods to actually produce coherent explanations (feature-based in the case of LIME). The problem with LIME, as we mentioned in point 4 above, is that there is no principled (i.e. non-controversial) way of doing so due to having two copies of the same features. On the other hand, we *are* able to include LIME in the functional evaluation because it only considers the outputs of LIME in assessing Generalized infidelity and Infidelity. Interpretation of the model is not evaluated there. \nWe thus do not agree that not including LIME in the user studies is a major flaw. In fact, we think that including these additional baselines in the functional evaluations (which we could have simply dropped) leads to a fairer positioning of our work.\n\n\n4) **Advantage over related work [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021]:** As mentioned above, all these works have been proposed for the image domain and require white-box access. The latter is not a minor hindrance for at least two reasons: i) In today's cloud-driven world, it is common for models to exist in different cloud platforms where explainability is provided as a service (Dhurandhar et. al, 2019). In such scenarios one does not have access to the model, other than being able to query it. ii) More importantly, requiring white-box access imposes certain constraints on the type of models that can be explained. For example, the cited methods need the model to be differentiable (to perform backpropagation), which restricts their usage to models such as neural networks. However, in real industrial applications, models are many times ensembles which may be a combination of business rules, trees, and neural networks making the overall model non-differentiable and hence these methods will not apply. However, our methods will. In fact, even with simpler ensembles such as random forests or boosted trees, one cannot use the cited methods, but our methods could readily be applied. We will clarify these points in the final version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tujvNHHBhcZ",
                "writer": "author",
                "reply_to": "tIEMF_46IMY",
                "title": "Thank you for responding. Hope you are feeling better. Further clarifications... [1/2]",
                "comment": " 1) **Analogies vs feature based explanations:** First, it would be great if you could provide a reference for the hypothesis you mentioned regarding analogies being preferred. Thanks in advance. With that said, we do not think there is a problem with such a hypothesis being violated in some cases, nor is there a problem in letting users gravitate toward different techniques based on their preference. Since explanations are ultimately consumed by human users who have different backgrounds and different requirements, one cannot expect a single type of explanation to best satisfy everyone.\nRegarding our user study specifically, as indicated in Hullermeier (MDAI 2020), analogies could be viewed as one more level of indirection over feature based explanations as the latter are a function of the feature embedding they utilize. In our user study, FbFull and AbE being close indicates that for the problem we studied, features, which essentially were words in the sentence pairs, had a critical role in determining a pair's similarity. However, certain latent factors (viz. context) also played a role which possibly led to the slightly improved performance of AbE. This type of setup seems natural, where an interpretable representation (viz. bag of words) for a feature based explanation method will carry a reasonable amount of information about the similarity but is not complete. The analogies might capture this additional information and it is still up to the user to be able to exploit this latent information. This may not be possible for or suited to all individuals. Our results (and comments in Appendix N) in the user study corroborate this argument, where some users were able to utilize this latent information, while others were not.\n\n2) **Ablations:** We performed ablations by removing each of the three terms in eq. 2 while obtaining analogous pairs. We report the results for one representative example here.\n\n*Original input pair:*\n(a) A group of men play soccer on the beach.\n(b): A group of boys are playing soccer on the beach.\nThe black-box distance, $\\delta_{BB}(x,y)$ for this input pair is $0.111$.\n\n*Analogies with the full objective:*\n1. (a) Two people in snowsuits laying in the snow making snow angels. (b) Two children lying in the snow making snow angels. $\\delta_{BB}(x, y) = 0.104$.\n2. (a) A sad man is jumping over a small stream to meet his companion on the other side. (b) A man is jumping over a stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n3. (a) A woman puts flour on a piece of meat. (b) A woman is putting flour onto some meat. $\\delta_{BB}(x, y) = 0.133$.\n\n*Analogies without the black-box fidelity term (1st term in eq. 2):*\n1. (a) A woman is bungee jumping. (b) A girl is bungee jumping. $\\delta_{BB}(x, y) = 0.045$.\n2. (a) The man is aiming a gun. (b) A boy is playing on a toy phone. $\\delta_{BB}(x, y) = 0.726$.\n3. (a) The religious people are enjoying the outdoors. (b) The group of people are enjoying the outdoors. $\\delta_{BB}(x, y) = 0.291$.\n\n*Analogies without the analogy closeness term (2nd term in eq. 2):*\n1. (a) A woman paints a picture of a large building which can be seen in the background. (b) A person paints a picture of a large building which can be seen in the background. $\\delta_{BB}(x, y) = 0.011$.\n2. (a) The company claims it's the largest single Apple VAR Xserve sale to date. (b) The company claimed it is the largest sale of Xserves by an Apple retailer. $\\delta_{BB}(x, y) = 0.363$.\n3. (a) A boy is at school taking a test. (b) The boy is taking a test at school. $\\delta_{BB}(x, y) = 0.111$.\n\n*Analogies without the diversity term (3rd term in eq. 2):*\n1. (a) Two people in snowsuits laying in the snow making snow angels. (b) Two children lying in the snow making snow angels. $\\delta_{BB}(x, y) = 0.104$.\n2. (a) A sad man is jumping over a small stream to meet his companion on the other side. (b) A man is jumping over a stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n3. (a) A man is jumping over a stream to meet his companion on the other side. (b) A sad man is jumping over a small stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tIEMF_46IMY",
                "writer": "official_reviewer",
                "reply_to": "BNWxWhH5SxE",
                "title": "Thank you for your thorough feedback",
                "comment": " I am sorry for the late reply. Unfortunately, I caught Covid and I was not in good health to engage with you until today.  \n\nI am very happy that the authors engaged with my review in the rebuttal phase. \n\n#2. One possible hypothesis in the literature is that users find analogies much more explainable because that is how humans provide explanations. Your study somewhat violates that hypothesis without a proper explanation. In addition, the result of choosing analogy or feature-based techniques lead to two completely different understanding of the explained scenario. It is not really convincing to me that you propose two techniques in the same paper and conclude that users can pick each one based on their own preference. \n\n#3.  It is difficult to see what terms in Equation 2 contribute to what exactly. There are many ways to tackle this, e.g. ablation studies where one removes one term at a time and sees how the quality of explanations change. Based on this, you can conclude if the added term is effective or not. I highly recommend you to perform such a study.\n\n#4. Am I right to believe that you still could use the same assumptions you had for LIME in the functional evaluations and include LIME in the human evaluation? I think this is a major flaw in your paper that different techniques are compared across the human and systematic evaluations. \n\n#6 It would have helped if you made us understand the challenges of using LIME or extending other cited related work for the problem in more details. Even after reading your paper as a reviewer, I still have a hard time positioning your work relative to other cited related work.  Remember that as a user of the explanation, I might not think having access to the trained model is a hinder if a technique from your cited related work can work well for the problem at hand. So in that case, should the user still prefer your approach? If so, why. \n\nFor me #",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "twQlSowjLDC",
                "writer": "author",
                "reply_to": "sT3nZIKjPzr",
                "title": "UBTb: Thank you for suggesting alternate designs, however our user study design is still valid...",
                "comment": " Thank you for engaging with us deeply and providing your thoughts.\n\n**Consensus/majorities have little effect by themselves:**\nFirst, for the cases where the three example-based methods (AbE, ProtoDash, DirSim) return a consensus (all three example pairs have the same label), participants agree with the consensus only 43.2\\% of the time. However, users agreed with AbE when there was consensus 71.2\\% of the times. There were 6 consensus questions overall and 2 for AbE out of the overall 30 questions. Since the methods are not known to participants, this indicates AbE explanations made more sense to the participants even in the case of label consensus. And as a reminder from our original response, if a participant simply accepted the majority label (at least 2 out of 3), their accuracy would be 40\\%, which is significantly less than not only AbE's performance (> 80 \\%) but also those of the other methods. We take these to be strong evidence that the participants were not overly swayed by consensus or majorities in the returned examples, and that they indeed used their judgement guided by the explanations, which was the goal of providing AbE explanations.\n\nThe SHAP example provided by the reviewer does not apply here since it is a *theoretical property* of SHAP explanations that summing up the attributions along with the baseline will result in the black-box model prediction. Our AbE explanations do not have such any such guarantee, both theoretically as well as when seen empirically.\n\n**Purpose of analogies:**\nWe feel that analogous examples do not need to share common words, content, or sentence structure. What is important is that they *point to latent factors* that may be responsible for the model's output. This is where analogy-based and exemplar-based explanations are different from other types of explanations. With this in mind, the examples that the reviewer provided for single-instance sentiment classification *could be good* analogies, since positive words (like \"best\" vs. \"impresses\") are present in both sentences and could have been picked up by the model (i.e. positive words are latent factors here). Moreover, while the reviewer may not see the value in such analogies, the user study participant comments in Appendix N suggest that many others do.\n\n> This knowledge is a characterization of the model prediction (i.e. for all sentence pairs with this same-type-ness present, does the model always produce a high similarity prediction?), and thus can be quantitively evaluated from only the input and output pairs of the model.\n\nWe disagree that this can be evaluated objectively as pairs that have the same-type-ness present are not annotated and it is then up to the subjective assessment of the user. More importantly, we are explaining a black-box model and stress that the black-box does not have to match human judgement, for example of same-type-ness, and hence a good explanation does not either. The potential mismatch is also why participants were given analogous pairs together with their predictions in asking them to guess what the model's prediction would be for an input pair. We believe that the provided analogies can help users subtly understand what the model \"thinks\" about the input pair locally.\n\nOverall, we agree that there could be other experimental designs which are worthy of investigation. However, as we have argued, our current design is meaningful for the stated goal (revealing latent factors that are responsible for the model prediction) and does *not* suffer from major loopholes that the reviewer indicated.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sT3nZIKjPzr",
                "writer": "official_reviewer",
                "reply_to": "Fk_pJTB7CLm",
                "title": "Response to authors",
                "comment": " I appreciate the authors for their detailed response. However, I am not convinced on the technical quality of the paper, for the reasons below. \n\nI think there is a misunderstanding by the authors on my point of the participant being convinced in the situation that all AbE instances to have the same label, which is also the same as the pair in the question. The authors mentioned that the majority vote accuracy is 40%, but presumably a concensus is not very frequent. What is the probability that the participants agree with the AbE instance labels when there is a concensus among them? In general, I still believe that giving the model explanation when asking the participant to predict the model output is fundamentally flawed, since a trivial post-hoc explainer can trivially produce an explanation that maximally correlate the prediction, but does not really reveal how the prediction is produced. Theoretical reasons are argued in the student-teacher distillation paper that referenced in the original review. As a \"natural\" and practical example, the SHAP explanation is defined to add up to a known baseline (typically 0.5 for binary classification). So the participant can deduce the model prediction by summing up all the attribution values, along with the baseline. \n\nI understand that the distance similarity is a factor in the explanation selection. However, without clear semantic relations, I am not sure how useful that would be. The equivalence for single instance classification (say sentiment classification from movie reviews) would be like this: in order to explain why the model makes a positive prediction on the sentence \"This movie is the best I have seen in many years\", the explanation being produced is another positively predicted sentence \"Director Smith impresses his audience for another time with his innovation\". Yes, the predictions are the same, but what else? They have different sentence structure, no common words, and are content-wise very different. So it is not clear how this explanation could be related to the sentence that it is intended to explain. Indeed, I find it hard to identify any semantic relations for some of the examples that the authors provided. \n\n> While it would be ideal if we could know with certainty whether the model recognizes concepts such as same-type-ness, we respectfully think that the reviewer's ask is somewhat unfair, given the state-of-the-art in the area and the complexity of the black-box sentence encoder in question. \n\n\"Know(ing) this with certainty\" does not require understanding the black-box complexity of the model. This knowledge is a characterization of the model prediction (i.e. for all sentence pairs with this same-type-ness present, does the model always produce a high similarity prediction?), and thus can be quantitively evaluated from only the input and output pairs of the model. As another way to evaluate this, a user study could be set up with a two-person cooperative game, where the first person receives raw model explanation, summarizes some high-level findings (such as the same-type-ness) and passes them to the second person, and the second person tries to simulate/predict the model prediction on a different (i.e. held-out) set of explanations. If the second person can achieve high performance, then that means the high-level findings, and thus the low-level explanations, are useful. Otherwise, it would be hard to argue for their usefulness in helping people to understand model. Note that if there is only one person but the evaluation is done on the held-out data, then this setting corresponds to the teacher-student evaluation setting referenced above. \n\nLast, as another way of automating such experiment, a ground-truth based experiment could be carried out, as also mentioned in the original review. Specifically, the authors could define some similarity rules; for example, similarity is correlated with two factors: the number of common words or the length difference of the sentence. Then if the model performs really well, then it must have picked up both cues. Then we can see if the extracted AbE instances share the same \"ground truth factor\" as the instance to be explained. For more details I would refer to the papers referenced in the original review. \n\nOverall, I believe that the experiments need to be made more \"bullet-proof\" and convincing in order to truly establish the usefulness of the proposed model. As such, I would maintain my assessment. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i0x1QMIkYSI",
                "writer": "author",
                "reply_to": "M5HpmdhBbfo",
                "title": "Further evidence of trustworthiness of user study results...",
                "comment": " In addition to the justification provided in bullet 3 above, to further verify our findings we evaluated what the accuracies would be considering *only* the questions in the survey where an input pair is seen by the subject for the first time (i.e. we ignored 2$^\\text{nd}$ and 3$^\\text{rd}$ repetition). In this case the number of questions per subject reduces to 10 (as there are 10 distinct input pairs), but there is for sure no risk of multiple interference (viz. order bias etc.) as every input pair has only a single explanation that the user has seen. We found the resultant (percentage) accuracies to be as follows: AbE-> 82.3, DirSim-> 55.2, Pdash-> 54.9, FbFull-> 77.8, FbDiag-> 56.1. These numbers are very similar to those seen in Figure 3 (left), where the advantage of our methods (AbE and FbFull) is maintained. This we believe further corroborates the fact that the results in Figure 3 based on our user study can be trusted.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "M5HpmdhBbfo",
                "writer": "author",
                "reply_to": "17vX_KiBbK",
                "title": "Thank you for your response. Further clarifications...",
                "comment": " **1. Equivalence of diagonal A with running LIME using the vector (x-y)**\n\nTo make LIME equivalent to diagonal $A$ (using our FbDiag method), the vector would have to be $(x - y)^2$, not $(x - y)$. More importantly, our diagonal $A$ is constrained to be non-negative so that the quadratic form is positive semidefinite, whereas LIME does not have this constraint. \n\n**2. Hyperparameter search**\n\nWe believe that preserving fidelity while ensuring the intuitiveness of analogies is a good scheme since ultimately we want the users to be able to consume these. Also by setting these hyperparameters only once per dataset, we ensure we do not overfit to any one input pair with the human effort being not unreasonable.\n\n**3. Current user study design**\n\nThanks for your comments. However, we would like to point out that our study also follows a standard experimental design, which can be trusted, and which makes more efficient use of the number of subjects.\n\nOur current design for the user study follows an \"alternating treatment design\" (https://www.sciencedirect.com/topics/psychology/alternating-treatments-design), where the treatments are alternated randomly even within a single subject. In our case, the treatments correspond to the different explanation methods, and the subjects correspond to the $41$ individuals who participated in the study. While the randomized treatment assignment that the reviewer pointed out is standard, in the presence of five different treatments, each treatment would be limited to $\\approx 8$ subjects, which limits statistical power. As we wrote in Sec. 5.2, Setup subsection, we wished to focus on people with certain backgrounds and so our number of subjects was smaller.  By using an alternating treatment design, we are able to make more efficient use of the number of subjects to understand the relative benefits of the explanation methods. Note that such designs are common in psychology (Barlow & Hayes, 1979). There is indeed a risk of \"multiple interference\" (viz. order effect/bias which probably is the reviewer's main concern) in these designs, but they can be mitigated by *randomizing* the order of the treatments as we have done. Given that our results clearly point to the superiority of our proposed explanation methods (i.e., the effect is large), we believe that these findings are thus generalizable and valid.\n\nFurthermore, well-known online survey platforms such as SurveyMonkey and QuestionPro also suggest randomization as a way to mitigate order bias (https://www.surveymonkey.com/curiosity/eliminate-order-bias-to-improve-your-survey-responses,  https://www.questionpro.com/blog/eliminate-order-bias-in-surveys-with-question-randomization/).\n\n- Barlow, D. H., & Hayes, S. C. (1979). Alternating treatments design: One strategy for comparing the effects of two treatments in a single subject. Journal of Applied Behavior Analysis, 12(2), 199-210.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "17vX_KiBbK",
                "writer": "official_reviewer",
                "reply_to": "gba_cupxoum",
                "title": "Response to author",
                "comment": " Thank you for your detailed response.\n\n1. I understand. But then if we run LIME with the vector (x-y), then LIME and having A diagonal would be equivalent or no?\n\n2. Thank you for the clarification about the hyperparameter search procedure. I think with 3 hyperparameters, it is quite hard to manually search over them especially without a clear objective. \n\n5. I don't believe randomizing the order is sufficient. It will remove the bias, but now the quantity we are evaluating is different. With the current study design, I don't think they can be trusted.\n\nIt is hard for me to increase my score given that the only valid systematic evaluation is the fidelity score which is not a stand-alone metric to evaluate an interpretability method. \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xpdig0VlWH0",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Checking in ...",
                "comment": " Thanks again to the reviewers for their valuable comments. We would be happy to provide any further clarifications before the discussion phase ends. Looking forward...",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ezA4pc9VIL",
                "writer": "author",
                "reply_to": "-w2o4Ox4mXw",
                "title": "Response to UBTb: Thank you for the references, but we believe our user study design is kosher",
                "comment": " 1) **User study/method gameable:** \n\nThank you for your comments and the references. We have carefully gone through them and we believe our setup is kosher. Regarding Trojan explanations, we do not believe that would have been possible in our case as it requires humans to be trained before doing the task so as to pick up on spurious associations to make a decision. In our study, we never provided the users with the true predictions of the input sentences that we tasked them to predict. Given this, there would presumably be no way in which they would be able to *cheat* the intended goal. In fact, similar setups have been used in multiple prior works where one is tasked to guess the prediction based on an explanation without training the user (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\nRegarding our method simply picking analogies that replicate the distance but not the semantics, yes it is possible, but that is why we conducted a user study wherein for users to guess the correct prediction of the input there would have to be some semantic connection in what we provided. Given that users were able to predict more accurately using our method, we believe that the analogies did have semantics that the users could exploit. In general, of course it is difficult to conclusively state that a method considers semantics and to outline the exact reasoning or thought process it respects, but that is precisely the reason user studies are conducted, where each user can reason for themselves based on the provided evidence. In aggregate, if the performance of explanations over such users is good, then the method should have some merit.\n\nRegarding your latest comment on showing the qualitative ranges (i.e. similar, somewhat similar, dissimilar) for the analogies: We understand your concern. We thus evaluated what would happen if a user simply picked the majority range amongst those provided (i.e. if two or more analogies are \"similar\" predict \"similar\" for the input pair). The resulting accuracy is 40\\%, which is significantly less than not only AbE's performance ($>80$\\%) but also those of the other methods. This strongly indicates that the users did reason about the explanations and that the effect of being swayed by the majority was minimal. Moreover, without providing the qualitative ranges corresponding to the analogies, there is no way for humans to calibrate their intuitive understanding to these ranges as they have to predict the black-box's output (mentioned in Section 5.2).\n\nLastly, note that our method has a tuning parameter $\\alpha$ in Eq. 3 which can control in a qualitative sense how much we want the analogies to respect certain feature attributions that may come out of a feature based explanation method or based on domain knowledge. This flexibility in design choice can further ensure that we return semantically meaningful analogies over and above the impact that *direction similarity* might have in this regard.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3eo6Dd8fA9",
                "writer": "author",
                "reply_to": "rS2uA3ypdgi",
                "title": "Response to  Un2j",
                "comment": " 1) **how can one compare doing LIME over the concatenated (x,z) to having A be diagonal?**\n\nGiven the two interpretable representations $\\bar{x}$ and $\\bar{y}$, LIME uses the concatenated vectors $(\\bar{x}, \\bar{y})$ as features to compute a linear explanation, whereas the proposed feature-based explanation with diagonal $A$ (FbDiag) is computed by taking the squared elementwise differences $(\\bar{x}_i- \\bar{y}_i)^2$ as features (See Section 4.1). They cannot be compared directly since the feature dimensions are not comparable ($(\\bar{x}, \\bar{y})$ has twice the feature dimension).\n\n2) **Optimization over $\\lambda_1$, $\\lambda_2$ is unclear, how can one systematically search over them to get intuitive analogies? Furthermore, why is $\\alpha$ set to 0 in all the experiments? What is the effect of  both quantitatively and qualitatively ?** \n\nFor setting $\\lambda_1$ and $\\lambda_2$, we first note that too high a value of $\\lambda_2$ may result in analogous pairs that do not have similarities close to the input. So we set it to a small value ($0.01$) in all cases and search around that range. Next, when we set $\\lambda_1$ we want to give somewhat equal priority to the first and second terms in equation 2. Hence, we search between $0.1$ and $1.0$. Again, we would like to have good fidelity between the input and the analogous pairs, and this guided our decision. Finally, we also consider how intuitive the analogies are for a randomly chosen set of inputs. At least for STS dataset, this consideration also guided our choice when setting these two hyperparameters. Such a human-in-the-loop process to tune explanations is also seen in prior works (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\n$\\alpha$ is set to $0$ because we wanted to evaluate independently the benefit of analogy-based explanations without any influence of feature-based explanations. Qualitatively, higher values of $\\alpha$ (along with high values of $\\lambda_1$) will mean that pairs with close predictions from feature-based explanations will be prioritized.\n\nOur feature-based explanations have high fidelity / low infidelity to the black-box (in-sample) (see Tables 2 and 3 rows named *Infidelity* and *Fidelity* respectively), so high values of $\\alpha$ could mean that the weights for first, second, and third terms in equation 2 are approximately $1+\\lambda_1 \\alpha$, $\\lambda_2$ and $\\lambda_3$, essentially providing higher weight to the black-box fidelity term.\n\n3) **Figure 2: why are the words on the x and y axis shuffled from their original order? Also this kind of visualization is a bit hard to parse, is there a better way to visualize the cross weights (off diagonal elements of A) ?** \n\nWe have provided updated Figures in Section O (Appendix) where we order the words in descending order of their contributions (top left to bottom right). We hope this representation is easier for the reviewer to parse, since you can focus more easily on the top left corner of each figure. We will replace the corresponding figure in the main paper with this one, if the reviewer is satisfied. We dabbled with other visualizations such as showing a list of univariate attributions followed by interaction terms, however, it got unwieldy fast and so we decided on the one we report now.\n\n4) **using google forms and non-paid participants raises questions on the effort:** \n\nThere are many published/established works that have performed unpaid user studies\n(Ramamurthy et al. 2020, Ribeiro et al. 2016, Lundberg et al. 2017, Kim et al. 2017). In fact, since the survey was done by folks willing to do it for free and with backgrounds in data science, engineering and business analytics, as mentioned in the paper (page 7, 3$^\\text{rd}$ paragraph), we expect to have received quality feedback (see Appendix N for the feedback), as opposed to people with unknown backgrounds taking the survey on platforms such as Amazon Turk where the main motive may be to earn the promised payment. An indication of the sincerity is the number of *optional* comments that people taking the survey decided to leave us (Appendix N).\n\n5) **showing participants the same example with different explanation methods:** \n\nThe design you proposed where different examples satisfying a condition are shown to the users is valid. However, our design was also valid since the order in which the explanations from the different methods appeared for different examples was randomized. This will break any systematic bias that could otherwise occur. For example, suppose A, B and C were three explanation methods used to show explanations (in that order) for a specific example. Then for a different example using the same methods, it is equally likely that the order could be any of the six possibilities such as B, C and A, or A, C and B, etc.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gba_cupxoum",
                "writer": "author",
                "reply_to": "3eo6Dd8fA9",
                "title": "Response to Un2j - continued",
                "comment": " 6) **no alignment between objective of user study (replicate BB model scores) and practical use cases:**\n\nThe setup of using explanations to guess the black-box (BB) model's prediction is an accepted procedure to evaluate explanations (Ramamurthy et al. 2020, Ribeiro et al. 2016, Luss et al. 2021, Dhurandhar et al. 2019). This tests *simulatability* of the model by a human (Lipton 2016, Doshi et al. 2017), where the model can be used for varied tasks. Incorporating a specific use case may bias the user study results towards that application, since some explanations may be innately more suited to an application than others. We thus adopted the accepted procedure to maintain generality.\n\n7) **why fidelity?** \n\nFidelity is one of the standard metrics (Ribeiro et al. 2016, Lunberg et al. 2017, Ramamurthy et al. 2020)\nused to evaluate quality of post hoc explanation models. The Black Box Invariance assumption (Sundarajan et al. 2017), which states that post hoc explanations should be the same if the black-box model behavior/outputs are the same, motivates the fidelity metric, as we would ideally want the post hoc explanation model to exactly mimic the behavior of the BB model.\n\n- Sundararajan, M., Taly, A. and Yan, Q., Axiomatic attribution for deep networks. ICML 2017.\n\n8) **implications of a low generalized infidelity score and a high score:** \n\nGeneralized infidelity (Table 1) measures how applicable an explanation for an example is to its neighboring examples. Hence, low values of generalized infidelity imply higher applicability, meaning that the explanation is robust and stable enough to explain not just the example in question but also examples near it which is what we would also intuitively expect in practice. High generalized infidelity means, the explanation for an example is not applicable to its neighboring examples.\n\n9) **(Ramamurthy et al., 2020) also relies on comparing the feature importance weights, , is it possible to do something here that is similar?** \n\nRamamurthy et al., 2020 perform comparisons of the global feature importances obtained from different local post-hoc explanation methods of classification models with the feature importances directly outputted by a black-box model. They perform this analysis only for black-box models that can output the global feature importance scores directly (such as random forests). However, in this paper, none of the black-box similarity models can directly output feature importances. The black-box models used in our case are, a Siamese neural network for the Iris dataset, the distance between leaf embeddings of random forests for MEPS dataset (note that this is different from a simple random forest regressor that can output feature importances), and cosine distance between embeddings of text obtained using a universal sentence encoder for the STS dataset. None of these are able to output feature importances directly, and hence the analysis mentioned in Ramamurthy et al., 2020 is not applicable in our case.\n\n10) **Your comment below on gameability:** Regarding your latest comment in response to Reviewer UBTb. Thank you for realizing that our explanations are doing something non-trivial. Their concern about showing the qualitative ranges (i.e. similar, somewhat similar, dissimilar) for the analogies, we believe had minimal effect. We evaluated what would happen if a user simply picked the majority range amongst those provided (i.e. if two or more analogies are `similar` predict `similar` for the input pair). The resulting accuracy is 40\\%, which is significantly less than not only AbE's performance ($>80$\\%) but also those of the other methods. This strongly indicates that the users did reason about the explanations and that the effect of being swayed by the majority was minimal. Moreover, without providing the qualitative ranges corresponding to the analogies, there is no way for humans to calibrate their intuitive understanding to these ranges as they have to predict the black-box's output (mentioned in Section 5.2). We hope this relieves your concern about this aspect.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "m6eZBZpzI5e",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Thanks to reviewers. All clarifications included in responses and paper updated.",
                "comment": " We thank the reviewers for their diligent reviewing and comments. We have now provided a complete set of responses to their comments including those on user study. The latest version of our paper has changes updated in blue.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "48uwGO3g0k",
                "writer": "author",
                "reply_to": "ezA4pc9VIL",
                "title": "Continued response to UBTb",
                "comment": " 2) **Qualitative examples:**\n\n> For the qualitative explanation, and AbE in particular, none of the three examples in Sec. 5.1 are convincing, and they feel more like post-hoc over-explanation on the authors' part based on the analogy pairs produced.\n\n> [Example 1:] Yes, but does this show that the model is recognizing the same-type-ness of the entity (musical instrument or sport) when making the similarity prediction?\n\nWhile it would be ideal if we could know with certainty whether the model recognizes concepts such as same-type-ness, we respectfully think that the reviewer's ask is somewhat unfair, given the state-of-the-art in the area and the complexity of the *black-box* sentence encoder in question. For analogies in particular, the idea is to provide a human expert with example pairs satisfying certain mathematical properties in equation 2 (closeness in degree of similarity, within-pair relationship, diversity) and then allow the human to interpret the results and decide whether the example pairs are useful. As we wrote in the first paragraph of Section 4.2, AbE does *require human judgement.* Our discussion in Section 5.1 attempts to portray what a real user might observe. Exemplar-based explanations (Gurumoorthy et al. 2019, Kim et al. 2016) have the same requirement in the unsupervised and standard supervised learning (prediction) settings: the algorithm provides the user with mathematically similar exemplars that the user then interprets. Even for post hoc explanations that approximate the black-box model with a simpler model, we may understand the mechanism of the simpler model and can ensure that it is reasonably close to the black-box model in an input-output sense, but we cannot be certain that the mechanism of the black-box model is similar to what we understand.\n\nIn fact, \"Black-box Invariance\" is stated as a desirable property for a model agnostic black-box explanation method in seminal works on XAI (Sundararajan et al. 2017). The property states that the explanation should be completely determined by the input-output behavior of the model as no further information is available about the model in such settings. In other words, if two black-box models produce the same outputs for the corresponding inputs, a ``good'' explanation method will produce the same explanations. As one can see it is possible that the mechanisms of the two black-box models to arrive at the same output can be different, nevertheless in such a setting there is no way to distinguish the models. Our proposed approaches are consistent with this property.\n\n- Sundararajan, M., Taly, A. and Yan, Q., Axiomatic attribution for deep networks. ICML 2017.\n\n> This statement is not about explanation, but rather merely about model prediction.\n\nSince closeness in model output is one criterion in equation 2 (the first term), we feel that it is relevant to comment on it.\n\n> I could not understand what a \"dolphin scheme\" is. Is it a particular way for economic fraud/exploitation (like \"pyramid scheme\")? As a result, I could not understand the author-provided explanations for this example, and I do not think it is a good opening example for the same reason.\n\nTo the best of our knowledge a \"dolphin scheme\" is also a type of fraud (now mentioned in the introduction). Nevertheless, we do not think that it is crucial to understanding why the model believes that the two sentences are similar, as the function of the phrase is simply to provide more context in one of the sentences.\n\n3) **\"Both sentences express the same idea (second half faster than first half) but in different ways, similar to the input pair.\" -- This is a very loose assertion. In fact, if both sentences truly express the same idea, I would expect the similarity to be much higher...:**\n\nIt is important to note that we are explaining a black-box model whose behavior may not align in all cases with human intuition. Hence, in the example we are explaining why we think it is a good analogy given the black-box's predictions and are not justifying the black-box models behavior in absolute terms or relative to what a human might expect. Keeping this in mind the reason we have stated for why the analogy is similar to the input pair is in our opinion valid.\n\n4) **More appropriate citations:**\n\nThank you for suggesting Simonyan et al. (2013). We have now cited it at the indicated location and have used `citet` as appropriate.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i25GBjKrMmD",
                "writer": "author",
                "reply_to": "kd-5M6neydC",
                "title": "Response to  F17R",
                "comment": " 1) **Extension to vision tasks:**\n\nCurrently we show results with both tabular and text data. However, this is an interesting idea for future work, which requires some modifications and extensions to our approach. This is because our feature based explanations approach requires computation of $\\bar{x}-\\bar{y}$, $\\bar{x}$ and $\\bar{y}$ are the interpretable representations of the original data $x$ and $y$. For images, this means that we have to set $\\bar{x} = x$ and $\\bar{y} = y$ (use the original pixel representation as the interpretable representation) or use some joint super-pixel segmentation, as having different superpixel segmentations for each image will not directly apply. In general, the current instantiation of the method is naturally suitable for data where all examples can be encoded using the same feature vocabulary.\n\n2) **MEPS analogies might be more relatable:**\n\nAppendix L discusses analogies from the MEPS dataset. We included more examples from the STS dataset as we thought it is better known and showing the applicability of our method for unstructured data was a stronger testament for it. If you believe that some of the examples in the appendix might bring out our message more strongly we would be happy to move them to the main paper.\n\n3) **It took me a while to understand Figure 1 when it is first mentioned. Consider using shorter samples or/and expanding the description in the introduction:** \n\nWe have now added a sentence in the introduction explaining why the analogy makes sense based on your suggestion.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WKIDZTnpGhv",
                "writer": "author",
                "reply_to": "BNWxWhH5SxE",
                "title": "Response to QDef - continued",
                "comment": " 7) **I see a potential problem in the additive definition of $w_{x_i, y_i}$ (section 4.1). In the current definition, the loss cannot differentiate between these two cases: perturbations $x_i$ s are close to x and many $y_i$ points are further away from y and vice versa:** \n\nIn the feature-based part of this work, our focus was on extending the idea of LIME to similarity functions, which led to our proposing a different proxy model, namely Mahalanobis distance. Weighting of neighbors for LIME-like methods is in general an open question and mostly orthogonal to our focus. We thus kept the method of weighing neighbors close to that of LIME so that we could more fairly compare our main innovation FBFull to other derivatives of LIME such as concatenated LIME and JSLIME. There is no obstacle to combining the $x$ and $y$ components of $w_{x_i, y_i}$ in a different manner if that improves fidelity in a specific application.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BNWxWhH5SxE",
                "writer": "author",
                "reply_to": "ZCDYNZBlUZ3",
                "title": "Response to QDef",
                "comment": " 1) **What are the reasons that LIME and JSLIME are performing relatively similar in comparison to the proposed FBFull and FBDiag methods on MEPS dataset (Table 1)? what are the benefits and limitations of the proposed explanation techniques?**\n\nWe note that for MEPS, our FbDiag method performs better than LIME and JSLIME in Table 1. MEPS is of course only one dataset and for the other datasets, our methods (FbFull, FbDiag) have wider gaps with the baselines. The smaller gap for MEPS could be due to its features being largely categorical and sparse.\n\n2) **AbE and FBFull (Figure 3) have similar accuracies so why prefer analogies?** \n\nAlthough AbE achieves slightly higher accuracy than FBFull in the user study, the difference is small, and so our recommendation would be to use one or the other based on individual preference. Comments from the user study (see Appendix N) show that some people preferred AbE while others preferred FBFull. Also, since both AbE and FbFull are our contributions, we do not believe that the similar accuracies diminish the significance of either.\n\n3) **Can authors provide explanations on the effect of each of five additive components in Equation 2?**\n\nThe components in equation 2 are already discussed in the paragraph below it and below equations 3, 4. We would be happy to further explain any specific aspect of this that remains unclear to the reviewer.\n\n4) **What are the reasons for not performing the human and functionally grounded evaluations on the same set of techniques?**\n\nFirst, regarding PDash, quantitative results are provided in Figures 4 and 5.\n\nJSLIME: One reason why we did not include JSLIME in the user study is that it did not standout as a natural baseline in our setup as it was proposed primarily for images in the context where a query image is provided to a search engine in order to retrieve similar images (not pairs of inputs provided to a BB as in our case). Moreover, the work is contemporaneous (not published yet in a peer-reviewed venue) and we became aware of it only recently. As a consequence, their code is also not publicly available so we had to re-implement their method based on their description. We thus included this additional baseline for the quantitative evaluation as we thought the comparison would be informative to readers more in there.\n\nLIME: We included LIME (applied to the concatenation of $(\\bar{x}, \\bar{y})$) in the quantitative studies because we wanted to show the performance of this simplest adaptation of LIME to our setting, as a basic baseline. The problem with it is that there is no principled way of deriving the importance of a feature as there are two copies of each feature that may be assigned drastically different coefficients, possibly with the same sign. Merely summing the two coefficients does not seem like the right thing to do as the similarity may be governed by some function of their difference. FBDiag can be seen as a version of LIME that does not have this problem with interpretation, and it is included in the user study.\n\n5) **Why lambdas and alphas are not tuned per example and what is the effect of this on the fidelity of \u201clocal\u201d explanations (section 5.1 - AbE hyper-parameters)?**\n\nWe honestly do not think tuning $\\lambda$'s and $\\alpha$ per example is advisable. First, it would be computationally burdensome, introducing significant latency in the explanation of every example. Second, it may result in ``overfitting'' to the example in the sense that fidelity for the example is high but *generalized* fidelity for nearby examples is poor. This is why tuning was done only once per dataset as is also done in prior works (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\n6) **Can authors provide a more detailed explanation for the problems that hinder the extension or use the work of [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021] for the problem at hand?**\n\nZheng et al., 2020, Plummer et al., 2020 and Zhu et al., 2021 explain image similarity models, and require *white box* access to the internals of the model, e.g., gradients, activations, or feature embeddings. In contrast, our method is designed for black-box models, where all we need is a model that can take in two inputs and output a similarity/distance score. Our method is also geared towards tabular and text data. Hence the problem setup and requirements between our work and the works mentioned by the reviewer are entirely different. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZkK-jj65Lao",
                "writer": "author",
                "reply_to": "9m8sSlCtpmi",
                "title": "Response to Lu2q",
                "comment": " Thanks to the reviewer for their comments. We have provided detailed responses below.\n\n1) **Applicability to Image Datasets:** experiments on them?\n\nCurrently we show results with both tabular and text data. However, this is an interesting idea for future work, which requires some modifications and extensions to our approach. This is because our feature based explanations approach requires computation of $\\bar{x}-\\bar{y}$, $\\bar{x}$ and $\\bar{y}$ are the interpretable representations of the original data $x$ and $y$. For images, this means that we have to set $\\bar{x} = x$ and $\\bar{y} = y$ (use the original pixel representation as the interpretable representation) or use some joint super-pixel segmentation, as having different superpixel segmentations for each image will not directly apply. In general, the current instantiation of the method is naturally suitable for data where all examples can be encoded using the same feature vocabulary.\n\n2) **Attributes used for explanations:**\n\nIn Section G (Appendix), we mention what the interpretable representations for the different datasets are: *The interpretable representation ($\\bar{x}, \\bar{y})$ is the same as the original features in Iris; for MEPS it involves dummy coding the categorical features, and with STS, we create a vectorized binary representation indicating just the presence or absence of words in the pair of sentences considered.*\n\n3) **Perturbations in the text domain:**\n\nIn  Section G (Appendix), we discuss how the perturbations are performed for the text data: *For STS, the perturbations were generated following the LIME codebase by randomly removing words from sentences.*\n\n4) **Compare results with the SHAP[3], LOO[4], RISE, and Occlusion-based method for input perturbation and U-CAM method for logit perturbation:** \n\nWe have compared our feature-based local explanation approaches (FbFull, FbDiag) with the most relevant competing methods adopting their perturbation schemes for a fair comparison. These are LIME and JSLIME as discussed in the paper. SHAP does not use any input perturbations. We also perform erasure of words when computing perturbations for text (one of the methods suggested in [4]) and the U-CAM method (which we think is Patro et al., U-CAM: Visual Explanation using Uncertainty based Class Activation Maps) does not seem to suggest any input perturbations that we can incorporate. In addition, our perturbation method incorporates random-masking-like strategy used in the RISE method (Petsiuk et al., RISE: Randomized Input Sampling for Explanation of Black-box Models). The reviewer has not provided any references to the Occlusion-based method, but we guess it also uses occlusion of portions of data to create perturbations, and we do this as well for perturbations with text data.\n\n5) **Difference between analogies and paraphrasing:**\n\nAnalogies for a pair of sentences are other pairs chosen from the same dataset. Paraphrasing is more about altering a given sentence so that it still implies the same thing. We choose our analogous pairs from a given dataset itself, whereas paraphrasing implies we need to actually modify the input sentence pair, which may result in data samples lying outside the dataset.\n\n6) **Similarity between inputs measured at word/phrase/sentence level?** \n\nFor the STS (text) dataset, the blackbox model uses sentence embeddings from the well-known [universal sentence encoder](https://tfhub.dev/google/universal-sentence-encoder/4). The explanation model uses a bag-of-words representation with $0$ indicating the absence of a word in a vocabulary and $1$ indicating its presence.\n\n7) **Provision of algorithm/pseudo code** \n\nWe have updated Section P (Appendix) of the paper with pseudo codes for the methods developed in this work.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gatLX34ylrY",
                "writer": "official_reviewer",
                "reply_to": "-w2o4Ox4mXw",
                "title": "reviewer discussion",
                "comment": " I agree with reviewer UBTb that it is very easy to game the user study by having the explanation leak the model prediction, but the approach is doing something non-trivial and subjective questions about how participants rate the explanations can reveal whether the study is gamed. \n\nHowever, the authors could have avoided this problem while still showing explanations at test time. What they could have done is split the experiment into two phases: one where they show explanations at test time to \"train\" the participants about the model and one without the explanations to test the participants.\n\nThe big issue though about the user study (in my review) is showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-w2o4Ox4mXw",
                "writer": "official_reviewer",
                "reply_to": "Fk_pJTB7CLm",
                "title": "Update on my review",
                "comment": " I just took a closer look at the user study form in the supplementary material. In Fig. 9(a) on Page 22, the issue with the human study is actually more severe than I was expecting: the model prediction on the analogous pairs (i.e. AbE) are given. In the particular case shown in this figure, even if the explanation is completely useless or unfathomable, the user would still very likely to select \"Similar\" simply because all the AbE pairs have the same prediction of \"similar\". \n\nThis shows how easy a completely non-informative explainer could game the system (again see https://arxiv.org/abs/2012.00893 and especially the related work section): just select three random pairs for which the model prediction is the same. It seems highly likely that the participants will be swayed to select the same label despite not being able to understand the explanation in any meaningful way. The essence to prevent such loophole, as the above paper argues, is that the explanation has to be *not* present when the user is asked \n to simulate the model's behavior. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9m8sSlCtpmi",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Official Review of Paper720 by Reviewer Lu2q",
                "comment": "The author addresses the problem of post hoc explanation for the black box model. In this paper, the author discusses the task of explanation for two inputs, and the model provides a similarity score as output. The author proposes a model agonistic local explanation method for tabular and structure data. In the proposed method, the author uses feature attributes to explain the similarity between two inputs. Finally, the author proposed an analogy-based explanation to select diverse analogous pairs of examples for the same similarity. The author claims that using the proposed method can explain state-of-the-art models in healthcare utilization applications. Strengths:\nThe main reason to accept this paper is empirical results, showing performance on the various methods. The author has done plenty of case studies to verify the explanation and with many examples of the proposed explanation method. \n\nWeaknesses: \nGeneralizability of the proposed method. The author shows results on language tasks.  Can it be applied to other tasks? If yes, the author should show results on vision tasks and shows and compare results with state-of-the-art methods.\n\nTo find better similar and contrasting examples in the vision domain, people use exemplar [1,2] theory to find supporting and opposing examples.\n\nWhat sort of feature attribute did the author consider for the explanation? Do you have a section discussing feature attributes?\n\n\u201cset of perturbations (x _i, y_ i ) in the neighborhood,\u201d what kinds of perturbation is used in the text domain?\n\nThe author should compare results with the SHAPE[3], LOO[4], RISE, and Occlusion-based method for input perturbation and U-CAM method for logit perturbation.\n\nHow is it(analogies) different from paraphrasing a sentence? The author could motivate the paper on why analogies help to improve the explanation.\n\n\nHow did the author measure similarity between two inputs (word or phrase or sentence level)? Do you have an analysis of this?\n\nHowever, the paper misses one of the core aspects of machine learning practice: readability and reproducibility of results. What are the various critical components in the proposed method? The author should provide an algorithm or pseudocode to reproduce the results missing in this paper.\n\n\nRef:\n1. J\u00e4kel, Frank, Bernhard Sch\u00f6lkopf, and Felix A. Wichmann. \"Generalization and similarity in exemplar models of categorization: Insights from machine learning.\" Psychonomic Bulletin & Review 15, no. 2 (2008): 256-271.\n\n2. Patro, Badri, and Vinay P. Namboodiri. \"Differential attention for visual question answering.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7680-7688. 2018.\n\n3. Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems, 4765\u20134774.\n\n4. Li, J.; Monroe, W.; and Jurafsky, D. 2016. Understanding neural networks through representation erasure. arXivpreprint arXiv:1612.08220.\n\n I have worked in this field and published related to this work.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "rS2uA3ypdgi",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Official Review of Paper720 by Reviewer Un2j",
                "comment": "Goal: provide local explanations for black box (BB) models that assign similarity scores to two input examples. \n\nApproach: Two explanations generated: 1) feature attribution and 2) similar pair of examples that serve as analogies. \n\nApproach for 1) is:\n\nApproximate the BB model on the instance as if it was a quadratic model of the pair of inputs and learn the weighting matrix A by sampling pairs of points around the input and solving the resulting SDP for the matrix A. The weights of A provide a way to assign value to each element of the inputs. \n\nApproach for 2 is:\n\nObtain K different pairs of examples that serve as analogies by solving a subset selection problem from the data that balances three terms:\n\n1) the analogy pair must have similar score to the query according to the BB model\n\n2) the analogy pair and query should have similar features highlighted (weighted by a HP $\\lambda_1$\n\n3) the K pairs should be diverse\n\nEvaluation: The authors show three different kind of evaluations\n\nQualitative examples: on text data from STS, the authors show three examples with the results of their method\n\nA user study on the STS dataset where participants have to predict if the BB predicted the sentences to be similar given the two kinds of explanations. They show that their method outperforms the baselines\n\nQuantitative results on 3 datasets: STS, UCI Iris and MEPS where authors show that their method outperforms LIME and other baselines in terms of a metric called \u201cgeneralized infidelity\u201d \n Strengths:\n\n- Novel formalization of objective function for finding analogies and feature attribution for BB similarity learners\n\n- Diverse evaluation of approach using both objective metrics and a user study\n\nWeaknesses/questions:\n\n- On objective 1: how can one compare doing LIME over the concatenated (x,z) to having A be diagonal?\n\n- On objective 2: Optimization over $\\lambda_1, \\lambda_2$ is unclear, how can one systematically search over them to get intuitive analogies? Furthermore, why is $\\alpha$ set to 0 in all the experiments? What is the effect of $\\alpha$ both quantitatively and qualitatively ?\n\n- Figure 2: why are the words on the x and y axis shuffled from their original order? Also this kind of visualization is a bit hard to parse, is there a better way to visualize the cross weights (off diagonal elements of A) ?\n\n- Issues with user study: 1) using google forms and non-paid participants raises questions on the effort each put into performing the user study.  2) showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions. 3) no alignment between objective of user study (replicate BB model scores) and practical use cases: supposedly the explanations are to check if the BB is correct or not, why wasn\u2019t that the use case for the user study? I expect it\u2019s because humans are perfect at judging similarity, then it might have been more interesting to introduce an end task where judging similarity is used. \n\n -  I am not sure why is fidelity the \u201cmetric\u201d to compare things across for judging similarity.  Furthermore, it would have been helpful to understand the implications of a low generalized infidelity score and a high score. (Ramamurthy et al., 2020) also relies on comparing the feature importance weights, is it possible to do something here that is similar? \n The paper presents a novel approach for obtaining explanations from a black box measure. The method appears sound, however, the evaluation is lacking in certain aspects. The user study has some flaws and the quantitative experiments rely on a single metric. My recommendation is a borderline reject that can be improved if authors can better argue their evaluation approach.\n",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "kd-5M6neydC",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Official Review of Paper720 by Reviewer F17R",
                "comment": "This paper introduces a novel form of explanations for similarity-based models. The authors present two methods to generate explanations, called Feature-based similarity explanations and analogy-based similarity explanations, where the latter one is a novel type of explanations explicitly developed to explain similarity learners. However, the authors state that both can be used simultaneously, i.e. the latter can use the output of the first.\nThe authors conduct a user study as well as a quantitative evaluation of the proposed methods with a comparison to previous approaches. Since the proposed setting (analogies) is novel, previous approaches compared with needed adaptations to fit the setting.  The paper\u2019s proposed explanation form seems to be very interesting. After the problem and the explanations methods are well motivated and introduced, the authors first illustrate them on selected examples. This shows nicely the methods\u2019 purpose. However, the STS dataset's task and the selected examples do not well support the quality of the generated explanations and the benefit of analogies-based explanations. I could imagine that the MEP dataset would be more relatable.\n\nAfter this illustration, the methods are extensively evaluated with a designed user study and quantitative evaluations both taking (adapted) previous methods into account. The results demonstrate the purpose and the benefits of the proposed methods. Summarized, the approaches seem to be very interesting, especially since similarity learners became more and more popular in recent years, even beyond the text and tabular data domain. Applying, evaluating and extending these methods for e.g. the vision domain seems to be interesting.\nTherefore, I'm tending to accept the present work.\n\nMinor comments:\n\nIt took me a while to understand Figure 1 when it is first mentioned. Consider using shorter samples or/and expanding the description in the introduction. After reading section 5.1 it became more clear.\n I'm tending to accept this paper as it is well written and provides interesting and novel approaches to explain similarity-based methods.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "Fk_pJTB7CLm",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Official Review of Paper720 by Reviewer UBTb",
                "comment": "This paper studies the problem of explanation for similarity prediction models. Given a pair of inputs (x1, x2), the model to be explained assigns a distance (or similarity) score. The task is then to explain the model prediction on individual inputs. Two methods are proposed. First, a feature-attribution style explanation is computed by learning a local distance approximation, similar to the LIME objective. Second, an analogy-based-explanation is used, in which a set of existing pairs of data are selected, and the pairs are encouraged to both be semantically diverse and share similar model reasoning process at the same time. In the experiments, qualitative and quantitative results are presented. Qualitative results are delivered mainly for the STS dataset. Quantitative results are provided via a user study demonstrating that the users can better predict the model prediction when given the proposed explanation compared to baselines, and an automatic evaluation showing that a global version of proposed method perform better than existing approaches on the fidelity metric.  ## Strengths: \n\nThis paper studies an under-studied problem of explanation for similarity models. Due to the particular natures of the similarity prediction task, methods that do not focus on interaction effects (i.e. pretty much all local explainers for the classification setting) understandably could not perform well. The authors solved this problem by the use of a learned local distance matrix, in which interaction effects are clearly shown. In addition, the proposed method of analogy-based-explanation seems novel. The explicit treatment of diversity sets it apart from other explanation methods that also use whole data point for explanation, such as counterfactuals. \n\n## Weaknesses: \n\nDespite the strengths, I do have serious concerns about the experimental evaluation, which fails to convince me of the quality of the explanation. \n\n### Qualitative analysis\n\nFor the qualitative explanation, and AbE in particular, none of the three examples in Sec. 5.1 are convincing, and they feel more like post-hoc over-explanation on the auhors' part based on the analogy pairs produced. As a concrete example, consider the (author-provided) explanation on the analogy-based explanation provided in example 1: \n\n\"The first analogy is very similar except that hackysack is a sport rather than a musical instrument.\" -- Yes, but does this show that the model is recognizing the same-type-ness of the entity (musical instrument or sport) when making the similarity prediction? \n\n\"The sentences in the second pair are more similar than the input pair as reflected in the corresponding BB distance.\" -- This statement is not about explanation, but rather merely about model prediction. \n\n\"The third analogy is less related (both sentences are about cricket player selection) with a larger BB distance.\" -- Again, this statement is only about model prediction. \n\nFor example 2, despite some Internet search, I could not understand what a \"dolphin scheme\" is. Is it a particular way for economic fraud/exploitation (like \"pyramid scheme\")? As a result, I could not understand the author-provided explanations for this example, and I do not think it is a good opening example for the same reason. \n\nFor example 3, \"Both sentences express the same idea (second half faster than first half) but in different ways, similar to the input pair.\" -- This is a very loose assertion. In fact, if both sentences truly express the same idea, I would expect the similarity to be much higher (i.e. distance much smaller), but this pair is actually the most dissimilar pair among the three. \n\n### Quantitative analysis\n\nI have serious concerns about the simulatability user study in the paper. In summary, this design is easily game-able. Since the users have access to the explanation at \"test time\", a simple AbE explanation for achieving such correct prediction would simply be to produce pairs with similar predicted distances, regardless of any similarity in the reasoning process. To make it even worse, if the users could be \"trained\" for a bit, a \"Trojan explanation\" could easily lead to very high user performance, without the users understanding the model at all. For more details about the \"game-ability\" of this approach and the \"Trojan explanation\" definition, see https://arxiv.org/abs/2012.00893 and https://arxiv.org/pdf/2006.01067.pdf. The authors are suggested to consult an earlier proposal for user study design https://arxiv.org/abs/2006.14779, which (in my opinion) avoids this loophole. \n\nIn addition, a synthetic task with known groundtruths could objectively evaluate various properties of the proposed methods, such as whether the highlighted words are indeed important, or whether the analogical pairs also use the same reasoning pattern. Some ideas are discussed in https://arxiv.org/abs/2104.14403 and https://arxiv.org/abs/2105.06506. \n\nMinor: \n\nThe authors could use \\citet in places such as \"Smith (2019) proposed\", rather than the current \"(Smith, 2019) proposed\". \n\nFor gradient-based explanation, the authors cite GradCAM, but I view it more as an adaptation of the original CAM to non-fully-convolutional architectures, and GradCAM are fundamentally about visualizing maximally activating input regions for certain convolution layers/filters. Instead, I would recommend the original Gradient saliency paper by Simonyan et al. (2013) or its SmoothGrad/IntegratedGradient successor.  Unfortunately, I do not believe that this paper meets the standard for publication. While I like the proposed theory, I am really unconvinced by the experimental evaluations. In fact, the qualitative AbE examples raise more questions than they answer, and make me doubtful that the method is really working as intended. A more careful experimental investigation, perhaps with some revision to the theoretical approach based on the problems found, would make this paper a much better one. ",
                "rating": 1,
                "confidence": 5
            }
        ],
        "label": "train"
    },
    "B1eWOJHKvB": {
        "paper_id": "iclr_2020_B1eWOJHKvB",
        "paper_title": "Kernel of CycleGAN as a principal homogeneous space",
        "paper_abstract": "Unpaired image-to-image translation has attracted significant interest due to the invention of CycleGAN, a method which utilizes a combination of adversarial and cycle consistency losses to avoid the need for paired data. It is known that the CycleGAN problem might admit multiple solutions, and our goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. We show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability spaces, and, furthermore, that the group of automorphisms acts freely and transitively on the space of exact solutions. We examine the case of zero pure CycleGAN loss first in its generality, and, subsequently, expand our analysis to approximate solutions for extended CycleGAN loss where identity loss term is included. In order to demonstrate that these results are applicable, we show that under mild conditions nontrivial smooth automorphisms exist. Furthermore, we provide empirical evidence that neural networks can learn these automorphisms with unexpected and unwanted results. We conclude that finding optimal solutions to the CycleGAN loss does not necessarily lead to the envisioned result in image-to-image translation tasks and that underlying hidden symmetries can render the result useless.",
        "paper_acceptance": "accept-poster",
        "meta_review": "This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation). The authors analyze the space of exact and approximated solutions under automorphisms.\n\nReviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.\n\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "H1e26UE6tr",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #2",
                "comment": "I have read the rebuttal of the authors . Thank you for you answer and for addressing some concerns.  While the question addressed is important, the theory presented here does not seem to hint to a solution, hence I am keeping my score.    \n\n###\nSummary of the paper: \n\nThis paper shows that the cycle GAN loss suffers from the presence of lot of symmetries that make the existence of a unique solution not possible , and moreover adding a regularizer that uses the identity loss is not enough to make the problem less prone to those invariances. \n\nReview of the paper: \n\nThe notations and the formalism  in the paper are heavy and cumbersome and don't come with any surprising result, since the transforms between unpaired spaces will be found always up to   symmetries since we have the composition of one map with another. The use of the identity loss is also shown to not to help either in fixing this invariance issue. \n\nExperiments are not interesting since without any structure on the map of F and G , the source domain and the target domain, one is expected to get permutations.\n\nThe paper points in the conclusion  that the use of skipconnection in F and G has the major influence.\n\n The study of cycle GAN might need some assessment of what is the mutual information between the domains , as on what  information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images. \n\nAn information theoretic analysis of cycle gan is needed using for example the objective of \"MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation\". \nor by using a radically new approach  for cycle gan such as the Gromov Wasserstein distance as done in \" Learning Generative Models Across Incomparable Spaces\"\n",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rylknOkRFr",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #1",
                "comment": "This paper focuses on CycleGAN method to show theoretically when the exact solution space is invariant with respect to authomorphisms of the underlying probability spaces for unpaired image-to-image translation. \n\n- The paper provides interesting theoretical results on identifying conditions under which CycleGAN admits nontrivial symmetries and has a natural structure of a principal homogeneous space.  Proposition 2.1 provides interesting insights into the invariance of the kernel space. \n\n- Propositions 2.5 and 2.6 are interesting in that they show that the existence of authomorphisms can worsen the performance of CycleGAN, however, it is unclear that in practice, how could one verify the conditions efficiently before applying CycleGAN.\n\n- The experimental results are interesting, however, they are very limited. Having a toy experiment is a good sanity check, but it would be more interesting to see the performance on a real-world applications, such as medical images or other use-cases brought in the introduction. Also, more discussion on the results provided in Fig 3, confusion matrices, would be very helpful. Are there any intuitions behind the large and low values in the table? It could be interesting to see what are the effects of other parameters such as alpha in producing the results. \n\n- Overall this paper presents interesting results regarding the theory of CycleGANs, however, the numerical results are very limited, and do not justify the motivations discussed in the introduction and the abstract. Moreover, although the paper introduces novel attempts and theoretically analyzing the CycleGAN, the scope of the work seems to be limited, and thus, it does not have a sufficient significance to be published in ICLR. I strongly suggest the authors to expand and provide more experimental evaluations.\n\n** update:\nThanks for your comments! I found the additional experiment useful and better aligned with the purpose of the model. The discussion added clarified the confusion about the automorphism. That is why I decided to change my score. ",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1l_X-GJcS",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #3",
                "comment": "This is an interesting, timely study.  CycleGAN has attracted a lot of attention in unpaired image-to-image translation. Although the basic idea of CycleGAN seem sensible, its precise behavior is not totally clear--can one really avoid mismatch with CycleGAN? Do we need additional constraints? This paper provides a nice answer the the first question.\n\nOverall I enjoyed reading this paper. The addressed issue is important, the investigation is reasonable, and the results are intuitive and plausible, with clear practical implications. I think it is a good paper.\n\n I acknowledge I read the authors' response and other reviews and would like to keep my original rating.",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HygDhZNnjH",
                "reply_to": "H1e26UE6tr",
                "title": "Thank you for your feedback!",
                "comment": "Thank you very much for your feedback and additional references, this is very interesting!\n\nWe opted to use the measure theory language because the language of probability distributions is not flexible enough to accomodate commonly occurring distributions, e.g. those (strictly) supported on lower dimensional manifolds. Unless we require the PDF explicitly to have some symmetries, it is not clear why the corresponding probability space would have any smooth automorphisms at all. Thus we show that the existence of automorphisms is a very general property, and in the setup of e.g. latent space with spherical Gaussian PDF we show that there are smooth automorphisms as well.\n\nWe would like to point out that the goal of the paper is to provide a well-grounded and mostly self-contained analysis for the basic CycleGAN approach and to analyze theoretically the effect of the commonly used identity loss, along with some experiments to justify the claims. While the problem of multiple solutions for the CycleGAN is commonly realised, a good theoretical explanation for this is lacking in ML literature. \n\nNaturally, there are other approaches to unsupervised image-to-image translation with different losses and architectures. While analyzing all of them in a single paper is not realistic, we think that the philosophy we suggest in this paper can help researchers better understand the potential and the limitations of these newer image-to-image translation models. The underlying automorphisms can always pose a problem, and the question then becomes if a new loss/new architecture explicitly restricts this set.\n\nWe have added some additional experiments on BRATS 2015 dataset. In this set of experiments we will show how the loss and PSNR (since we have a ground truth) change when we vary the weight for identity loss. We introduce a (approximate) probability automorphism in the form of left/right flips and show that this highly unwanted transformation still obtains low loss values unless an identity loss is used.\n\nWe have also added a discussion about the MISO paper you suggested, where we hypothesize that the MISO approach does not 'solve' the issue of unwanted automorphisms, but rather restricts the set of these automorphisms to those that leave the style of the image fixed. Therefore the amount of uncertainty in this solution is connected to the capacity of the style encoder, and should ideally be quantified. When some important style content is present - e.g., anatomical landmarks - it seems reasonable that one should make sure that the style encoder learns this information. We think it is a an interesting question for future work. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkelelV3sB",
                "reply_to": "rylknOkRFr",
                "title": "Thank you for your feedback!",
                "comment": "Thank you very much for your feedback!\n\nAs for the MNIST2MNIST task, we observed that adding identity loss here forces the network to preserve the original image quite easily. This is in line with our expectations since the domains are identical and the identity loss should trivially remove the ambiguity. The high values in the confusion matrix correspond to the digit class which is very definitive, and the smaller ones correspond to the cases when the digit class is somewhat ambiguous. It can be for instance digit \u20182\u2019 which is written a bit like \u20186\u2019 with a closed loop in the bottom, and it happens with digits '3', '5', '8' as well. \n\nFollowing your feedback, we have added some additional experiments on the BRATS 2015 dataset with medical images. In this set of experiments we show how the loss and PSNR (since we have a ground truth available) change when we vary the weight for identity loss, and we compare these values with flipped version of the image. We see that in the absence of identity loss the final CycleGAN loss is very similar for both original and flipped network output, while the PSNR drops significantly for the flipped version. Increasing the identity loss weight does not always result in improved performance in terms of PSNR.\n\nWe have  also added additional discussion of some newer image-to-image translation models from the 'automorphism point of view', and we hope that some of the questions we pose can be answered in future work.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1exJyVnoB",
                "reply_to": "B1l_X-GJcS",
                "title": "Thank you for your feedback!",
                "comment": "We kindly thank for your feedback! \n\nWe have added some additional experiments on BRATS 2015 dataset to expand the experimental section, and provided an additional discussion of some newer multimodal image-to-image translation models from the 'automorphism point of view'.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "train"
    },
    "r1xMnCNYvB": {
        "paper_id": "iclr_2020_r1xMnCNYvB",
        "paper_title": "JAX MD: End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python",
        "paper_abstract": "A large fraction of computational science involves simulating the dynamics of particles that interact via pairwise or many-body interactions. These simulations, called Molecular Dynamics (MD), span a vast range of subjects from physics and materials science to biochemistry and drug discovery. Most MD software involves significant use of handwritten derivatives and code reuse across C++, FORTRAN, and CUDA. This is reminiscent of the state of machine learning before automatic differentiation became popular. In this work we bring the substantial advances in software that have taken place in machine learning to MD with JAX, M.D. (JAX MD). JAX MD is an end-to-end differentiable MD package written entirely in Python that can be just-in-time compiled to CPU, GPU, or TPU. JAX MD allows researchers to iterate extremely quickly and lets researchers easily incorporate machine learning models into their workflows. Finally, since all of the simulation code is written in Python, researchers can have unprecedented flexibility in setting up experiments without having to edit any low-level C++ or CUDA code. In addition to making existing workloads easier, JAX MD allows researchers to take derivatives through whole-simulations as well as seamlessly incorporate neural networks into simulations. This paper explores the architecture of JAX MD and its capabilities through several vignettes. Code is available at github.com/jaxmd/jax-md along with an interactive Colab notebook.",
        "paper_acceptance": "reject",
        "meta_review": "The paper is about a software library that allows for relatively easy simulation of molecular dynamics. The library is based on JAX and draws heavily from its benefits.\n\nTo be honest, this is a difficult paper to evaluate for everyone involved in this discussion. The reason for this is that it is an unconventional paper (software) whose target application centered around molecular dynamics. While the package seems to be useful for this purpose (and some ML-related purposes), the paper does not expose which of the benefits come from JAX and which ones the authors added in JAX MD. It looks like that most of the benefits are built-in benefits in JAX. Furthermore, I am missing a detailed analysis of computation speed (the authors do mention this in the discussion below and in a sentence in the paper, but this insufficient). Currently, it seems that the package is relatively slow compared to existing alternatives. \n\nHere are some recommendations:\n1. It would be good if the authors focused more on ML-related problems in the paper, because this would also make sure that the package is not considered a specialized package that overfits to molecular dynamics.\n2. Please work out the contribution/delta of JAX MD compared to JAX.\n3. Provide a thorough analysis of the computation speed\n4. Make a better case, why JAX MD should be the go-to method for practitioners.\n\nOverall, I recommend rejection of this paper. A potential re-submission venue could be JMLR, which has an explicit software track.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SkxoFBTijr",
                "reply_to": "Hylmq1Ik9H",
                "title": "Reply",
                "comment": "Thank you for your careful review of our work and useful suggestions!\n\n> Description of the elements of the design of JAX which are useful here are presented, and appear distinct from other \n> AD libraries like Tensorflow or PyTorch, although the authors stop short of explicitly stating which functionality \n> would be more difficult/impossible to support with the possible alternatives (automatic vectorization of the \n> simulations seems like one?).\n\nWe agree with your assessment that automatic vectorization would probably be the largest pain point associated with implementing JAX MD in a different AD library. Indeed, automatic vectorization is deeply integrated with JAX MD since a number of quantities are defined per-particle pair and then vectorized across systems of particles. Having said this, we believe that a TensorFlow version is probably possible since TF2 now supports \u201cvectorize\u201d. Despite our name, we have contemplated building a TF backend in a similar manner to Pyro.\n\n> Limitations of the library and drawbacks of any design decisions (something must be traded at some point?) are not explicitly mentioned.  \n\nThis is a great question and we have added a discussion of this point to the text in section 4. The main tradeoff that we experience is that the primitives that XLA exposes are sometimes at odds with the most efficient primitives for a molecular dynamics simulation. This is particularly important for spatial partitioning where more complicated data structures are often used that are challenging to implement using XLA. We do have an implementation of a cell-list but it is complicated by the fact that shapes must be static in XLA. Our cell-list implementation uses a sort which scales like O(Nlog^2N) on GPU while standard cell-list implementations scale like O(N) when coded directly in CUDA. We have contemplated writing some custom code to circumvent these issues.\n\n> Despite mentioning numerous existing MD libraries, no performance comparison is drawn against any other.  \n\nThis is also a great question (and related to the above). JAX MD is certainly slower than production quality / custom CUDA MD systems. We benchmarked JAX MD against HOOMD Blue on a very standard physical system using a V100 GPU in each case. We found that JAX MD took ~3200 microseconds / step while HOOMD Blue took ~112 microseconds / step. Thus, JAX MD appears to be around 25x slower. We believe that this performance gap will narrow as we improve infrastructure on our end along with improvements to XLA (and possibly MLIR). Having said this, JAX MD is fast enough to do many kinds of research with and where appropriate, we believe that the improvements to research productivity are worth the reduction in performance. We have added a discussion of this point to the text.\n\n> Could also show some demonstration of running an experiment which has complexity on par with state of the art\n> research?  The bubble raft example is great for illustrative purposes, but it could be better to save some of that for a \n> tutorial and use space to exercise this library on a relevant problem and show performance there.\n\nGreat idea! We have added a short discussion to the text cooling a liquid to form a glass. This represents an experiment of similar complexity to research being published today. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1loEHaiiH",
                "reply_to": "SklDy3jycr",
                "title": "Reply",
                "comment": "Thank you for taking the time to review our work.\n\nWe would like to discuss the applicability of this work to the ML community. While it is true that JAX MD will be of use to Physicists, we note that there has been significant research among ML practitioners that would be aided by JAX MD. In particular, we note the following (non-exhaustive) list of papers [1-7] that were published recently in Machine Learning Conferences. In each case, these papers leverage physical simulation; however, they were hindered since the simulations used were not integrated with deep learning libraries. This is precisely the gap that JAX MD hopes to fill. We would like to draw particular attention to [1], \u201cLearning Protein Structure with a Differentiable Simulator\u201d that could have been implemented out-of-the-box using JAX MD and received an oral at ICLR last year. \n\nApart from this, we believe (though there has been less work in this direction so far) that physical systems are an ideal environment to explore meta-optimization since the inner-loop is much better understood than neural networks in deep learning.\n\n[1] Learning Protein Structure with a Differentiable Simulator\nIngraham et al.; ICLR 2019\n\n[2] Interaction Networks for Learning about Objects, Relations and Physics\nBattaglia et al.; NeurIPS 2016\n\n[3] Visual Interaction Networks: Learning a Physics Simulator from Video\nWatters et al.; NeurIPS 2017\n\n[4] SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\nSch\u00fctt et al.; NeurIPS 2017\n\n[5] Learning Invariant Representations of Molecules for Atomization Energy Prediction\nMontavon et al.; NeurIPS 2012\n\n[6] A Compositional Object-Based Approach to Learning Physical Dynamics\nChang et al.; ICLR 2017\n\n[7] End-to-End Differentiable Physics for Learning and Control\nBelbute-Peres et al.; NeurIPS 2018\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkeCer6iiB",
                "reply_to": "Hkga6iGecB",
                "title": "Reply",
                "comment": "Thank you for taking the time to review our work and we appreciate your advice about the writing. We will fix the sentence you noted and generally make the writing more formal.\n\nWe would like to discuss the applicability of this work to the ML community. While it is true that JAX MD will be of use to Physicists, we note that there has been significant research among ML practitioners that would be aided by JAX MD. In particular, we note the following (non-exhaustive) list of papers [1-7] that were published recently in Machine Learning Conferences. In each case, these papers leverage physical simulation; however, they were hindred since the simulations used were not integrated with deep learning libraries. This is precisely the gap that JAX MD hopes to fill. We would like to draw particular attention to [1], \u201cLearning Protein Structure with a Differentiable Simulator\u201d that could have been implemented out-of-the-box using JAX MD and received an oral at ICLR last year. \n\nApart from this, we believe (though there has been less work in this direction so far) that physical systems are an ideal environment to explore meta-optimization since the inner-loop is much better understood than neural networks in deep learning.\n\n[1] Learning Protein Structure with a Differentiable Simulator\nIngraham et al.; ICLR 2019\n\n[2] Interaction Networks for Learning about Objects, Relations and Physics\nBattaglia et al.; NeurIPS 2016\n\n[3] Visual Interaction Networks: Learning a Physics Simulator from Video\nWatters et al.; NeurIPS 2017\n\n[4] SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\nSch\u00fctt et al.; NeurIPS 2017\n\n[5] Learning Invariant Representations of Molecules for Atomization Energy Prediction\nMontavon et al.; NeurIPS 2012\n\n[6] A Compositional Object-Based Approach to Learning Physical Dynamics\nChang et al.; ICLR 2017\n\n[7] End-to-End Differentiable Physics for Learning and Control\nBelbute-Peres et al.; NeurIPS 2018\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hylmq1Ik9H",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #3",
                "comment": "This paper announces a new software package for simulating molecular dynamics which includes close integration with a neural network / machine learning library--the first to do so.  Straightforward access to hardware acceleration (e.g. GPU) is provided for both the simulation and machine learning.\n\nI lean toward accepting this submission.  If it were only about simulation molecular dynamics using hardware accelerators, I would question the appropriateness of the venue, but because it is explicitly intended to support training and usage of learned potential functions, it seems suitable.  Still might be better placed in a physics/chemistry venue, as where most of the references come from and likely where users would, too. The application area is no doubt an important research technique.  The paper is clearly written, with enough specific examples to contrast previous pain points in this line of work against its smoother interface.  \n\nAll of these points are fine for a package-release/tutorial paper, but for a conference paper, might hope to see these addressed:\nDescription of the elements of the design of JAX which are useful here are presented, and appear distinct from other AD libraries like Tensorflow or PyTorch, although the authors stop short of explicitly stating which functionality would be more difficult/impossible to support with the possible alternatives (automatic vectorization of the simulations seems like one?).\nLimitations of the library and drawbacks of any design decisions (something must be traded at some point?) are not explicitly mentioned.  \nDespite mentioning numerous existing MD libraries, no performance comparison is drawn against any other.  \nCould also show some demonstration of running an experiment which has complexity on par with state of the art research?  The bubble raft example is great for illustrative purposes, but it could be better to save some of that for a tutorial and use space to exercise this library on a relevant problem and show performance there.\n\nI took a quick glance at the code on github; it is substantial but not huge, cleanly organized, and is well-documented.  ",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SklDy3jycr",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #1",
                "comment": "The paper presents a python package, called JAX MD for simulating molecular dynamics (MD). JAX MD provides automatic derivations and allows to easily incorporate machine learning models in the MD workflow.\n\nThe paper is clearly written and seems technically correct. However, given that I am a specialist of neither package implementation nor physics, I can not really asses that all the details are correct/useful.\n\nFurthermore, even if this work will surely be of great use for the physics community, I am not not sure that the contribution of this paper is sufficient for ICLR. ",
                "rating": 3,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hkga6iGecB",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #2",
                "comment": "\n\nThis paper describes a general purpose differentiable molecular dynamics physics package, JAX MD. It shows several instances, where it simplifies the research process and enables new avenues of work. \n\nThe Github link is provided for reproducible research and future development. It should be encouraged.\n\nI am sure whether this paper fit the ICLR or not, or how deep learning community can benefit from it.\n\nThe writing does not feel academic enough sometime. For example,  \"Please let us know if there are features that you would find interesting. We are always seeking contributions!\" Please consider the rephrase it.",
                "rating": 3,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "uFORMPcA_b": {
        "paper_id": "nips_2021_uFORMPcA_b",
        "paper_title": "Pipeline Combinators for Gradual AutoML",
        "paper_abstract": "Automated machine learning (AutoML) can make data scientists more productive.  But if machine learning is totally automated, that leaves no room for data scientists to apply their intuition.  Hence, data scientists often prefer not total but gradual automation, where they control certain choices and AutoML explores the rest.  Unfortunately, gradual AutoML is cumbersome with state-of-the-art tools, requiring large non-compositional code changes.  More concise compositional code can be achieved with combinators, a powerful concept from functional programming.  This paper introduces a small set of orthogonal combinators for composing machine-learning operators into pipelines.  It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers.  On that foundation, this paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study.\n",
        "paper_acceptance": "accept",
        "meta_review": "Overall there is not enough support from reviewers for me to recommend acceptance.\n\nReviewers agreed on some real strengths in the paper, including (1) that is well-written and well-organized, and (2) that it tackles an important problem.\n\nOn #1:\n* Reviewer G6BM: \"I really enjoyed reading this paper! The paper is extremely well-written and well-organized\"\n* Reviewer cjD3: \"The authors introduced their AutoML library in a sorted, logical way\"\n\nOn #2:\n* Reviewer G6BM: \"The paper considers a very important problem and provides a very satisfactory solution both in technical and practical point of view\"\n* Reviewer D3TC: \"AutoML is an important application, and if successful, can greatly reduce data scientists' effort\"\n* Reviewer H7JW: \"AutoML is a useful tool [...] and efficient autoML system is an interesting research topic\"\n\nBut the weight of opinion was that the paper (1) doesn't present a clear and significant scientific contribution, (2) shows limited empirical validation, and (3) doesn't concern a software package with large enough practical impact on the NeurIPS community.\n\nOn #1:\n* Reviewer cjD3: \"Furthermore, I don\u2019t see a clear path for scientific future work building on top of Lale\" and \"I'm not convinced that the new operators can express substantially more than the formats of already existing AutoML tools [...] If the expressiveness of the format would be larger than prior work, I would have expected that we need also specialized optimizers for this [...] I have the impression that Lale could be a convenient package for new AutoML users, but this alone does not justify a NeurIPS paper\"\n* Reviewer H7JW: \"be more clear about the scientific contribution from the very beginning of the paper, e.g., a more expressive formalization of the AutoML system.\"\n* Reviewer D3TC: \"The high-level scientific contribution is missing. The paper provides too many low level details without describing what are the main challenges to implement combinators.\" and \"It is not clear why the proposed method is contributing to AutoML\"\n\nOn #2:\n* Reviewer cjD3: \"the user studies only include 9 participants, making statements not very meaningful. Additionally, the survey might even be biased towards Lale\"\n* Reviewer H7JW: \"organize the presentation of the empirical study following the general scientific principles; and to recruit more participants if possible so that one could draw some statistically significant conclusions.\"\n\nOn #3:\n* Reviewer cjD3: \"Overall, I\u2019m not fully convinced that there will be many users of Lale at the end of the day.\" and \"Lale is not even close to the level of pytorch. It has 225 stars on github and was forked 52 times. If that would be the level of impact we expect from a NeurIPS software paper, we will have thousands of papers of those each year.\"\n\nReviewer G6BM disagreed on #1 and #3, and felt the paper does clear the bar, but offered this follow-up commentary: \"I suggest the authors address those concerns in the next version of the paper to make the paper stronger, e.g., by highlighting the first point more and providing concrete/quantitative evidence on how much useful the Lale library is (and will be) for the NeurIPS community.\"\n\n\nThe other reviewers offered several suggestions for the authors to improve the paper, in addition to those quoted above:\n* Reviewer cjD3: \"I liked Section 3 regarding the gradual automation which is a real problem for new (Auto)ML practitioners and I believe that there is a lot of untouched potential here. Unfortunately, this is not really the main focus of the paper.\"\n* Reviewer cjD3: \"Maybe JMLR MLOSS would be a better fit for this paper\"\n\n\nOverall, we weren't able to justify acceptance based either on the significance of the scientific contribution or on the practical impact of the software described here within the NeurIPS community. For that reason I recommend rejecting the paper, but I hope that the reviewers' feedback is helpful to the authors in improving it.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "GNuZexsVC3",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "Official Review of Paper10426 by Reviewer G6BM",
                "comment": "This paper aims at constructing a system for gradual AutoML that is concise, modular (or compositional), and easy-to-use. To this end, the paper introduces three orthogonal combinators (i.e., higher-order functions) which enable compositional code for gradual AutoML, and hyperparameter schemas which describe search spaces of hyperparameters. To support various backend AutoML optimizers, the paper proposes a translation scheme which translates pipelines, described by combinators and hyperparameter schemas, into search spaces for those optimizers. The paper implements these ideas into a Python library (called Lale) which includes a new execution mode (called AutoML search) for running AutoML searches. Through user studies and experiments, the paper shows that Lale is easy-to-use, and can express various pipelines and support various optimizers.  I really enjoyed reading this paper! The paper is extremely well-written and well-organized---it was easy to understand both low-level details and high-level ideas clearly. In particular, the paper succeeds at abstracting out unimportant details and explaining core ideas succinctly. Moreover, the technical contents of the paper look novel and sound to me.\n\nThe paper considers a very important problem and provides a very satisfactory solution both in technical and practical point of view. Technically, the paper introduces three combinators important for compositional gradual AutoML, among which the or combinator is particularly novel. Also the entire system (including combinators, hyperparameter schema, and translation scheme) seems to be designed and engineered in a principled way, and the paper describes some important design principles. Practically, learning and using Lale look easy to me (which is confirmed by user studies) in that it is implemented in Python and based on popular frameworks (sklearn and JSON Schema). Also Lale supports various pipeline structures and optimizer backends. For these reasons, I think Lale will have a huge positive impact on the NeurIPS community.\n\nQuestions & Comments:\n- Line 357 describes \u201cdataset schemas\u201d but I think it is not explained elsewhere in the paper. What is it in detail?\n- Line 291. \u201cparticipants where able\u201d --> \u201cparticipants were able\u201d.\n\n-----\n**Updates after the author response.** Thank the authors for answering my questions. I am still fond of the paper and will keep the same score. \n\n----\n**Updates after internal discussions.** I like the paper for two reasons (which I wrote in the internal discussion):\n\n* First, I do think the paper has scientific contributions. Most importantly, the paper identifies three combinators (pipe, and, or) for the domain of AutoML, and shows that they are enough to express most tasks in AutoML and further enable \"gradual\" AutoML by making a programming system for AutoML \"modular\"---note that the combinators are the key to the modularity. I think this is an important scientific contribution, and would be useful for the NeurIPS community as it enables gradual AutoML (which has not been possible in existing AutoML systems).\n* Second, I have made the following assumption: engineering software that is useful to the NeurIPS community is considered as one of the contributions NeurIPS appreciates. In this context, the paper makes an another contribution by providing the Lale library which I think would be quite useful at least for beginners of AutoML. Of course, my above assumption could be wrong and I totally understand if others disagree with it.\n\nHowever, during internal discussions, other reviewers expressed different thoughts and raised concerns on each of the two points. Given this, I suggest the authors address those concerns in the next version of the paper to make the paper stronger, e.g., by highlighting the first point more and providing concrete/quantitative evidence on how much useful the Lale library is (and will be) for the NeurIPS community. Limitations and societal impact are discussed in the paper.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "nQMS0UiH_3G",
                "writer": "author",
                "reply_to": "5ahnEpsJ71",
                "title": "Author response to updates towards the end of the discussion period.",
                "comment": " Thank you for updating your review after our author response!\n\nYou wrote:\n\n> I hope during this procedure, the author would also feel that both\n> my comments and comments from other reviewers could help to make it\n> a better paper.\n\nWe appreciate the time, effort, and constructive comments from you and\nthe other reviewers and will try to use them to make our paper better.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5ahnEpsJ71",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "Official Review of Paper10426 by Reviewer H7JW",
                "comment": "This paper propose a new autoML system with the formalization of pipeline combinators. At the high-level, machine learning can be automatically used by novices without any expertise; at the low-level, a formalization is proposed to facilitate the search of optimal hyper parameters. User studies and experiments are included to justify the designs.    AutoML is a useful tool to help novices apply machine learning in their domains, thus, to design easy-to-use and efficient autoML system is an interesting research topic. This paper proposes a system with some new formalizations under its software layout. The motivation is clear: to make the utilization of the system as easy as possible while enabling efficient optimization of the hyper-parameters demanded by different machine learning pipelines.\n\nTechnically, I have a few comments:\n+ I am confused by the design philosophy  of the formalization of combinator in Section 2.  Essentially, why do we need these formalizations? The author keeps mentioning that there are some roughly equivalent operators within the sklearn toolkit. Does this section just offer some more formal definition of a language, or is the syntax defined in this section more expressive than the existing systems? It would be better to have a more explicit description about the motivation of the design.     \n+ W.r.t the grammar of \"pipeline\", I am wondering if this could make the formalization be able to be used for neural network architecture search? It seems there is no such discussion.\n\nThe presentation of the empirical study should be organized better: \n+ For RQ1, even though there is limited number of participants (which is fine due to the difficulty of organizing the user study), there still should be some formal description about the null hypothesis in this user study, then there should be some statistical analysis about the statistical significance of the result, e.g., one can consider bootstrap based methods to report the p-value w.r.t the hypothesis. Further, this section should be self-contained, there should be some brief introduction about the tasks.\n\n+ For RQ2 and RQ3, it seems that only the end-to-end performance boost is reported, on the other hand, it is important to understand where the performance gain comes from (perhaps based on some micro-benchmarks). In other words, the section should answer the question why the formalization of the combinators is effective for the black-box optimizer.\n\nPost rebuttal updates:\n\nI really appreciate the great effort the author has made to address my concerns. On the other hand, I hope during this procedure, the author would also feel that both my comments and comments from other reviewers could help to make it a better paper. Here are some follow-up suggestions:\n\n+ To be more clear about the scientific contribution from the very beginning of the paper, e.g., a more expressive formalization of the AutoML system. \n\n+ To organize the presentation of the empirical study following the general scientific principles; and to recruit more participants if possible so that one could draw some statistically significant conclusions.\n\n+ To provide some analysis about the performance gain and to discuss how the gain relates to the proposed design---at the end of the day, as scientists, we not only want to know if something works, we want to know why it works as well.\n \n\n Yes.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "c34WxydEJQb",
                "writer": "author",
                "reply_to": "n7ivTK7ITDY",
                "title": "Author Response to Reviewer cjD3",
                "comment": " > The novelty part can be narrowed down to the unified translation\n> schema, which is the first work in the domain of AutoML to my\n> knowledge. However, it is not clear to me whether this is more\n> novelty in the sense of engineering some useful software or whether\n> this sufficient scientific novelty in the method directly.\n\nWe argue that the novelty of our paper is the programming model\ntogether with the translation scheme that makes the programming model\npossible.  In your words, one way to view this is as \"engineering some\nuseful software\", which is well-aligned with the call-for-papers\ncategory \"Infrastructure (e.g., datasets, competitions,\nimplementations, libraries)\". A more scientific way to phrase this is\nthe hypothesis \"combinators and JSON schemas can effectively specify\nsearch spaces for AutoML\". This hypothesis felt radical at the outset\nof our project, but in retrospect, our answer is a resounding \"yes\".\n\n> experts in ML and AutoML tend to work as closely to the optimizer\n> interfaces as possible to have full control over the optimizer.\n\nThis may be true for people who are experts in both ML and AutoML.\nHowever, that is a small population, and there are far more people who\nmay have hands-on experience in ML and data science but at most a\nworking knowledge in AutoML. Lale is designed for this broader\npopulation.\n\n> Table 2: It seems like Auto-Sklearn outperforms Lale-Auto. What are\n> the reasons for that?\n\nIf you look at all datasets, then Lale outperforms auto-sklearn.\nIf you exclude the dataset on which auto-sklearn performs worst\n(shuttle), then on the remaining datasets, the average accuracy with\nLale is 0.3% worse than with auto-sklearn. Note that unlike Lale,\nauto-sklearn benefits from warm-start via meta-learning.\nThat said, given that 0.3% is less than the standard deviation on most\ndatasets, and given the shuttle result, we did not deem the difference\nimportant enough to investigate further.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hNOvO7qHDwY",
                "writer": "author",
                "reply_to": "5ahnEpsJ71",
                "title": "Author Response to Reviewer H7JW",
                "comment": " > a formalization is proposed to facilitate the search of optimal\n> hyper parameters\n\nThe goal of our library goes beyond hyperparameter search, but also\ncovers the whole spectrum of \"gradual AutoML\" where users can access\nany point in the spectrum (full automation to partial automation to\ndefining arbitrary complex search spaces for directed acyclic\nmachine-learning pipelines) with a consistent programming interface.\nAnd even for expert users who would define complex search spaces\n(which can be done succintly using the syntax facilitated by the\nproposed combinators), the library provides various backends\n(Hyperopt, SMAC) to explore these search spaces regardless of their\ncomplexity.\n\n> Does this section [Section 2] just offer some more formal definition\n> of a language, or is the syntax defined in this section more\n> expressive than the existing systems?\n\nThe syntax is more expressive than existing systems. Specifically, the\nsyntax is more expressive than sklearn, since sklearn lacks a choice\ncombinator (|), and since Lale allows omitting hyperparameters from\nindividual operators. Also, the syntax is more expressive than that of\nmost existing AutoML tools, since it supports higher-order operators,\nwhere hyperparameters passed to an outer operator are themselves inner\npipelines with optimizable operator choices and hyperparameters of\ntheir own.\n\n> W.r.t the grammar of \"pipeline\", I am wondering if this could make\n> the formalization be able to used for neural network architecture\n> search?\n\nYes, the formalization could also be used for neural network\narchitecture search. In fact, we have already created prototypes and\nrun experiments using Lale for simple topologies of neural network\ncomponentry including back-propagation. However, these results are\nstill preliminary and too premature to include in this paper.\n\n> For RQ1, even though there is limited number of participants (which\n> is fine due to the difficulty of organizing the user study), there\n> still should be some formal description about the null hypothesis in\n> this user study, then there should be some statistical analysis\n> about the statistical significance of the result (...)\n\nAs the reviewer points out, the limited number of participants makes meaningful\nquantitative analysis difficult. However, we will provide the following null hypotheses\nto test: 1) T1 correct is the same between Lale and Sklearn, 2) T4 correct is the same, \n3) T4 lines of code (LoC) is the same, and 4) total time taken is the same. The \nMann-Whitney test T4 LoC rejects the third null hypothesis with \na p-value of 0.0039. We are unable to reject the other null hypotheses. This is probably\ndue to the small sample size that is common for laboratory-style user studies. When\nbootstrapping the user study data to 100 observations for each task, all four null \nhypotheses are rejected.\n\n> Further, this section should be self-contained, there\n> should be some brief introduction about the tasks.\n\nWe agree that there should be brief introductions about the tasks and note our\ndescriptions for the tasks starting on line 257. Given the page limit, more detailed\ndescriptions would be difficult to include in the main body but note that detailed \ndescriptions along with the actual test notebooks are available in the supplemental\nmaterial.\n\n> For RQ2 and RQ3, it seems that only the end-to-end performance boost\n> is reported, on the other hand, it is important to understand where\n> the performance gain comes from (perhaps based on some\n> micro-benchmarks). \n\nGiven the page limit, such detailed experiments would be difficult to\ninclude in the main body of the paper. In fact, earlier versions of\nthis paper did include additional performance results, for instance\nwith other data modalities besides tabular data.  Based on your\nfeedback, we may add more drill-down experiments to the supplemental\nmaterial.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oAThYj8H5e",
                "writer": "author",
                "reply_to": "GNuZexsVC3",
                "title": "Author Response to Reviewer G6BM",
                "comment": " Thank you very much for your review! We hope you can convince the\nother reviewers with your positive opinion of our paper.\n\n> Line 357 describes \"dataset schemas\" but I think it is not explained\n> elsewhere in the paper. What is it in detail?\n\nLale uses JSON schema to describe datasets, including the schema of\ndata that an operator expects as input to various methods (fit,\npredict, predict_proba, etc.) or produces as output from various methods.\nTypically, this ends up being a (possibly nested) array schema, sometimes with\ndifferent per-item schemas for columns with different types.\nFurthermore, Lale includes functionality to automatically deduce JSON\nschemas from ARFF files, numpy ndarrays, pandas dataframes, etc.\nThen, Lale performs subschema checking: the schema of data passed\nneeds to be a subschema of the schema of data expected, starting from\nthe input and proceeding along all edges in a pipeline's dataflow graph.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qqg-u4YmyOl",
                "writer": "author",
                "reply_to": "ogiUYIbxm2k",
                "title": "Author Response to Reviewer D3TC",
                "comment": " > It is not clear why the proposed method is contributing to\n> AutoML. It seems like it is just combining existing AutoML\n> approaches with the combinators.\n\nThe contribution of our paper is a library that offers a more\nconvenient programming interface for AutoML.  This is well-aligned\nwith the NeurIPS call for papers, which includes a category on\n\"Infrastructure (e.g., datasets, competitions, implementations,\nlibraries)\". Also, NeurIPS has published papers on innovative\nprogramming interfaces for machine learning in the past.\n\n> The high-level scientific contribution is missing. The paper\n> provides too many low level details without describing what are the\n> main challenges to implement combinators.\n\nLines 52-54 in the introduction crisply state the contributions of\nthis paper. The main challenge is to support the modular\ncombinator-based syntax by rewriting it to a form suitable for a given\noptimizer. This challenge is stated in Lines 215-217 of the paper.\n\n> All the datasets used in this paper seems rather small\n\nWe chose OpenML datasets that allow for meaningful optimization (as\nopposed to just the initial few trials) with the default 1 hour\nsetting of auto-sklearn. Our datasets are drawn from the AutoML\nBenchmark [1] and four of them were also used in the auto-sklearn\nevaluation. Also note that the goal of our evaluation is orthogonal to\nthe sizes of the datasets.\n\n[1] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sebastien Poirier, \nBernd Bischl, and Joaquin Vanschoren, \"An Open Source AutoML Benchmark\".\nICML Workshop on Automated Machine Learning (AutoML@ICML), 2019.\n\n> it is not clear to whether the underlying framework can support\n> diverse model architectures.\n\nSince Lale supports arbitrary nesting of simple core constructs, the\ndiversity of supported architectures is one of its strengths. For\ninstance, Table 2 presents results for four significantly different\nmodel architectures (Lale-Auto, Lale-TPOT, Lale-AD3M, Lale-ADB), and\nSection A.2 in the supplemental material shows details for them.\nWe have also successfully used Lale on other data modalities, such as\nimages, text, and time series. Furthermore, Lale also works with\ndeep-learning operators, such as BERT embeddings.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ogiUYIbxm2k",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "Official Review of Paper10426 by Reviewer D3TC",
                "comment": "This paper proposes a combinators that can combine two functions without naming datasets. They implement these combinators as a part of sklearn pipeline---both prediction and training. Finally, it also adds a hyperparameter search using AutoML to optimize the models.   Strength\n\n+ AutoML is an important application, and if successful, can greatly reduce data scientists' effort.\n+ It looks like a significant engineering effort and mature tool. They support many backend for the optimizers. \n+ Besides reporting testing accuracies for 14 OpenML classification tasks, they also reported  results of an user study demonstrating usefulness of the tool. \n\nWeakness\n- The high-level scientific contribution is missing. The paper provides too many low level details  without describing what are the main challenges to implement combinators.\n- All the datasets used in this paper seems rather small, and it is not clear to whether the underlying framework can support diverse model architectures.\n- It is not clear why the proposed method is contributing to AutoML. It seems like it is just combining existing AutoML approaches with the combinators.  N/A",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "n7ivTK7ITDY",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "Official Review of Paper10426 by Reviewer cjD3",
                "comment": "The authors introduced a wrapper for hyperparameter optimization, named Lale, to create and verify search spaces more easily. To this end, instructions with combinators are used, which further are translated to fit the underlying optimizer (like SMAC, Hyperopt, etc.). Lale is meant to be an interface between machine and human s.t. parts of the pipeline can be easily changed, if desired.  # Novelty\nThe novelty part can be narrowed down to the unified translation schema, which is the first work in the domain of AutoML to my knowledge. However, it is not clear to me whether this is more novelty in the sense of engineering some useful software or whether this sufficient scientific novelty in the method directly.\n\n# Significance / Impact\nThe advantages for Lale over existing AutoML frameworks are especially advantageous for inexperienced users due to two reasons. (i) The unified interface makes it easy to quickly set up an AutoML pipeline for any implemented optimizer. (ii) Graph overviews are a helpful and easy way to check whether the search space is defined correctly. \n\nHowever, experts in ML and AutoML tend to work as closely to the optimizer interfaces as possible to have full control over the optimizer. Since defining search spaces is transparent and easily understandable in several frameworks too, e.g. SMAC and Hyperopt, Lale would not give many advantages over these. Full AutoML systems such as AutoSklearn, on the other hand, would definitely benefit from Lale. Overall, I\u2019m not fully convinced that there will be many users of Lale at the end of the day.\n\nFurthermore, I don\u2019t see a clear path for scientific future work building on top of Lale.\n\nOverall, it is uncertain whether Lale will have a great impact in the community. \n\n# Soundness (method and experimental setup)\n\nThe authors described their method and experiments detailedly and the concept is sound. \n\nI appreciate the overall idea of having several API levels which allow to gradually build up expertise in using AutoML tools. Depending also on the level of the user\u2019s expertise, this should allow to find an appropriate API without switching between different packages. However, I have the impression that conceptually this is a very high-level idea of software engineering and unclear how this scientifically contributes to the NeurIPS community.\n\n# Scholarship\nRelated work covers both combinators in general and in the AutoML domain. The domain is sufficiently covered.\n\n\n# Clarity\nThe authors introduced their AutoML library in a sorted, logical way.\n\n# Minor Comments\n* Using Lale still requires a decent background on machine learning. Calling methods and Interpreting graphs (Figure 11) may not be straight forward for inexperienced users.\n* As already mentioned in the author\u2019s limitations, the user studies only include 9 participants, making statements not very meaningful. Additionally, the survey might even be  biased towards Lale.\n* Figure 6 shows a default routine with the only difference of \u201cencode search space\u201d. There\u2019s no novelty aspect at all.\n* Table 1 would not convince me to use Lale over Auto-Sklearn.\n\n# Questions for Rebuttal\n* Table 2: It seems like Auto-Sklearn outperforms Lale-Auto. What are the reasons for that?\n Yes.",
                "rating": 4,
                "confidence": 5
            }
        ],
        "label": "train"
    },
    "BJlowyHYPr": {
        "paper_id": "iclr_2020_BJlowyHYPr",
        "paper_title": "CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting",
        "paper_abstract": "This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (D-Conv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The D-Conv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms.\n          We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e. mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of neural network models.",
        "paper_acceptance": "reject",
        "meta_review": "The paper presents an approach to forecasting over temporal streams of permutation-invariant data such as point clouds. The approach is based on an operator (DConv) that is related to continuous convolution operators such as X-Conv and others. The reviews are split. After the authors' responses, concerns remain and two ratings remain \"3\". The AC agrees with the concerns and recommends against accepting the paper.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "rJxT5QF55S",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #3",
                "comment": "=========== Update after rebuttal\n\nThanks for the clarifications and the update. I recommend acceptance of the paper and updated to 8.\n\nLast comment: please still improve the appearance of Figure 4 by using a more diverse set of marker shapes as well as overlay and offset tricks -- see https://www.cs.ubc.ca/~schmidtm/Software/prettyPlot.html for an example. \n\n============\n\nThis paper introduces a new convolution operator (D-conv) specifically tailored to model point-cloud data evolving over time, i.e. a set of n points with features and localization-coordinates that can evolve over time. The main idea is to use the k-nearest neighbor structure for each point to get a fixed size k window to use in the convolution to determine the new location and feature values of a point (and a permutation-invariant operation). The D-Conv operator is included in a LSTM architecture (CloudLSTM) to enable the spatio-temporal modeling of point-cloud data, and can be combined in standard neural network architectures such as a Seq2Seq with attention. This is in contrast to previous approaches which modeled the data on a grid through preprocessing, or did not include the temporal component for cloud data. Experiments is conducted on 4 benchmark datasets, covering two point-cloud stream forecasting applications, showing how CloudLSTM give lower prediction error than numerous baselines and alternatives.\n\nWhile the D-Conv idea seems fairly simple and natural, it is novel AFAIK and fairly appropriate to model point-cloud data streams. The approach is well situated in the literature, and the experiments are indicative that this method can improve on the current approaches. I am thus leaning towards accept.\n\nThe paper is fairly clear, though the notation is a bit confusing and somewhat sloppy (see detailed comment below).\n\nImportant clarification requested: the current notation suggests that each channel could have a different location for a point p_n, the K nearest points seem to be defined irrespective on the channel. So is the location fixed across channels; or does this paper allow the neighborhood structures to vary across channel?\n\n== Other detailed comments ==\n\n- p.3 Q_n^K -- it seems it would be more appropriate to define it as an ordered list of k points (rather than a set, as this would loose all information about the order); unless you append a new dimension to each point where you put the ordering information there for the purpose of defining the k points in Q_n^K.\n\n- (2) the notation is a bit weird and overloaded for the summation (without being defined). Examples include \"i in U1\" (when U1 is an integer, not a set); \"p_n^k in Q_n^K\" when p_n^k does not appear in the summation (a clearer alternative would be using the notation v(p_n^k)_i^h for the h^th value of channel i of point p_n^k, e.g.; now p_n^k would indeed appear in the expressoin); \"v_n^h in v_n\" -> why not just summing over h as it is really doing? Etc.!\n\n- (2) S_out^j: each p_n^' should be a *tuple* (not a set like currently written).\n\n- Figure 4: the lines are really hard to distinguish just by the similar colors -- please use different markers for the different lines (and offset the marker so that they can be seen)\n\n- Several neighborhood sizes are experimented with. Note though that smaller neighborhood sizes are just *special cases* of bigger neighborhood sizes (by using zero weight on the last few neighbors in the convolution). Wouldn't it make sense to use a big neighborhood size and regularize in some way the weights for the further neighbors?\n\n- Table 2: for SSIM, there are two rows with 0.69 +/- 0.07 (minimal value) -- they could be both bolded.\n\n- Appendix B, they claim that the complexity of finding the K nearest neighbors (in dimension L for n points) is close to O(K L log(n)) if using KD trees. I vaguely recall issues in high dimension though (in particular that the above complexity is only valid for specific distributions of points in low dimension). E.g. see https://en.wikipedia.org/wiki/K-d_tree#High-dimensional_data where it is mentioned that L << log(n) is normally needed to guarantee efficiency. The claim should properly be nuanced.",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HkgqkZFBjB",
                "reply_to": "ByeeBVfN9S",
                "title": "Response to Blind Review #2",
                "comment": "Thank you for your feedback and for indicating which aspects can be improved on. We note that some of the comments made are already addressed in the original manuscript.\n\nQ: Given that the proposed convolution operator use KNN to choose the nearest neighbors. It would be good to empirically to study how K would affect the performance, does it data-dependent?\n\nA: The impact of K (ranging from 3 to 9) is analyzed in Tables 1 and 2, which suggest that K only affects marginally the prediction performance. Regarding other baseline models based on k-nearest-neighbours (i.e., MLP and LSTM), we test with K ranging from 1 to 100 and the results obtained give a similar conclusion. These results can be found in Table 6 of Appendix L in the revised manuscript.\n\nQ: 2. Is it possible to study the time complexity for various models?\n\nA: We gave a detailed complexity analysis of our D-Conv in Appendix B, which suggests that compared to the convolution operator whose inputs, outputs, and filters have the same size, the D-Conv only introduces additional complexity by searching the $\\mathcal{K}$ nearest neighbors for each point $O(\\mathcal{K}\\cdot L\\log N)$. Such complexity does not increase much even with higher dimensional point-clouds.\n\nQ: It might be good to do further oblation test to study which mechanism actually contribute to the performance. The choice of RNN, attention, or the new operator? \n\nA: We conducted our experiments using strict variable-controlling methodology, i.e., only changing one factor while keep the remaining the same. Therefore, it is easy to study the effects of each factor. For example, taking a look at the performance of LSTM, ConvLSTM, PredRNN++, PointLSTM and CloudLSTM, which employ dense layers, and CNN, PointCNN and D-Conv as core operators but using LSTM as the RNN structure, it is clear that the D-Conv contributes significantly to the performance improvements. Further, by comparing CloudRNN, CloudGRU and CloudLSTM, it appears that CloudRNN $\\ll$ CloudGRU $<$ CloudLSTM. Similarly, by comparing the CloudLSTM and Attention CloudLSTM, we see that the effects of the attention mechanism are not very significant. Therefore, we believe the core operator $>$ RNN structure $>$ attention, ranked by their contribution. \n\nQ: Furthermore, the std is quite large, which makes one wonder if the improvement is statistically significant.\n\nA: We note that the large std exists only in the Cluster A of the air quality dataset. We checked this dataset more carefully, and found that the level of noise therein is more severe than in other case studies. We believe that is the root cause of the larger std. In addition, we note that all models achieve similar std in this dataset (except for MLP), while our proposal obtains the best mean for all metrics, which proves that the improvement is statistically significant.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1goK1W3jS",
                "reply_to": "H1gC3MZcsr",
                "title": "Thanks for your reply, more experiments were added",
                "comment": "Thanks very much for taking time to read our response. We have updated the paper to address your concerns, as follows:\n\nQ: I find it a bit hard to believe that storing (sparser) traffic data long enough to capture day of week effects wouldn't be worth it.  I think at least showing some concrete comparison of the current window vs a 7 day window would be needed.\n\nA: We wish to further clarify that in our previous response, we mean that using shorter input to perform long-term forecasting is more desirable, given the overhead incurred by data measurements, as suggested by [1]. That is not to say that exploiting periodic information may not help improve the forecasting performance. However, recall that a 7-day window corresponds to a 2016-long sequence as input, since the data is sampled every 5 minutes. It is very difficult for RNN-based models to handle such long sequences. In addition, by considering the number of mobile services (38) and antennas (792), the input for 7 days would have 60,673,536 data points. This would make any forecasting model extremely large and therefore impractical for real deployment.\n\nTo capture seasonal information more efficiently, we concatenate the 30 minute-long sequences (sampled every 5 minutes) with a sub-sampled 7-day window (sampled every 2h). This forms an input with length 90 (6 + 84). We conduct experiments on a subset of the mobile traffic dataset (City 1) and show the results in the Appendix M of the revised paper. The results suggest that the performance can indeed be improved by introducing seasonal information, and we believe this is a promising direction that we can further explore in future work.\n\nHowever, and more importantly to the present study, we also remark that the improvement determined by considering a richer input is fairly uniform across models (see Table~7 in the paper): therefore, our conclusion that CloudLSTM outperforms state-of-the-art benchmarks in the mobile traffic forecasting task still holds.\n\n[1] Chaoyun Zhang and Paul Patras. \"Long-term mobile traffic forecasting using deep spatio-temporal neural networks.\" Proc. ACM MobiHoc. 2018.\n\nQ: It seems weird to use K=3 for your method (seems best), but only let the baseline use K=1 or K=9 or higher. \n\nA: K=3 was not shown for the baselines, because we did not see clear performance improvements in their performance with different K values. To eliminate any doubt, we added results with K = {3 ,6} for each baselines and updated Table 6 in the revised paper (note the numbering has changed).\n\nQ: Even if there is some lone data point way far from the existing \"normal\" data, your approach will still force its predictions to be based off (potentially very unrelated) neighbors, rather than say falling back on some simpler heuristic. I am happy that there is some experimental evidence that the method is somewhat robust to outliers, though.\n\nA: Thanks for acknowledging our efforts to how the robustness of our CloudLSTM to outliers. We want to emphasize that the DConv is only an operator of the model and it relies on neighbors of each point. Therefore, only taking one DConv layer for the forecasting might indeed force its predictions over outliers to be based on unrelated neighbors. This is because one DConv will not have sufficient representability to learn proper weights individually for inliers and outliers. However, the DConv is only a component of the entire architecture. By stacking multiple such components (DConv) via dedicated structure (LSTM), the CloudLSTM has much stronger representability and therefore it will be able to learn appropriate weights for outliers between target points and their neighbors. \n\nThis is similar to convolutional operator over images. Though one Conv operator only sums the shared weights over each anchor point and its neighbors, a deep stack of CNN can learn very complex correlations within pixels, and therefore performs remarkably in many Computer Vision applications. Our experiments in Appendix K clearly show that the performance of outliers will not be affected by their \"lone\" positions.\n\nQ: I think I'd like to see a more controlled experiment, where there is some outlier with opposite response to any neighbor, and we see how its prediction changes as its distance grows.\n\nA: To further show that our CloudLSTM is robust to outliers, we re-run the experiments on the air quality dataset, where we push some weather stations away from the center with different distances, so as to construct artificial outliers. Our experiments show that our CloudLSTM performs equally well when forecasting over inliers and outliers, irrespective of the moving distance of the outliers. Importantly, CloudLSTM achieves significantly better performance over its counterpart PointLSTM (PointCNN + LSTM). New results can be found in Appendix K of the revised paper, and the synthetic dataset will be also released publicly for the sake of reproducibility.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1gC3MZcsr",
                "reply_to": "rJgfGGKrsB",
                "title": "Thanks for a thorough rebuttal; some concerns remain",
                "comment": "Overall, I'm still a bit borderline on this paper, but I'm leaning more positive than before based on the thorough rebuttal. If others are voting for acceptance I won't stand strongly against it. I'm glad authors are thinking about a reproducible version of the urban-air dataset and were willing to implement a few baselines I suggested.\n\nRE experiments not considering day-of-week / seasonality effects\n\nI am not quite satisfied with this explanation. I find it a bit hard to believe that storing (sparser) traffic data long enough to capture day of week effects wouldn't be worth it. Surely there is more Saturday activity in downloading game apps than weekday activity, why throw this away? Plus, I think the dataset you have would let you quantify the value of using longer windows, etc. I think at least showing some concrete comparison of the current window vs a 7 day window would be needed. \n\nI looked thru the plots in J, I'm not too convinced the current solution is \"good enough\" and would not be substantially improved with day-of-week effects.\n\nRE baselines with nearest neighbors\n\nOne lingering issue: it seems weird to use K=3 for your method (which seems to consistently do best), but only let the baseline use K=1 or K=9 or higher. \n\nRE outliers\n\nI don't think just using the sigmoid to rescale things solves my concern... even if there is some lone data point way far from the existing \"normal\" data, your approach will still force its predictions to be based off (potentially very unrelated) neighbors, rather than say falling back on some simpler heuristic.\n\nI am happy that there is some experimental evidence that the method is somewhat robust to outliers, though.\n\nI think I'd like to see a more controlled experiment, where there is some outlier with opposite response to any neighbor, and we see how its prediction changes as its distance grows (and how this differs from simpler approaches). ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1eRY1JvjS",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Paper has been revised",
                "comment": "We appreciate the valuable feedback from all reviewers.  We have revised the manuscript to address the concerns ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgfGGKrsB",
                "reply_to": "BkgVBUphtH",
                "title": "Response to Blind Review #1 (1)",
                "comment": "Thank you for acknowledging the contribution of our paper and offering insightful comments. We address the concerns raised below.\n\nQ: The present paper's new D-Conv operator appears new, though it looks really like a simplification of the PointCNN's \"X-Conv\" operator rather than a brand new operator. The most similar work seems to be the PointCNN (Li et al NeurIPS 2018). This work's contribution was a new X-Conv operator, which also consumes point-clouds and produces learned representations. X-Conv, like the present paper's D-Conv, computes K-nearest neighbors of each point p, but performs first an embedding of each neighbor to a learned \"local\" feature space and then performs convolution on this embedding. Perhaps the biggest practical difference is that D-Conv has fewer parameters (does not perform the embedding) and does not reduce dimensionality from input to output. \n\nA: Our D-Conv is inspired by convolution over grids, which computes summations over small regions, as we also do but over point-clouds. This may of course introduce some problems, such as the permutation of the points. X-Conv addresses this by an X-transformation, while our work solves the problem by keeping the order of the k-nearest-neighbor weights. Therefore, we believe this is a different way of solving the problem, rather than a simplification of the X-Conv. Further, different from X-Conv, our D-Conv treats the value and coordinate features differently, by regularizing the coordinate features using a sigmoid function to avoid outliers in space. By looking at the performance of PointLSTM (X-Conv + LSTM, which is also a new model we derive) and  CloudLSTM (D-Conv + LSTM), our CloudLSTM performs much better, which proves that D-Conv is more suitable for the forecasting task.\n\nImportantly, we want to note that the D-Conv is not the only contribution of our paper. Combining a convolutional operator dedicated for point-cloud with an RNN structure for forecasting is also an important contribution and, to the best of our knowledge, we are the first to do this.\n\n\nQ: My biggest concerns are that the D-Conv has a strong reliance on nearest neighbors. This means the D-Conv has not much accommodation for \"outlier\" points that are far from others.  I would imagine that data with outliers (whose values are unlike most others) would dramatically hurt performance, as the weights of D-Conv would need to be shared equally by outliers and inliers.\n\nA: In Eq 2, we regularize the coordinate features of each point using a Sigmoid function, such that those points which are far from others will move closer to each other and will be more involved in the computation. This is exactly how the D-Conv handles outliers. Further, though the weights are shared by outliers and inliers, the final forecasts are made by multiple stacks of CloudLSTM and through multiple steps of computation. This means that the model would have sufficient representability to move those outliers to positions where they are best to be, thus the performance will not be compromised.\n\nFor demonstration, we add a new section in Appendix K in the revised paper, to test the performance of each model especially with outlier points found by DBSCAN in the air quality dataset. Observe that our CloudLSTM remains the best when performing forecasting with outliers. Compared with the full forecasting in Table 2, our proposals achieve even better performance, which proves that the CloudLSTM remains reliable when forecasting over outliers.\n\nQ: Is there a good reason to not try to compare on publicly available datasets like those used in the PointCNN paper (focusing only on the non-temporal versions of the model)? Using proprietary datasets makes following up on this work a bit hard, would be nice to have some reproducible experiment.\n\nA: The datasets in the PointCNN paper are dedicated to point-cloud classification and segmentation, while our work focuses on point-cloud stream forecasting, hence the scope is different. Since the D-Conv, along with the CloudLSTM, are designed for the task of temporal forecasting, we do not test the model on those datasets.\n\nWe fully agree however that it would be good to have a reproducible experiment with publicly available datasets. While the mobile traffic dataset can not be released, the air quality datasets are publicly available and can be found at https://www.microsoft.com/en-us/research/project/urban-air/.\nWe will release the code and processed dataset upon final decision, so as to support the reproducibility of our results.\n\nto be continued...\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1eohztSjH",
                "reply_to": "BkgVBUphtH",
                "title": "Response to Blind Review #1 (2)",
                "comment": "Continues  from the `````````````\u201cResponse to Blind Review #1 (1)\u201d\n\nQ: It's not clear to me that the experiments here consider realistic scenarios.  Certainly there are time-of-day, day-of-week, and seasonal effects that are all important. At a minimum, I'd think that for the mobile traffic case you could at least look at consuming the last 48 hr of data and predicting the next 30-90 minutes. I suspect that would make even simpler models do much better.\n\nA: There are several reasons for not using long sequences as input: (1) Data measurements are expensive; while measuring air quality is easier, mobile data collection is not straightforward, as it relies on dedicated hardware and involves substantial data processing overhead. Therefore, mobile traffic collection is not activated all the time and thus we may not obtain full 24-48 hours of data for making predictions [1]. Therefore, relying on short-term input to make long-term performance predictions is in fact more realistic [2]; (2) The time complexity for RNN-based models grows linearly with the length of the input. In our case, the mobile traffic dataset is sampled every 5 minutes and a 48-hour data trace would correspond to 576-long sequence, which will hard for RNN-based models to handle; (3) By looking at the heat map of the forecasting in Figs 11 and 12 in Appendix J.2, we see that the CloudLSTM is already performing well, given short time series as input; (4) We agree that including periodic information may improve performance; instead of increasing the length of the input, a smarter way is to mix that with the predictions made [2] or taking those as different channels of the input, as they do not increase complexity significantly. This is an avenue we will pursue as part of future work.\n\n[1] Chaoyun Zhang, Xi Ouyang, and Paul Patras. \"ZipNet-GAN: Inferring fine-grained mobile traffic patterns via a generative adversarial neural network.\" Proc. ACM International Conference on emerging Networking EXperiments and Technologies. 2017.\n\n[2] Chaoyun Zhang and Paul Patras. \"Long-term mobile traffic forecasting using deep spatio-temporal neural networks.\" Proc. ACM International Symposium on Mobile Ad Hoc Networking and Computing. 2018.\\\\\n\n\nQ: I think the experiments are missing some key simple baselines (or I misunderstand something). For example, rather than the complicated CNN/LSTM architectures, why not try to directly see how much value there is in \"neighbors\" in this 2d space? At each point, you can make predictions using only the K nearest neighbors' data, with K swept from 1 to 100 or something. I would expect with these features, using just a simple MLP or RNN would do quite well. I'd like to see a stronger qualitative case made for why we expect the complicated D-Conv weighting operator here to do better than this baselines.\n\nA: We agree such baselines should be used for comparison, therefore in the revised manuscript we consider them in Appendix L, where we show the performance of MLP and LSTM (which only take a sequence of k-nearest neighbors of each point as input). We test with K in \\{1, 9, 25, 50, 100\\}, and find that K does not affect the forecasting performance significantly. Importantly, our CloudLSTM achieves much better performance than these baselines.\n\n\nQ: Overall, the results tables appear promising (for app traffic forecasting in Table 1, the proposed CloudLSTM achieves 3.66 MAE compared to 4.95 for PointCNN and 4.8 for an MLP). However, it's not clear why and I'd like to understand why. Is it that the other approaches are overfitting? \n\nA: Our CloudLSTM is able to learn dynamic spatial correlations in point-clouds through different time steps, while the others does not have this nice property. According to our reply to Reviewer 2, it appears that the D-Conv contributes most to the performance, followed by the RNN, and the attention mechanism. This is not because other approaches are overfitting. We add such a discussion in Sec. 4.4 of the revised paper.\n\n\nQ: I would suggest avoiding calling the method \"$\\mathcal{D}$-Conv\", and instead use just \"DConv\", since this is easier to type into search engines and easier to search for in a PDF document\n\nRelated: Point clouds could be represented as graphs, and then use graph embeddings as feature representations\n\nA: Thanks for the suggestion. We change $\\mathcal{D}$-Conv to DConv in the revised paper. Regarding the graph embedding, we agree this will have potential and will consider this for future work.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1l5J0OSsS",
                "reply_to": "rJxT5QF55S",
                "title": "Response to Blind Review #3",
                "comment": "We appreciate the detailed and positive comments, which clearly reflect many of the essential contributions of our work. We answer the points raised in what follows.\n\nQ: Important clarification requested: the current notation suggests that each channel could have a different location for a point $p_n$, the K nearest points seem to be defined irrespective on the channel. So is the location fixed across channels; or does this paper allow the neighborhood structures to vary across channel?\n\nA: The K nearest points can indeed vary for each channel at each location. The reason is that the channels in the point-cloud dataset may represent different types of measurements. For example, channels in the mobile traffic dataset are related to the traffic consumption of different mobile apps, while those in the air quality dataset are different air quality indicators (SO2, CO, etc.). The spatial correlations will vary between different measurements (channels), due to human mobility. For instance, more people may use Facebook at a social event, but YouTube traffic may be less significant in this case. This will be reflected by the data consumption of each app. The same applies to air quality indicators affected by vehicle movement and factory working times. We want these spatial correlations to be learnable, so we do not fix the K nearest neighbors across channels, but encourage each channel to find the best neighbor set. This is also a contribution of the CloudLSTM, which helps improve the forecasting performance. We add this discussion in Sec. 3.2 of the revised paper.\n\nQ:- p.3 $Q_n^K$ -- it seems it would be more appropriate to define it as an ordered list of k points (rather than a set, as this would loose all information about the order); unless you append a new dimension for the defining the k points in $Q_n^K$.\n\n- (2) the notation is a bit weird and overloaded for the summation (without being defined). Examples include \"i in U1\" (when U1 is an integer, not a set); \"$p_n^k$ in $Q_n^K$\" when $p_n^k$ does not appear in the summation (a clearer alternative would be using the notation $v(p_n^k)_i^h$ for the $h^{th}$ value of channel i of point $p_n^k$, e.g.; now $p_n^k$ would indeed appear in the expression); \"$v_n^h$ in $v_n$\"  why not just summing over h as it is really doing? Etc.!\n\n- (2) $S_{out}^j$: each $p_n'$ should be a tuple (not a set like currently written).\n\nA: We very much appreciate the detailed comments on these issues. We have update notation accordingly in the revised manuscript, which is available online via OpenReview. We trust the revision eliminates any confusion. \n\nQ:  Figure 4: the lines are really hard to distinguish just by the similar colors -- please use different markers for the different lines.\n\nA: Thank you for raising this issue. We acknowledge the original figure had readability issues and have updated it (and Fig. 9) to address this problem.\n\nQ:  Several neighborhood sizes are experimented with. Note though that smaller neighborhood sizes are just special cases of bigger neighborhood sizes. Wouldn't it make sense to use a big neighborhood size and regularize in some way the weights for the further neighbors?\n\nA: This is an excellent point. We agree that smaller neighborhood sizes are special cases of bigger neighborhood sizes. The reasons we do not use a large K are because (1) as analyzed in Appendix B, the complexity of D-Conv grow linearly with K; (2) While testing with K ranging between 3 and 9, we do not see clear performance improvements with higher K; (3) The K is equivalent to the receptive field of a normal CNN kernel (e.g., K=9 is is equivalent to a 3*3 CNN kernel), and a small CNN kernel has been proven effective in imaging applications. To answer Reviewer 1's question, we test with K between 1 and 100 for k-nearest-neighbor based MLP and LSTM in Appendix L; the results also suggest a large K does not improve performance. At the same time, we agree that using a big neighborhood size and regularizing in some way may be appropriate, as it may reduce overfitting caused by large K. We will consider this approach for future work.\n\nQ: Table 2: for SSIM, there are two rows with 0.69 +/- 0.07 (minimal value) -- they could be both bolded.\n\nA: This has been revised. Thanks.\n\nQ: Appendix B, they claim that the complexity of finding the K nearest neighbors (in dimension L for n points) is close to O(K L log(n)) if using KD trees. I vaguely recall issues in high dimension though (in particular that the above complexity is only valid for specific distributions of points in low dimension).  where it is mentioned that L $<<$ log(n) is normally needed to guarantee efficiency. The claim should properly be nuanced.\n\nA: This is correct, and, in fact, in real life the dimensions of a point-cloud dataset are normally 2 or 3. Also, we usually have much more than 3 points in the dataset. Therefore, L $<<$ log(n) should hold for most applications. We clarified this aspect as a footnote in Appendix B.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgVBUphtH",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #1",
                "comment": "Review Summary\n--------------\nOverall this is almost above the bar for me to accept, but I think there's enough concerns about the method and experiments that I'm hesitant. Strengths include the invariance to point cloud order and the relative simplicity of the architecture (compared to PointCNN). Weaknesses include a vulnerability to outliers, experiments that don't seem to think about practical effects like day-of-week in forecasting, and experiments that leave out baselines to help directly assess the impact of neighbors.\n\n\nPaper Summary\n-------------\nThe paper develops a new neural net architecture for processing data structured as spatial point clouds that vary over time (e.g. hourly traffic at several antennas spread throughout a city).\n\nThe core of the approach is a new neural net unit: the \"D-Conv\" operator (See Eq. 2). The output value at each point is obtained via a weighted combination of nearby coordinates and features, using only the K-nearest neighbors (stored in ranked order) to maintain invariance to the original order of points. This layer can be included in modern convolutional (CloudCNN) or recurrent (CloudLSTM) or attention-based architectures in a straightforward way.\n\nUnlike many previous methods that require converting point clouds to quantized regular grids, the present approach directly consumes point cloud data. Unlike some existing methods like PointCNN, it avoids information loss (does not reduce dimension from input to output layer).\n\nTwo experimental evaluations are conducted: forecasting mobile app traffic across 2 European cities (given past 30 min, predict next 30 min), and air quality across several regions in China (given last 12 hrs, predict next 12 hrs). In both experiments, the locations of the sensors are fixed across time. Fig 4 further looks at traffic forecasting as a function of the lookahead time, from 0-3 hours ahead.\n\n\nNovelty & Significance\n-----------------------\n\nThe paper definitely tackles an important problem (point cloud forecasting). \n\nThe present paper's new \"D-Conv\" operator appears new, though it looks really like a simplification of the PointCNN's \"X-Conv\" operator rather than a brand new operator. \n\nThe most similar work seems to be the PointCNN (Li et al NeurIPS 2018). This work's contribution was a new \"X-Conv\" operator, which also consumes point clouds and produces learned representations. X-Conv, like the present paper's D-Conv, computes K-nearest neighbors of each point p, but performs first an embedding of each neighbor to a learned \"local\" feature space and then performs convolution on this embedding. Perhaps the biggest practical difference is that D-Conv has fewer parameters (does not perform the embedding) and does not reduce dimensionality from input to output. \n\nTechnical Concerns\n------------------\n\nMy biggest concerns are that the D-Conv has a strong reliance on nearest neighbors. This means the D-Conv has not much accomodation for \"outlier\" points that are far from others. The X-Conv operator has some nice properties in this regard (it changes coordinate systems so neighbor locations are centered around the current point), but I don't see this in the D-Conv operator, as in Eq. 2, where the coordinate locations are fed directly into the weighted sum after global rescaling to (0,1). I would imagine that data with outliers (whose values are unlike most others) would dramatically hurt performance, as the weights of D-Conv would need to be shared equally by outliers and inliers.\n\n\nExperimental Concerns\n---------------------\n\nIs there a good reason to not try to compare on publicly available datasets like those used in the PointCNN paper (focusing only on the non-temporal versions of the model)? Using proprietary datasets makes following up on this work a bit hard, would be nice to have some reproducible experiment.\n\nIt's not clear to me that the experiments here consider realistic scenarios. Why would I predict mobile app traffic using only the past 30 minutes of data? Why predict air quality using only the last 12 hours? Certainly there are time-of-day, day-of-week, and seasonal effects that are all important. At a minimum, I'd think that for the mobile traffic case you could at least look at consuming the last 48 hr of data and predicting the next 30-90 minutes. I suspect that would make even simpler models do much better.  \n\nFurther, I think the experiments are missing some key simple baselines (or I misunderstand something). For example, rather than the complicated CNN/LSTM architectures, why not try to directly see how much value there is in \"neighbors\" in this 2d space? At each point, you can make predictions using only the K nearest neighbors' data, with K swept from 1 to 100 or something. I would expect with these features, using just a simple MLP or RNN would do quite well. I'd like to see a stronger qualitative case made for why we expect the complicated DConv weighting operator here to do better than this baselines.\n\nOverall, the results tables appear promising (for app traffic forecasting in Table 1, the proposed CloudLSTM achieves 3.66 MAE compared to 4.95 for PointCNN and 4.8 for an MLP). However, it's not clear why and I'd like to understand why. Is it that the other approaches are overfitting? \n\n\nMinor Concerns\n--------------\nI would suggest avoiding calling the method \"\\mathcal{D}-Conv\", and instead use just \"DConv\", since this is easier to type into search engines and easier to search for in a PDF document\n\nRelated: Point clouds could be represented as graphs, and then use graph embeddings as feature representations",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ByeeBVfN9S",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #2",
                "comment": "The paper proposed a new convolution operator, named dynamic post-cloud convention over spatiotemporal data, and the convolution operator can be embedded in different neural network architectures, like recurrent neural networks. In order to achieve the convolution over point-clouds by using both value features and the spatial-features, given a data point, the convolution is conducted over its k-nearest neighbors generated by CNN.  They compared the proposed convolution method by embedding it into RNN, GRN, and LSTM against a number of existing methods on two datasets, in terms of MAE, RMSE, PSNR, and SSIM. Overall, this paper is interesting but needs some clarifications on \n\n1. Given that the proposed convolution operator use KNN to choose the nearest neighbors. It would be good to empirically to study how K would affect the performance, does it data-dependent\n2. Is it possible to study the time complexity for various models?\n3. Table 1 and table 2 seem to show that the proposed convolution operator contributes to the performance in terms of the mean of each metric. It might be good to do further oblation test to study which mechanism actually contribute to the performance. The choice of RNN, attention, or the new operator? Furthermore, the std is quite large, which makes one wonder if the improvement is statistically significant.",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test"
    },
    "HkldyTNYwH": {
        "paper_id": "iclr_2020_HkldyTNYwH",
        "paper_title": "AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT",
        "paper_abstract": "Generative adversarial networks (GANs) have attracted huge attention due to\n      its capability to generate visual realistic images. However, most of the existing\n      models suffer from the mode collapse or mode mixture problems. In this work, we\n      give a theoretic explanation of the both problems by Figalli\u2019s regularity theory of\n      optimal transportation maps. Basically, the generator compute the transportation\n      maps between the white noise distributions and the data distributions, which are\n      in general discontinuous. However, DNNs can only represent continuous maps.\n      This intrinsic conflict induces mode collapse and mode mixture. In order to\n      tackle the both problems, we explicitly separate the manifold embedding and the\n      optimal transportation; the first part is carried out using an autoencoder to map the\n      images onto the latent space; the second part is accomplished using a GPU-based\n      convex optimization to find the discontinuous transportation maps. Composing the\n      extended OT map and the decoder, we can finally generate new images from the\n      white noise. This AE-OT model avoids representing discontinuous maps by DNNs,\n      therefore effectively prevents mode collapse and mode mixture.",
        "paper_acceptance": "accept-poster",
        "meta_review": "The authors present a different perspective on the mode collapse and mode mixture problems in GAN based on some recent theoretical results. \n\nThis is an interesting work. However, two reviewers have raised some concerns about the results and hence given a low rating of the paper. After reading the reviews and the rebuttal carefully I feel that the authors have addressed all the concerns of the reviewers. In particular, at least for one reviewer I felt that there was a slight misunderstanding on the reviewer's part which was clarified in the rebuttal. The concerns of R1 about a simpler baseline have also been addressed by the authors with the help of additional experiments. I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. \n\nHaving said that, I strongly recommend that in the final version, the authors should be a bit more clear in motivating the problem. In particular, please make it clear that you are only dealing with the generator and do not have an adversarial component in the training. Also, as suggested by R3 add more intuitive descriptions to make the paper accessible to a wider audience.\n\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "S1ew3XnmsS",
                "reply_to": "SylWjepycS",
                "title": "Response to Reviewer #1",
                "comment": "\n\n----------------------------\nQ1: My concern is whether the proposed method is overkill because the singular point detection can\nbe very tricky and relies on heavy linear programming.\n\nAnswer: The detection of singularities is direct and simple, for the convex polyhedron of the Brenier\npotential, just compute the inner product of the normals to each pair of adjacent facets. If the inner\nproduct is too big, then the projection of the intersection between the facets is in the singularity set.\nSo this work doesn\u2019t involve any linear programming at all.\n\n----------------------------\nQ2: Could you explain why not using the following substitute: Step 1. Fit an auto-encoder just as\nyou did in the paper and get an empirical distribution \u03bd. Step 2. Fit a Gaussian mixture model on \u03bd\nand do model selection over # clusters. Step 3. Sample from the Gaussian mixture model to generate\nfresh images.\n\nAnswer: We thank Reviewer #1 for the suggestions. The proposed approach is inspiring, but it has\npotential drawbacks:\n\u2022 If the empirical distribution has only one mode, but the support is concave, then the proposed\nmethod still can not avoid generating unrealistic samples.\n\u2022 If the empirical distribution has multiple modes, the resulting Gaussian mixture will fill\nthe gaps among the modes, therefore the proposed method still can not avoid generating\nunrealistic samples (mode mixture).\n\u2022 Fitting Gaussian mixture itself is expensive and without further assumptions, the convergence\nof the GMM fitting cannot be guaranteed.\n\nIn order to show the above claims, we did the following experiments: firstly we fit the 60K latent code\nof MNIST dataset by GMM, with the number of modes set to be 10, 30, 100. Then t-SNE is used to\nvisualize the data. The blue crosses are the generated data by the GMM model and the green circles\nare the training data. In the anonymous website https://drive.google.com/file/d/12HbiQNAoTpxnk-h10LY0O90j8QhSIKqw/view?usp=sharing, we provide the results: Fig. (a)(b)(c) show the generation results of GMM,\nfrom which we can see that there are huge number of generated samples in the regions among the\nmodes. While for the proposed method, as shown in Fig. (d), nearly no generated samples fall into\nthe gaps.\n\n----------------------------\nQ3: Since this method relies on a high-quality auto-encoder model, it is hard to say this paper\nmakes progress in fixing the GAN\u2019s mode collapsed problem. Besides, the paper does not involve an\nadversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall,\nthe proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the\npaper aim to achieve.\n\nAnswer: The real goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #1 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which illustrate that the proposed AE-OT model solves the discontinuity problems encountered by both GANs and VAEs. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part. \nAccording to Figali\u2019s Fields medal work, it shows the intrinsic reason for mode collapse is the discontinuity of transportation map, caused by the concavity of the support of the data distributions. Based on this theoretic discovery, the AE-OT model is proposed. This model is not a conventional GAN model, but a novel generative model that exactly solves the main problems we are targeting at.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxxhH_wjS",
                "reply_to": "ryetcUV7iH",
                "title": "Response to Reviewer #4",
                "comment": "\n----------------------------\nQ1:  I have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension.   The illustration in Fig. 3,  and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper.\n\nAnswer: Here we want to clarify that singular set detection is *piece-wise linear* separation, rather than *linear* separation. In Fig. 3(a), the singular set (shown in red lines) is illustrated by a piece-wise linear curve. Also, Fig. 6 of the appendix shows another example with the numerically computed singular set (also piece-wise linear) by our method, and it is much more curved and complicated.\n\n----------------------------\nQ2: Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nAnswer:  As shown in the the last paragraph of Section 4.1, we conducted experiments of mode collapse on real images like stacked MNIST and CelebA on Section C.3 and C.4 of the appendix.\n\n----------------------------\nQ3:  Singular set detection seems to be the most tricky part in this paper, the Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nAnswer: (1) There is no simplex projection assumption in our paper. In fact, Fig.3(a) illustrates the Brenier potential and the corresponding power diagram. The upper hyperplane envelope in top of Fig.3(a) is the graph of Brenier potential, and the bottom of Fig. 3(a) shows the source domain of the Brenier potential, expressed as a cell decomposition structure. Each facet in the image of Brenier potential corresponds to a cell in the source domain (\u2126), and the ridges on the image of Brenier potential corresponds to edges of cells in the source domain.\n\n(2) In the image of the Brenier potential, the \"sharp ridges\" are composed of the edges where the angles between the corresponding pairs of adjacent facets are large (as shown in Fig.3(a)). In fact, the normal of a facet n= (p_1, p_2,..., p_d, \u22121) actually corresponds to a latent code y= (p_1, p_2,..., p_d). And the large angle between two adjacent facets means that the distance between the corresponding latent codes is large.  This often happens when the codes come from different modes.  Thus, the singular set, or equivalently the \"sharp ridges\" gives the information about different modes.\n\n(3) Singular set detection is proposed in our paper for the following reason. Firstly, the singular set is totally decided by the semi-discrete OT map, or equivalently, the Brenier potential (Fig.  3(a)). Secondly, the image of the semi-discrete OT map itself is the given discrete latent code, thus we extend it with a piece-wise linear manner, so that the extended OT map can be used to *generate new codes* (Fig. 3(b)). Thirdly, the samples around the singular set will be mapped to the gaps among the modes by our extended OT map and cause the mode mixture problem, thus the singular detection is needed. Finally, given a sample x, if it falls around the singular set (checked by Alg. 2), we just don\u2019t use it to generate new latent code.\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkgkludPiH",
                "reply_to": "SJx3_vOPiH",
                "title": "Response to Reviewer #4 ",
                "comment": "\n----------------------------\nQ5: The authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view.\n\nAnswer: The main goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #4 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which analyze the discontinuity problems encountered by GANs and VAEs. Then we propose a new generative model called AE-OT. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part.\n\nBecause our main focus is to solve mode collapse/mixture problems, we didn\u2019t apply the most\nadvanced auto-encoder (AE). If the capacity of AE is insufficient, the result is not satisfying, such as\nthe celebA dataset noticed by the reviewer. But, as we explained in section 4.2, the 3rd paragraph, if\nthe capacity of AE is sufficient, our model outperform others.\n\nAs recent GAN improvements mostly focus on the discriminator, our work complements these\nworks by critically analyzing and making improvement on the generator. Future research on adding\nadversarial loss to our current model is also intriguing.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJx3_vOPiH",
                "reply_to": "BkxxhH_wjS",
                "title": "Response to Reviewer #4 ",
                "comment": "\n----------------------------\nQ4: Singular set detection seems to be the most tricky part in this paper, which should have been explained further.\n\nAnswer: In the following, we justify our algorithm using the theoretic works summarized in the following book:\nFigalli, A. (2017). The Monge\u2013Amp\u00e8re equation and its applications.\n\nAccording to Brenier\u2019s theorem, the optimal transportation map T is the gradient of the convex Brenier potential u, and u satisfies the Monge-Amp\u00e9re equation.\nIn his book, the Fields medalist Figalli proved the existence and the uniqueness of the solution to the\nMonge-Ampere equation in Chapter 2, where he used Alexandrov\u2019s approach:\n1. Approximate the data distribution \u03bd to a sequence Dirac distributions \u03bdn, such that the sequence of {\u03bd_n} weakly converges to \u03bd;\n2. For each Dirac measure \u03bd_n, there exists an Alexandrov\u2019s solution u_n, which is exactly the discrete Brenier potential in our paper;\n3. The weak solutions {u_n} converges to the real solution u, u is C^1 almost everywhere, except at the singular set.\n\nOur Semi-Discrete OT algorithm is completely equivalent to Alexandrov\u2019s solution. In fact, the proof\nin Figalli\u2019s book is not constructive, (Alexandrov\u2019s original proof is based on Algebraic topology), which doesn\u2019t induce an computational algorithm. Therefore, the theorem 2 in the Appendix gives a variational framework to explicitly compute the discrete Brenier potential. By Figalli\u2019s work, the discrete Brenier potential {u_n} converges to the smooth Brenier potential, which is C^1 except at the singular set. The piece-wise linear map in Fig.3(a) converges to the real optimal transportation map.\n\nThe singular set is the non-differentiable points (only C^0 but not C^1) of the Brenier potentials, namely the ridges of the graph of u. This ridge structure becomes prominent and well-preserved in\nthe process of approximating u by piece-wise linear polyhedra {u_n} in Fig.3(a).\n\nCompared to Fig.3, Fig. 6 and Fig. 7 in the appendix gives better illustration for the singularity. The\noriginal version of Fig. 6 is given by Figalli as the Fig. 3.2 in the following article,\nFigalli, A. (2010). Regularity properties of optimal maps between nonconvex domains in the plane.\nCommunications in Partial Differential Equations, 35(3), 465-479.\n\nWe can see that the singular set has complicated geometric and topological structures, which can not\nbe captured by linear separation, but still can be found by *piece-wise linear approximation*. In\nfact, the optimal transport map shown in Fig. 6 is numerically computed by our algorithm, and the\nsingular set is piece-wise linear, approximating the singular set in the smooth case (shown as Fig. 3.2\nin the above mentioned article).\n\nNext we show that the singular set structure of the smooth Brenier potential is well preserved by\nour SDOT map. From chapter 2 in Figalli\u2019s book, we know that the piece-wise linear functions un\n(discrete Brenier potential) converges to the real smooth Brenier potential u, which is C^1\neverywhere except at the singular points. Therefore the graph of the smooth Brenier potential has ridges, these ridge structure are well preserved during the piece-wise linear approximation by the discrete Brenier\npotential. \n\nTherefore, singularity detection boils down to locate the ride structure of the graph of the discrete\nBrenier potential, which is a convex polyhedron. The ridge on a convex polyhedron can be easily\nfound by computing the angles between each pair of adjacent facets (dihedral angles for 2D case).\nBecause the discrete Brenier potential is convex, its projection induces a power diagram, each cell is\nconvex. The dual of this power diagram gives the power Delaunay triangulation of training samples\n(y_i\u2019s) (Fig. 2 of the following article). This geometric interpretation of semi-discrete OT doesn\u2019t\nrequire the linear separation assumption. The relation among discrete Brenier potential, power\ndiagram and power Delaunay triangulation is explained in details in\nGu, X., Luo, F., Sun, J., & Yau, S. T. (2016). Variational principles for Minkowski type problems,\ndiscrete optimal transport, and discrete Monge\u2013Amp\u00e8re equations. Asian Journal of Mathematics,\n20(2), 383-398. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Syeh0W3Xor",
                "reply_to": "r1e5aB-z9r",
                "title": "Response to Reviewer #3",
                "comment": "\n\n----------------------------\nQ1: Although this paper brings a new perspective, based on optimal transport theory, as far as I can\nunderstand this paper does not establish formal new results. Thus I think some strong claims about\nproviding deep theoretical explanation should be more moderate. In essence, it seems that the paper\nverifies *numerically* (in section B.3) that Figalli\u2019s theorem (stated in Appendix B) holds in this\ncontext.\n\nAnswer: This work focuses on using Figalli\u2019s regularity theory of Optimal Transportation Map to\nexplain mode collapse/mixture in generative models and propose a novel model to tackle it, not to\ndevelop the new regularity theorems. We will follow the reviewer\u2019s suggestion to make our claims\nmore moderate.\n\n----------------------------\nQ2: This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation\ncould help.\n\nAnswer: We will follow reviewer\u2019s suggestion to add more intuitive explanations and simplify the\nnotations.\n\n----------------------------\nQ3: After Eq. (5) in the Appendix the authors mention Newton\u2019s method, and Thm 3 is also specific\nto Newton\u2019s method. Then they mention that *Gradient Descent* is used (and in the main part of the\npaper they mentioned Adam). This is confusing. All these algorithms are different, and Newton\u2019s\nmethod does not imply convergence results for gradient descent. I don\u2019t see how Thm 3 is relevant.\n\nAnswer: According to the variational framework of semi-discrete optimal transportation map,\ntheorem 2, the computation of OT map is reduced to a convex optimization. Hence both gradient\ndescend and Newton\u2019s method converge. We will modify Thm 3 accordingly. Furthermore, this work\nfocuses on gradient descend method, in the future work, we will explore Newton\u2019s method as well.\n\n----------------------------\nQ4: This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the\nBrenier potential and is able to locate the singularities. I wonder if using a simpler approach through\noptimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve\nthis issue? In the negative case, why not?\n\nAnswer: Although the OT map is discontinuous, and the Brenier potential is non-differentiable, the \nenergy to be optimized is C^2 in terms of h. Therefore, in the current work, for the optimization\npurpose it is unnecessary to use Moreau envelope or proximal methods. Specifically, the convex\nenergy E(h) we aim to optimize is differentiable with respect to h. With the optimal h, the OT map\ncan be induced. Therefore for the optimization, we actually do not need smoothing techniques to carry\nout the optimization. Secondly, the non-differentiability of Brenier\u2019s potiential uh(x) is considered\nwith respect to x given the optimal h. This is independent of the optimization process.\n\n----------------------------\nQ5: Some Minor comments: 1. Define OT in the abstract (Optimal Transportation?) 2. What is AE?\n(not defined also; Auto Encoder?) 3. There are lots of typos through the text, such as missing \"the\",\n\"a\", etc. and a couple mispelled words. I suggest the authors proofread the draft more carefully. 4.\npp. 4 ... what is a \"PL convex function\". PL is not defined.\n\nAnswer: We thank the reviewer 3 for the comments. In the paper, OT represents optimal transport,\nAE means autoencoder and PL is the abbreviation of piece-wise linear. We will add more explains to\nthe abbreviations and find native speakers to help proofread the updated manuscript.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SylGfl3XiH",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Thanks for your careful comments",
                "comment": "We thank reviewers for carefully examine our work in such a short time.  Since our work involvesnon-trivial theories from optimal transportation, such as the brand new theorems of Figalli, andregularity theorems for Monge-Ampere equation, the review requires huge amount of efforts.  Wedeeply appreciate all reviewers from deep of our hearts. Since all the reviews and rebuttals will bepublic online, we prepared our rebuttal with great caution, and addressed all the questions raised byreviewers carefully",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryetcUV7iH",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #4",
                "comment": "This paper deals with an important problem of mode collapse and mode mixture. In order to\ntackle the both problems, the paper proposes to separate the manifold embedding and the\noptimal transportation problems; the first part being carried out using an autoencoder to map the\nimages onto the latent space and the second part is accomplished using a GPU-based\nconvex optimization to find the discontinuous transportation maps.\n\nI have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension. The illustration in Fig. 3, and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper. Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nSingular set detection seems to be the most tricky part in this paper, which should have been explained further. The Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nThe authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view. ",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SylWjepycS",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #1",
                "comment": "Contributions:\n1. This paper proposes a new problem in GAN distribution mapping: the concavity of support problem.\n2. This paper provides a solution to the concave support together with mode collapsed problem in GAN, via a discrete-continuous optimal transport model, given some post-processing techniques to rule out \"singular points\".\n3. Empirical results show the effectiveness of the proposed method.\n\nTo summarize their method. First, they fit a good auto-encoder model to get embeddings for the observed data as an empirical distribution \\nu on space Z. Second, they use a semi-discrete OT to map a noise distribution \\mu to \\nv. Since OT will be aware of all modes in \\nu, singular points can be detected by checking the angle between \"shards\" and those points that are around the \"ridge\" should be rejected. Thus, the proposed method could handle both the concave support problem and the mode collapse problem.\n\nMy concern is whether the proposed method is overkill because the singular point detection can be very tricky and relies on heavy linear programming. Could you explain why not using the following substitute: \nStep 1. Fit an auto-encoder just as you did in the paper and get an empirical distribution \\nu.\nStep 2. Fit a Gaussian mixture model on \\nu and do model selection over # clusters.\nStep 3. Sample from the Gaussian mixture model to generate fresh images.\n\nSince this method relies on a high-quality auto-encoder model, it is hard to say this paper makes progress in fixing the GAN's mode collapsed problem. Besides, the paper does not involve an adversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall, the proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the paper aim to achieve.",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1e5aB-z9r",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #3",
                "comment": "General Comments:  The generator in Generative Adversarial Networks (GANS) computes an optimal transportation from the noise distribution to the data distribution.  However, such maps are in general discontinuous.  Since deep neural networks can only represent continuous maps, this brings two problems: mode collapse and mode mixture. This paper approaches both problems using Figalli's regularity theory. They separate the manifold embedding (here an autoencoder maps input data to a latent space) from the optimal transportation (this map is found by convex optimization). Composing these two steps yields the proposed method. Their method basically avoids representing discontinuous maps by the generator. Empirically, the proposed method performs similar or better than state-of-the-art.\n\nI think the idea of the paper is nice, and an interesting perspective  on GANs is presented. A new method is proposed. The numerical contributions are certainly significant. Therefore, I believe the paper deserves publication.\n\nNevertheless, I have some comments below.\n\n1) Although this paper brings a new perspective, based on optimal transport theory, as far as I can understand this paper does not establish formal new results. Thus I think some strong claims about providing deep theoretical explanation should be more moderate. In essence, it seems that the paper verifies *numerically* (in section B.3) that Figalli's theorem (stated in Appendix B) holds in this context.\n\n2) This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation could help.\n\n3) After Eq. (5) in the Appendix the authors mention Newton's method, and Thm 3 is also specific to Newton's method. Then they mention that *Gradient Descent* is used (and in the main part of the paper they mentioned Adam). This is confusing. All these algorithms are different, and Newton's method does not imply convergence results for gradient descent. I don't see how Thm 3 is relevant.\n\n4) This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the Brenier potential and is able to locate the singularities. I wonder if using a simpler approach through optimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve this issue? In the negative case, why not?\n\n5) Some Minor comments:\n1. Define OT in the abstract (Optimal Transportation?) \n2. What is AE? (not defined also; Auto Encoder?)\n3. There are lots of typos through the text, such as missing \"the\", \"a\", etc. \nand a couple mispelled words. I suggest the authors proofread the draft\nmore carefully.\n4. pp. 4 ... what is a \"PL convex function\". PL is not defined.\n",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "BJzuKiC9KX": {
        "paper_id": "iclr_2019_BJzuKiC9KX",
        "paper_title": "Revisiting Reweighted Wake-Sleep",
        "paper_abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow. \nThe work is well-written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables.\nThe reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger-scale real-world data and model would provide a more complete and convincing evidence. \nThe paper should be made stronger on at least one (and ideally both) of these accounts.\n\n",
        "meta_review_title": "Interesting empirical observations of the advantage of RWS, but lacking formal theoretical analysis, and larger scale experiments",
        "reviews": [
            {
                "review_id": "SJxOX9_p14",
                "reply_to": "SyeZgH_T1E",
                "title": "Experiments",
                "comment": "Note that our claim is not based only on the GMM experiment. It is also backed up by results from training (i) a VAE with continuous latent variable on MNIST data (compared against IWAE since VIMCO is not needed) and (ii) the AIR model on moving MNIST data (compared against VIMCO; VQ-VAE not applicable).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeZgH_T1E",
                "reply_to": "Skx2_YQ6kV",
                "title": "Real world experiments",
                "comment": "I second the reviewer suggestion of real-world experiments. Improvements on toy data-sets like GMMs do not necessarily transfer over to real world data. And if the authors make the claim that  \n\"Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models\", then they should back it up with results that match or improve state-of-the-art generative models like VQ-VAE/VIMCO in bits/dim on large scale, real data-sets.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Skx2_YQ6kV",
                "reply_to": "r1gDQE7nJ4",
                "title": "Thank you for reconsidering our paper",
                "comment": "There are two reasons why we think RWS should be used based on solid (although simple) theoretical reasoning. First, in discrete latent variable models, we don\u2019t need reparameterization, continuous relaxation or control variates. Second, in all models, RWS can be helpful in light of the \u201ctighter variational bound is not better\u201d effect. The GMM and AIR experiments support both points. The continuous VAE (on MNIST data) gives further evidence for the second point.\n\nWhy not compare to RBM and DVAE? We agree that even more evidence would be good. However:\n- For RBMs, this is an entirely different class of models (the joint density p(z, x) can be evaluated only up to a normalizing constant, instead of directly), which is not learnable using RWS or other ELBO-maximizing approaches (learning RBMs requires contrastive divergence or similar). \n- For DVAE, in addition to being slightly different in focus due to branching, it is also orthogonal in another way. DVAE can be used in conjunction with IWAE to tighten the bound. We show through the continuous VAE experiment that RWS can help. It might be interesting to see whether RWS can be used on the continuously relaxed model defined in DVAE in order to improve DVAE further.\n\nWhy not test on real-world data? We agree that the transfer from synthetic to real-world data is difficult. Our paper is methodological and RWS is in its core a statistical method for maximizing the log marginal likelihood and minimizing the KL divergence from p to q. Whether a model works on a real dataset is not a function of the learning and inference algorithms, but rather the particular generative model and inference network. This is also true for other learning and inference algorithms (like VAE, IWAE, REBAR, RELAX, DVAE, etc.).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJxxyGwinm",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "An interesting experimental paper but more insights are expected",
                "comment": "Main idea:\nThis paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   \nTo support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.\n\nClarity issues:\n1. \"branching\" has been used many times, but AFAIK, this seems not a standard terminology. What do \"branching on the samples\", \"conditional branching\", \"branching paths\" mean?\n2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence \n\"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x\". \nHowever, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. \n\nQuestions:\n1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?\n2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  \n3. In Fig 4, why WS doesn't achieve a better performance when K increasing?\n\nExperiments:\n1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. \n2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. \n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1gDQE7nJ4",
                "reply_to": "rJxb9BFYaQ",
                "title": "Somehow convinced but still expect more experiments",
                "comment": "\" the resulting discreteness cannot be used for directing the control flow\"\n\nAt least RBM doesn't need any continuous relaxation in training. \nTo test on those discrete applications which can be continuously relaxed during training is also important. It offers us a better understanding of when RWS should be applied.  I don't think your reasons are valid to not compare with RBM, DVAE etc. \n\nI still insist on real dataset/tasks, e.g. semantic segmentation, since there is always a gap between synthetic world and real world.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SylgzBTohX",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Revisiting Reweighted Wake-Sleep",
                "comment": "This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS.\n\nWhile this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rJxb9BFYaQ",
                "reply_to": "HJxxyGwinm",
                "title": "Response to AnonReviewer2",
                "comment": "Clarity issues:\n\nBy stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next. For example, in AIR, this decides when the program halts, in GMM the cluster index decides which likelihood function is evaluated. Another example of this are probabilistic context-free grammars (PCFGs) where discrete variables are used to describe which production rule is used (for example https://arxiv.org/abs/1806.07832). This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn\u2019t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data. (also see our general comment)\n\nWe will clarify the \u201czero-forcing\u201d failure mode and delta-WW in the updated manuscript.\n\nQuestions:\n\nTo estimate KL(q(z|x) || p(z|x)), we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples.\n\nThe statement that \u201cIWAE learns a better model only up to a point\u201d is justified by the IWAE curve in the middle of Figure 2: the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles. This is even more pronounced in Figure 1, where IWAE performance decreases for k > 10.\n\nIn Fig 4, WS actually does achieve better performance as K increases - the final value of the learning curve goes down, although only very slightly.\n\nExperiments:\n\nRegarding experiments, RBM/DVAE/++/# and VQ-VAE allow learning models with discrete latent variables in general; however, the resulting discreteness cannot be used for directing the control flow of a generative model (see also response to AnonReviewer3 and our general comment).\n    - In the DVAE family of algorithms, learning in discrete latent variable models is achieved by a continuous relaxation. This prevents using these variables as hard branching conditions.\n    - In the VQ-VAE algorithm, the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic. This limits the use of a discrete latent variable (cannot be used to model a cluster identity or stopping of a while loop). \n\nEven though we do not have experiments on large-scale real-world datasets, AIR is a non-trivial model, and using it can be seen as a large-scale experiment - taking several days (and several GPUs) to obtain results summarized in Figure 1. Similarly, to the best of our knowledge, ours is the first reported result of an MNIST model trained with IWAE with 512 particles.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJeBGBtFaQ",
                "reply_to": "SylgzBTohX",
                "title": "Response to AnonReviewer1",
                "comment": "The key formal justification is relatively straightforward: RWS, unlike IWAE, does not suffer from the \u201ctighter bounds\u201d problem. On the contrary, RWS uses self-normalized importance sampling to estimate the gradient with respect to \\phi. Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles. This means that increasing number of particles improves our gradient estimator and thus the optimization procedure.\n\nWe will explain this in more detail in the updated manuscript.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BylkLVYKa7",
                "reply_to": "BJepNja0hQ",
                "title": "Response to AnonReviewer3",
                "comment": "Baselines: As set out in our overall response, we aim to show that RWS is a better choice for inference in models that have stochastic control flow, where the choice from the discrete latent variables matters explicitly. In our GMM example, the cluster identity is such a choice, and in AIR, the stopping condition for the loop is another such choice. The work done in DVAE++, DVAE#, and other such approaches do not really handle this general class of problems well---by typically requiring enumeration of all possible branches and choices.\n\nGMM: We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript.\n\nTheoretical Rigour: We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript. Briefly, the justification for why RWS does not suffer from the \"tighter bounds\" problem is due to RWS's use of self-normalised importance sampling to compute the gradient of proposal parameters---resulting in both the asymptotic bias and variance decreasing linearly with number of samples.\n\nEmpirical Rigour: Our experiments strongly support our hypotheses:\n  a. Unlike IWAE, RWS performs better with more particles, both in terms of the generative model and inference network, and\n  b. It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation---something that requires expensive enumeration with continuous relaxations, or extremely finicky and unreliable construction with control-variate methods.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Syg9R7FF6m",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Discrete latent variables, stochastic control flow and probabilistic programming",
                "comment": "We thank the reviewers for their time and for appreciating our arguments about why RWS is preferable to IWAE-based approaches as we increase number of particles.\n\nWe would like to re-emphasize an important implication from our paper: RWS is a good inference algorithm for models that have _stochastic control flow_: i.e. models where latent variables can be instantiated dynamically, and different branching paths explored, based on the choice from a random variable[4-7]. Note that this is orthogonal to a large number of recent work that employ discrete latent variables in deep generative models[8-10]---where either the discrete variable is transformed via continuous relaxations, or, marginalized out entirely. The crucial difference is that none of these approaches address models where the model execution itself is determined via the discrete choices (if-statements or for-loops), as opposed to simply passing it through to a neural network---what is done with the discrete choice matters.\n\nThis is best illustrated in the domain of universal probabilistic programs (like Pyro [1]) which can contain arbitrary continuous and discrete latent variables, and where latent variables are instantiated dynamically and defined by running the program (or generative model). Stochastic control flow is a feature of such models [2] and allows the definition of expressive models, with potentially infinite number of latent variables [3], as mentioned in the discussion.\n\nUniversal (or higher-order) probabilistic programs form the largest family of samplable distributions and thus are a powerful tool to model data. Amortized inference and model parameter learning in such probabilistic programs, however, is typically only done using variational methods in the VAE/IWAE family of algorithms (as summarized in our manuscript). We\u2019re trying to say: RWS is a simple and often superior algorithm to use in this model family.\n\nOur point about probabilistic programs, hard selection and stochastic branching is illustrated by our choice of experiments (GMM and AIR). However, we will more strongly emphasize this point in the updated manuscript.\n\n[1] http://pyro.ai/\n[2] http://pyro.ai/examples/intro_part_i.html#Universality:-Stochastic-Recursion,-Higher-order-Stochastic-Functions,-and-Random-Control-Flow\n[3] Quote from [2]\n    \"For example, we can construct recursive functions that terminate their recursion\n     nondeterministically, provided we take care to pass pyro.sample unique sample names\n     whenever it\u2019s called.\"\n[4] GMM models in this work\n[5] Tree-structured latent variables in https://arxiv.org/abs/1806.07832\n[6] Memory-based models in https://arxiv.org/abs/1709.07116\n[7] AIR-like models in this work and https://arxiv.org/abs/1806.01794\n[8] DVAE++ - https://arxiv.org/abs/1802.04920\n[9] DVAE#  - https://arxiv.org/abs/1805.07445\n[10] VQ-VAE - https://arxiv.org/abs/1711.00937\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJepNja0hQ",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Nice experimental discoveries",
                "comment": "This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. \n\nThe experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, https://arxiv.org/abs/1805.07445. Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings.\n\nIn all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. \n",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "3R--2TdxMps": {
        "paper_id": "iclr_2021_3R--2TdxMps",
        "paper_title": "Defuse: Debugging Classifiers Through Distilling Unrestricted Adversarial Examples",
        "paper_abstract": "With the greater proliferation of machine learning models, the imperative of diagnosing and correcting bugs in models has become increasingly clear. As a route to better discover and fix model bugs, we propose failure scenarios: regions on the data manifold that are incorrectly classified by a model. We propose an end-to-end debugging framework called Defuse to use these regions for fixing faulty classifier predictions. The Defuse framework works in three steps. First, Defuse identifies many unrestricted adversarial examples--naturally occurring instances that are misclassified--using a generative model. Next, the procedure distills the misclassified data using clustering into failure scenarios. Last, the method corrects model behavior on the distilled scenarios through an optimization based approach. We illustrate the utility of our framework on a variety of image data sets. We find that Defuse identifies and resolves concerning predictions while maintaining model generalization.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The manuscript describes a method for identifying and correcting classifier performance when labels are assigned incorrectly. The identification is based on clustering classification failure regions in a VAE latent space and the correction phase is based on fine-tuning the classifier with additional synthetic samples from the VAE.\n\nReviewers agreed that the manuscript is not ready for publication. The main issue is that the suggested training method is similar to adversarial training methods used to gain adversarial robustness. The method does not help in debugging and fixing failures in general.\n",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "-oKqABtZVlB",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "The paper describes a technique for debugging classifiers through distilling unrestricted adversarial examples.",
                "comment": "The technique is described in sufficient detail and the paper is easy to read. Experimental results involving three datasets: MNIST, street view house numbers, and German traffic signs. The experimental results show that the proposed technique finds significant failures in all datasets, including critical failure scenarios. After correction, the performance of the method improves. \nAn interesting aspect of the method, which distinguishes it from similar techniques, is involvement of users/experts in the training process to indicate the classification errors in order to improve the performance of the method in the future. Engaging users in the training of classifiers has its advantages and disadvantages. For example, it can make easier to create \u201cpersonalised\u201d classification models that could be applied, e.g. in recommender system or information retrieval, where finding a perfect item depends on user\u2019s subjective perception of certain qualities. At the same time, user involvement in the training process can be tricky if it requires expert judgment as they may not always be available (as the authors demonstrated in the case of their third dataset consisting of German traffic signs). Further, involving user generated assessments requires well defined procedures in terms of requirement of assessors, determining the appropriate number of assessors, resolving disagreements between assessors, to ensure robustness of the final classifier. In the examples provided in the paper, the authors state that they used 5 workers (annotators) and the majority vote was used to decide the final label. What was the inter-annotator agreement? Since using human labellers is a crucial part of the proposed method, I would like to see more discussion of this aspect.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rZKvxsvSJdt",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "An interesting idea, but experiments and analysis do not support it as a significant contribution",
                "comment": "The paper proposes a method to identify and correct regions on the data manifold in which a trained classifier fails. The *identification* phase is based on clustering classification failure regions in a GAN latent space and the *correction* phase is based on fine-tuning the classifier with additional synthetic samples from the GAN.\n\nThe proposed method is strongly based on Zhao et al 2018 (Generating Natural Adversarial Examples), a method to generate on-manifold black-box adversarial examples using a GAN. The authors of the current paper describe some differences of their identification step from Zhao et al (end of section 3.2.1), but in my opinion they are minor.\n\nThe main contribution of the current paper over Zhao et al seems to be clustering the adversarial examples (using GMM) and using them to fine-tune the classifier. This, in my opinion, is potentially an interesting idea, however, the authors do not show sufficient evidence of its success. Specifically, the authors claim to \"achieve near perfect failure scenario accuracy with minimal change in test set accuracy\", but they do not provide any details (e.g. table of accuracy values on the train, test and adversarial sets before and after the fine-tuning). I would also expect to see an ablation study comparing the proposed method to simply including the adversarial examples found using Zhao et al (w/o GMM fitting and sampling) as additional training example - a standard adversarial defense approach (see e.g. [1]).\n\nPerhaps more importantly, the objective of the proposed method is not, in my opinion, clear. The title and abstract describe the goal as \"debugging\" a classifier and correcting fail regions, however the described method seems like a defense against on-manifold adversarial attack. If the method, as claimed, helps debugging and correcting the classifier, I would expect to see an improved accuracy on the (natural) unseen test set - not just on the synthetically generated adversarial examples.\n\nThe quality and clarity of the writing can be improved as well. A lot of space is allocated to describing well-known methods (e.g. VAE, GMM), however, critical information about the experimental results are missing. I'm also not sure all the formally defined algorithms and equations actually help in the understanding (e.g. algorithm 1, equation 2). Some of the mathematical notations are not standard.\n\nMinor comment: The norm in definition 3.1 is a regular vector norm (l2?) and not a matrix norm.\n\nTo summarize:\n\npros:\n- interesting idea (clustering on-manifold failures, labeling them and then using them to improve the classifier)\n\ncons:\n- contribution over Zhao et al not well established\n- insufficient and inaccurate experimental results\n- general quality of writing\n- not sure actual work and experiments match the stated objective\n- significance\n\n*Update:* Following the authors' response, I upgraded my rating, but I still think there are critical issues with the paper. The most problematic point, in my opinion, is the only-marginal improvement on the test data, indicating that the suggested training method only improves the specific \"failure scenarios\", making it is similar to adversarial training methods used to gain adversarial robustness. However, the abstract and introduction indicates that the paper helps in debugging in fixing failures in general, which, I think should have been evident in improved test accuracy.\n\n[1] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML 2019",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "R6Z858ZdVcb",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "Author response",
                "comment": "We thank all the reviewers for their useful comments.  We have responded individually to the reviewers below and made substantial changes to the paper. In summary we:\n\n1. Include more Defuse experimental details into the paper in section 4.1.  We additionally provide greater justification for our parameter and model choices in section 4.1 and in regards to the GMM in section 3.2.2.\n2. Reduce the description of known methods (VAE+GMM) and focus the writing more on our contributions.\n3. Provide more samples from the failure scenarios in figure 3 in the main text.  We emphasize one of our main contributions is the identification of failure scenarios.  The failure scenarios are useful because they summarize the unrestricted adversarial examples into key failure trends in the model.  We bring greater emphasis to this in the paper by highlighting more examples.\n3. Compare Defuse to fine tuning on the unrestricted adversarial examples as a baseline (per Reviewer 3's recommendation).  We highlight the Defuse finetuning and baseline results in tabular form in figure 4.  We see that Defuse improves the accuracy on the failure scenarios considerably compared to before finetuning and the baseline.\n4. Provide analysis of the annotator agreement in section 4.4.  We see the annotators voted for the majority label on average 85.2% and 82.1% of the time for MNIST and SVHN respectively across the annotated unrestricted adversarial examples. \n\nWe also note we had an issue with data storage that effected the MNIST experiments.  We thus reran these experiments with minimal change to our final results.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SUnCTtW-RMY",
                "reply_to": "rZKvxsvSJdt",
                "title": "Response 2",
                "comment": "We thank the reviewer for their useful comments and interest in the work.  In response to the reviewer\u2019s comments we\u2019ve revised a number of aspects of the paper. \n\n\u201c..but they do not provide any details (e.g. table of accuracy values on the train, test and adversarial sets before and after the fine-tuning)\u201d\n \n- While we previously provided these values in graphical form (what is now figure 5), we provide such a table in figure 4 of the updated paper to more easily parse the results.\n\n\u201cI would also expect to see an ablation study comparing the proposed method to simply including the adversarial examples found using Zhao et al\u201d\n\n- This is a useful point of comparison and thank the reviewer for the suggestion.  We add results finetuning only on the unrestricted adversarial examples.  The results can be found in figure 4.  We find the accuracy on the failure scenario testing data is considerably higher using Defuse than finetuning on the unrestricted adversarial examples.\n\n\u201cPerhaps more importantly, the objective of the proposed method is not, in my opinion, clear.\u201d \n\n- The objective of our work is to systematically find and correct model bugs. Defuse helps to do this through both identifying trends in misclassified data and offers a route to correct the predictions on such data.  See for instance figure 3 in our paper. In the upper right hand corner, a certain style of skinny 6\u2019s are misclassified as 1\u2019s.  This result is insightful for a model designer because it indicates the model struggles with very skinny numbers. Further, our finetuning results demonstrate we can adequate correct the model predictions on these mistakenly classified data indicating Defuse also successfully corrects the fault predictions.\n\n\u201cI would expect to see an improved accuracy on the (natural) unseen test set - not just on the synthetically generated adversarial examples.\u201d\n\n- The test set accuracy marginally improves for MNIST and marginally decreases for SVHN and German signs (figure 4).  We point out however that the important aspect of our work is that accuracy on the failure scenarios (which we have confirmed are model bugs through human verification) are corrected.  We see this is the case with Defuse.\n\n\u201cThe quality and clarity of the writing can be improved as well..\u201d\n\n- The reviewer is right to point out there a number of places to improve.  We have reduced the emphasis on VAE + GMM background and added more experimental details.  We have additionally moved the psuedo code for the algorithms to the appendix.\n\nAs a minor note, we use VAE\u2019s to perform all our experiments and do not use GANs as the reviewer indicates.  We would appreciate any further response the reviewer has to the above comments and revisions. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "L7JwiOoyX65",
                "reply_to": "-oKqABtZVlB",
                "title": "Reponse",
                "comment": "We thank the reviewer for their comments. The reviewer is right to point out that inter annotator agreement is an important aspect to consider.  We add additional details to our annotation process in section 4.1.  Further, we add section 4.4 describing the annotator agreement.  Please see the response to the first reviewer in regards to these details. \n\nWe would appreciate any further questions or comments on the new annotator results.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JSLF3PgD8QZ",
                "reply_to": "EDYhz-YIQb-",
                "title": "Response 1",
                "comment": "We thank the reviewer for their response and comments and appreciate the interest in the work. In response to the reviewer\u2019s points, we\u2019ve significantly improved the experimental detail in the paper. \n\nThe choice of certain algorithms and their parameters needs to be justified clearly:\n\n- We better justify the use of the Dirichlet process GMM in section 3.2.2. We point out there are two main requirements with our approach.  First, we must be able to infer the number of clusters from the data.  Second, we must be able to sample new instances from each of the clusters.  Both these requirements greatly limit the clustering approaches we can use. We use the Dirichlet process GMM because the dirichlet process nicely models the clustering problem where the number of clusters in unknown ahead of time satisfying our first criteria.  Additionally, we can sample new instances from the clusters satisfying our second criteria.  Though it could be possible to propose other bayesian clustering methods that meet both these criteria, we focus on evaluating Defuse with this particular choice of clustering method and leave evaluating other clustering methods up to future work.\n\nThe paper should be rewritten to have sufficient details of experiments in the text rather than delegating them to the Appendix A:\n\n- We\u2019ve moved many of the Defuse details from the appendix to section 4.1.  We additionally add further justification to our parameter choices.\n\nThe motivation of why certain parameters are chosen for experiments should be discussed. For example, \"we sample 10 instances from each cluster in the distillation step. We ask 5 workers to label the instance\" -- Why are these choices appropriate?\n\n- We\u2019ve added additional justification for our annotation choices in section 4.1.  In addition, we\u2019ve better motivated our parameter choices throughout section 4 in general. In response to this specific question, it is usually apparent the classifier disagrees with many of the ground truth labels within 10 instances, and thus, it is appropriate to label the cluster as a failure scenario.  For example, in figure 3 it is generally clear the classifier incorrectly predicts the data within only a few examples. Thus, 10 instances is a reasonable choice.  We ask 5 workers to label the instance in order to reduce the noise in the annotation process. \n\nDescription of the annotation task ought to be more detailed -- \"labeling them ourselves\" -- Who constitutes \"ourselves\"? \n\n- For MNIST and SVHN, we use annotator labels.  For German signs, we, the authors reviewed and assigned the failure scenarios.  Though this is less rigorous than the MNIST and SVHN experiments, it is still useful to see the classifier bugs exposed with Defuse.\n\nWhat is the agreement between the annotators?\n\n- We add section 4.4 describing the annotator agreement during labeling.  We generally find the annotators were in agreement about the labels.  For MNIST, the annotators voted for the majority label on average 85.2% of the time and 82.1% for SVHN over all the unrestricted adversarial examples.\n\nWe would appreciate any further reviewer comments or questions from the reviewer to the above responses and revisions. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EDYhz-YIQb-",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "The authors present DEFUSE a system for debugging classifiers using adversarial examples",
                "comment": "The authors present a system DEFUSE which is geared towards identifying and correcting classifier performance when labels are assigned incorrectly. There are three phases that are used to design DEFUSE: (1) Identify unrestricted adversarial examples using Variational Auto Encoders (2) Use a clustering approach to distill the above examples into failure scenarios and (3) Correct the classifier predictions.\n\nOverall, the idea of using adversarial examples to correct incorrect classifications is very interesting. \n\nThe choice of certain algorithms and their parameters needs to be justified clearly. While it is understandable that a non-parametric model be used for the clustering step, it it not clear why a dirichlet process is the best fit. How does this choice compare with other clustering approaches? Do the results generalize?\n\nThe paper should be rewritten to have sufficient details of experiments in the text rather than delegating them to the Appendix A. \n\nThe motivation of why certain parameters are chosen for experiments should be discussed. For example, \"we sample 10 instances from each cluster in the distillation step. We ask 5 workers to label the instance\" -- Why are these choices appropriate? \nDescription of the annotation task ought to be more detailed -- \"labeling them ourselves\" -- Who constitutes \"ourselves\"? What is the agreement between the annotators?\n\nMinor comments:\n\n1. Section 3.2: how we identity -> how we identify \n2. Section 3.2.3: The paragraph ends with \"For instance.\" The sentence needs to be completed and an example provided.\n3. Section 4.1: 32x32 should be replaced with 32X32. Similarly 128x128 should be replaced with 128X128",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test"
    },
    "HJe6uANtwH": {
        "paper_id": "iclr_2020_HJe6uANtwH",
        "paper_title": "Capsules with Inverted Dot-Product Attention Routing",
        "paper_abstract": "We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. \n      The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing.\n      When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it performs at-par with a powerful CNN (ResNet-18) with 4x fewer parameters.\n      On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer.  We believe that our work raises the possibility of applying capsule networks to complex real-world tasks.",
        "paper_acceptance": "accept-poster",
        "meta_review": "This work presents a routing algorithm for capsule networks, and demonstrates empirical evaluation on CIFAR-10 and CIFAR-100. The results outperform existing capsule networks and are at-par with CNNs. Reviewers appreciated the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets. In particular, removing the squash function and experimenting with concurrent routing was highlighted as significant progress. There were some concerns (e.g. claiming novelty for inverted dot-product attention) and clarification questions (e.g. same learning rate schedule for all models). The authors provided a response and revised the submission , which addresses most of these concerns. At the end, majority of reviewers recommended accept. Alongside with them, I acknowledge the novelty of using layer norm and parallel execution, and recommend accept.\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SklNZ1259S",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #4",
                "comment": "Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. Furthermore, they experiment with concurrent routing rather than sequential routing (route all caps layers once, then all layers concurrently again and again). This is an interesting development since provides better gradient in conjunction with layerNorm. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance.\n\nFirst, I want to point out that inverted attention is exactly what happens in dynamic routing (sabour et al 2017), proc. 1 line 4,5, and 7. In dynamic routing the dot product with the next layer capsule is calculated and then normalized over all next layer capsules. The only difference that I notice between alg. 1 here and proc. 1 there is replacement of squash with layer norm. There is no \"reconstructing the layer bellow\" in Dynamic routing as authors suggest in intro. \n\nSecond, the Capsules are promised to have better viewpoint generalizability than CNNs while having comparable performance. Replacing the 1 convolution layer with a ResNet backbone and replacing the activation with a classifier on top seems reducing the proposed CapsNet to the level of CNNs in terms of Viewpoint Generalization. Why should someone use this network rather than the ResNet itself? Fewer number of parameters by itself is not interesting, the reason it is reported usually is that it indicates lower memory consumption or fewer flops. Is that the case when comparing the baseline ResNet with the proposed CapsNet? Otherwise, a set of experiments showcasing the viewpoint generalizability of proposed CapsuleNetworks might only justify the switch between resnets to the proposed capsnets.\n\nThirdly, Fig. 4 top images seems to indicate all 3 routing procedures are following the same Learning Rate schedule. In the text it is said that optimization hyperparameters are tuned individually. Did authors tune learning rate schedule individually?\n\nForth, the proper baseline for the current study is the dynamic routing CapsNet. Why the multiMNIST experiment lacks comparison with dynamic routing capsnet?\n\nFor the reasons above, the manuscript in its current format is not ready for publication.\n\n------------------------------------------------------rebuttal\nThank you for your response. I acknowledged the novel contributions of this work. My comment was that some claims in the paper are not right. i.e. \"inverted dot-product attention\" is not new and \"reconstructing the layer bellow\" does not happen in Sabour et al . Parallel execution + layer norm definitely is novel and significant.\n\nRegarding the LR-schedule, I am not sure how fair it is to use same hyper-params tuned for the proposed method on the baselines. \n\nRegarding the viewpoint, the diverseMultiMNIST is two over lapping MNIST digits shifted 6 pixels. There is no rotation or scale in this dataset. An example experiment verifying the viewpoint generalizability of the proposed model is training on MNIST testing on AFFNIST. \n",
                "rating": 3,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BkelrkX5sB",
                "reply_to": "H1x0kqZJ5S",
                "title": "Reponse to Review #1",
                "comment": "We thank the Reviewer for their valuable feedback. \n\n[Code] \nWe will release the code. Reproducibility is our priority.\n\n[Higher Memory Usage than CNNs]\nTwo main reasons result in higher memory usage than CNNs. \n\nThe first reason is that we perform iterative routing, which means that we perform routing multiple times. Since CapsNet is a weight-tied architecture, the memory usage scales linearly with the number of iterations. The details can be found in the illustration in Figure 3 and the bar plot of memory usage in Figure 4. Note that this phenomenon is also observed in Deep Equilibrium Models (DEQs) [1]. Inspired by DEQs, a potential solution is to refer to a fixed-point optimization for finding equilibrium points of the routing updates. Then, we can enjoy the benefits of constant memory. \n\nThe second reason is that CapsNet uses the routing mechanism. As compared to the operations in CNNs, the routing mechanism performs agreement calculation between layers. This calculation introduces additional memory usage.However, we note that the routing mechanism is a dense operation, which means that we need to perform routing between all lower-layer capsules and all higher-layer capsules. We can instead randomly sample the capsules for routing, making the routing mechanism a sparse operation. We leave this dense-to-sparse modification as our future work.\n\n\n[1] \u201cDeep equilibrium models\u201d, S Bai, JZ Kolter, and V Koltun. NeurIPS 2019.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyxmfJXqir",
                "reply_to": "rygSM4-q9r",
                "title": "Reponse to Review #5",
                "comment": "We thank the Reviewer for the valuable feedback.\n\n[Learning Rate Scheduler]\nWe use the same learning rate scheduler for all three models. The learning rate degrades by 0.1 on the 150th and 250th epochs. There are two distinct points for both the EM and Dynamic routing models, yet the second point is more noticeable when zooming in the convergence plot in Figure 4.\n\nThe difference between all three models is the type of optimization method used. We use SGD for our model, and we use Adam for Dynamic and EM routing models. The type of the optimization method is selected to reach the best performance for each model. For example, SGD leads to worse performance than Adam for the Dynamic routing model. \n\nIn the original submission, we have included these details in Section A.1 in Supplementary.\n\n[Uniform Prediction in Table in Figure 4]\nFor Inverted Dot-Product Attention-A, when routing iteration increases to 5, we observe NaN values in neural network parameters. The prediction result becomes uniform across all classes. Since we consider a 10-class classification, the prediction accuracy becomes 10.00%. We rephrase the \u201crandom guess\u201d to \u201cuniform prediction\u201d in the revised manuscript. \n\n[Non-zero Pose Initilization]\nWe thank the Reviewer for raising the discussion about the capsule's initialization. As compared to 0 initialization, we observe that a random initialization leads to a similar converged performance but slower convergence speed. On the other hand, learning biases for capsules results in similar converged performance and same convergence speed. As a summary, we initialize the capsule's value to 0 for simplicity. We include the discussion in the revised manuscript.\n\n[Sudden Performance Jump in Convergence Plot in Figure 4]\nThe phenomenon of the performance jump is due to applying LayerNorm on the low-dimensional pose. To be more precise, the dimension of the pose used in the convergence plot is 16, and we apply LayerNorm to these 16 units. When increasing the pose\u2019s dimension, the jittering no longer existed. Nevertheless, we empirically find that it does not affect the model\u2019s prediction result once the model converges. \n\nIn the original submission, we have included these details in the last few sentences of the Convergence Analysis in Section 5.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1xGyJm9sH",
                "reply_to": "SklNZ1259S",
                "title": "Reponse to Review #4",
                "comment": "We thank the Reviewer for constructive feedback. We hope the following response will address the concerns of the Reviewer.  \n\n[Remarks on Inverted Dot-Product Attention Routing]\nWe agree that our routing method has similar components to Dynamic Routing (Sabour et al 2017), and we would like to emphasize their differences : 1) Sequential iterative routing is replaced with concurrent iterative routing, 2) Squash activation is replaced with Layer Normalization, and 3) We use cross-entropy loss instead of margin loss. The comparison is summarized in Section 4.3.\n\nWe humbly argue that these modifications are not trivial and stabilize the training, which leads to improved performance. For example, we observe that only our model has improved performance when the routing iteration number increases (CIFAR10 classification Table of Figure 4).\n============================================\nMethod   | Iteration=1 | Iteration = 3 | Iteration = 5\nDynamic  |    84.08%     |    82.88%       |    82.11%\nEM            |    58.08%     |    78.43%       |    31.41%\nOurs         |    84.24%     |    84.83%       |    85.09%\n============================================\n\n[Remarks on using ResNet]\nWe agree that using a deeper CNN such as a ResNet (vs. a single convolutional layer) to produce primary capsules makes our model inherit the disadvantages of CNNs (such as less view-point generalizability) and blunts the potential impact of capsules. However, at this stage, our intent is not to replace CNNs completely with CapsNets, but take a meaningful step towards building a routing mechanism that can at least do the job of the higher layers of a CNN. Previously proposed routing algorithms fail to do so and perform worse than their baseline CNNs.\n==================================================\nMethod                                                                      |  Accuracy\n-----------------------------------------------------------------------------\nDynamic routing with DenseNet backbone [1]  |   89.71%\n-----------------------------------------------------------------------------\nDynamic routing with ResNet backbone             |   92.65%\nEM routing with ResNet backbone                       |   92.15%\nOur routing with ResNet backbone                      |   95.14%\n-----------------------------------------------------------------------------\nOriginal ResNet                                                        |   95.11%\n==================================================\n[1] Phaye et al. \u201cDense and Diverse Capsule Networks: Making the Capsules Learn Better.\u201d Arxiv 2018.05.\n\n[Remarks on Memory Consumption]\n\n\uff37e agree with the Reviewer that reporting only the number of parameters may not be satisfying. Therefore, in Figure 5, we report the memory consumption comparisons between CapsNets and CNNs given the same model architecture. Please see the response to Reviewer #1, where we outline the reasons for why CapsNets consume more memory compared to CNNs even with fewer parameters, and we also suggest some possible solutions on reducing memory consumption, which we leave as our future work. \n\nWe also like to point out that the networks with fewer model parameters but larger runtime memory footprint may still be preferable for certain IC designs, where the L1 cache can store all the parameters.  \n\n[Learning Rate Scheduler]\nWe use the same learning rate scheduler for all three models. The learning rate degrades by 0.1 on the 150th and 250th epochs. In the original submission, we have included these details in Section A.1 in Supplementary.\n\n[Dynamic Routing Methods for DiverseMultiMNIST]\nWe provide the results for the Dynamic routing method by applying it on the DiverseMultiMNIST dataset. For a fair comparison, we consider the same optimizer, the same number of layers, and the same number of neurons per layer for the Dynamic routing method and the other methods. \n\nThe results are highlighted below (CapsNet denotes our routing method):\n================================================\nMethod    |  Pose Structure  | Test Acc.  |   # params.\n-----------------------------------------------------------------------------\nDynamic  |      vector              |  83.39%    |   42.48M \n-----------------------------------------------------------------------------\nCapsNet   |      matrix             |  80.59%    |   9.96M \nCapsNet   |      vector              |  85.74%    |   42.48M \n-----------------------------------------------------------------------------\n            BaselineCNN                |  79.81%    |   19.55M\n================================================\n\nCompared to the Baseline CNN, both our routing method and the Dynamic routing method achieve better performance on the DiverseMultiMNIST dataset. This result suggests a better viewpoint generalization from CNNs to the Capsule networks. Furthermore, our routing method outperforms the Dynamic routing one. We have updated our manuscript with these results. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1xoapM5iS",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Manuscript Revision",
                "comment": "We have updated the manuscript, and we highlight the additional results/ discussions in red.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1x0kqZJ5S",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #1",
                "comment": "In this paper, the authors propose a simple and effective routing algorithm for capsule networks. The paper is well written. A nice analysis of the proposed routing algorithm is provided. Experiments of varying the routing iterations demonstrate the stableability of proposed routing algorithm compared to others.\n\nHere are some issues:\n1. Would the authors release the code for reproducing the results in the paper? It will be helpful for future research in this area.\n\n2. In Fig.5, it would be better to give some brief explanations about why CasNet (Matrix) occupies much more memory while possessing less parameters.",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rygSM4-q9r",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #5",
                "comment": "\nThis paper presents a new simpler routing mechanism for capsule networks and achieves good performance on real world data sets making use of this new capsule structure along with a restnet backbone. Strong performance on the cifar10 and cifar100 datasets are presented and the network outperforms earlier versions of capsule networks. This new structure also performs well on an augmented MNIST dataset of overlapping digits (similar to the one used by Sabour et al 2017). \n\nOverall the paper is well written and presents solid results. The paper also presents a thorough comparison of two earlier versions of capsules which is a worthwhile contribution in its own right.\n\nThe paper could be improved by clearing up a few ambiguities:\n\n- is the learning rate schedule the same for all three models? in figure 4 it looks like the learning rate is decayed at two distinct points for your model, but only one distinct point for both the EM and Dynamic routing models.  \n\n-\"Notably, the prediction becomes random guess when the iteration number increases to 5.\" this sentence is a little confusing. Do you mean when the iteration number the performance is equivalent to not random assignments?  \n\n- This new algorithm requires that the capsules in L+1 have initialized poses with which to compare agreement between the poses in L. This is initial value seems like it may greatly effect the performance of the model. In the paper it is set to 0 and not expanded upon. It would be interesting to see if randomizing this value, or learning a bias for it would effect performance.   \n\n-unlike the two previous versions of capsules, the inverted dot product capsules show in figure 4 sudden huge decreases in test accuracy while training. These moments seem to be overcome quite quickly and the model ends up outperforming the other two. But it would be worth mentioning this behavior and perhaps attempting to explain it.\n",
                "rating": 8,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HylYeJdU_r",
                "reply_to": "BkxFhbtE_B",
                "title": "Response to Code Release",
                "comment": "We are cleaning the code, and planning to release it once it is ready.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxFhbtE_B",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Request for code",
                "comment": "Hi.\nI feel that this is a good step forward getting capsules closer to state-of-the-art on complicated datasets like CIFAR10.\nCould you please release the code ASAP.\nThanks.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "train"
    },
    "r1lUl6NFDH": {
        "paper_id": "iclr_2020_r1lUl6NFDH",
        "paper_title": "Mirror Descent View For Neural Network Quantization",
        "paper_abstract": "Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. NN quantization is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework (Bubeck (2015)) for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we discuss a numerically stable implementation of MD by storing an additional set of auxiliary dual variables (continuous). This update is strikingly analogous to the popular Straight Through Estimator (STE) based method which is typically viewed as a \u201ctrick\u201d to avoid vanishing gradients issue but here we show that it is an implementation method for MD for certain projections. Our experiments on standard classification datasets (CIFAR-10/100, TinyImageNet) with convolutional and residual architectures show that our MD variants obtain fully-quantized networks with accuracies very close to the floating-point networks.",
        "paper_acceptance": "reject",
        "meta_review": "The paper proposes to use the mirror descent algorithm for the binary network. It is easy to read. However, novelty over ProxQuant is somehow limited. The theoretical analysis is weak, in that there is no analysis on the convergence and neither how to choose the projection for mirror mapping construction. Experimental results can also be made more convincing, by adding comparisons with bigger datasets, STOA networks, and ablation study to demonstrate why mirror descent is better than proximal gradient descent in this application.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SJlXGL9j2B",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #4",
                "comment": "The paper proposes to use the mirror descent algorithm for the binary network. The key point is Theorem 3.1, which enables the mirror map. The paper is easy to read and follow, and the main contributions are clearly stated.\n\nHowever, I suggest a weak rejection of this paper. The reasons are\n\nQ1. As Review #3, it is better for authors to provide more theoretical analysis, which better includes the nonconvex objective function and the effect of annealing. \n\nQ2. It is not clear to me, why mirror descent is better than proximal gradient descent, i.e., proxQuant, in this application. The authors repeatedly claim \"MD allows gradient descent to be performed on a more general non-Euclidean space\". This cannot be told by Table 1, which is just overall performance. So, it is better to empirically show this point by an ablation study.\n\nQ3. Since the technical contributions are not enough, I expect more experimental comparisons.\n- Could the authors perform experiments on ImageNet?\n- While VGG and ResNet are taken as a protocol for experimental comparison, it is better to do an extra comparison with STOA networks. VGG and ResNet are too old and easy to be compressed, compression these networks are of little practical values. EfficientNet [1], Mobilenets [2], and Shufflenet [3] can be good ones. The paper will be more convincing with these methods.\n\n[1]. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n[2]. Mobilenets: Efficient convolutional neural networks for mobile vision applications\n[3]. Shufflenet: An extremely efficient convolutional neural network for mobile devices",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ryg5FaBptB",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #2",
                "comment": "This paper proposes a Mirror Descent (MD) framework for the quantization of neural networks, which, different with previous quantization methods, enables us to derive valid mirror maps and the respective MD updates. Moreover, the authors also provide a stable implementation of MD by storing an additional set of auxiliary dual variables. Experiments on CIFAR-10/100 and TinyImageNet with convolutional and residual architectures show the effective of the proposed model. \n\nOverall, this paper is well-written and provide sufficient material, both theoretical and experimental evidence to support the proposed method. Although the novelty of this work is somehow limited, i.e. appling MD from convex optimization to NN quantization, the authors provides sufficient effort to explore how to success to adopted it the literature. Hence, I lean to make an accept suggestion at this point. \n\nConcern: it would better to provide the code to validate the soundness of the model.\n\n##post comments\nThe rebuttal addresses my concerns and I will not change my score. Thanks.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Bkem9Mo9iB",
                "reply_to": "ryg5FaBptB",
                "title": "Thank you for the positive feedback",
                "comment": "We appreciate that the reviewer finds that our paper has sufficient material on both theoretical and experimental aspects. Below we address the reviewer\u2019s concerns.\n\n# Novelty\n- We appreciate that the reviewer acknowledges adopting MD for NN quantization has some challenges and our paper addresses them successfully (eg, time-varying mirror maps, deriving mirror maps from projections, numerically stable implementation using STE) and introduces the first practical MD based algorithm for NN quantization and demonstrate superior empirical performance against directly comparable baselines.\n\n# Code\n- We will provide the code upon publication and we have provided it for the reviewers and ACs in a separate confidential comment.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgjviP5sH",
                "reply_to": "r1gG6JwAYS",
                "title": "Thank you for the positive feedback",
                "comment": "We appreciate that the reviewer finds that our method is novel and interesting. Below we address the reviewer\u2019s concerns.\n\n# Writing suggestions\n- We agree with the reviewer\u2019s suggestions and we have appropriately revised the paper. \n- Regarding $y^0$, we would like to clarify that the first iterate of $y$ is $y^1$ and it is obtained using Eq. 4 where $x^0$ is initialized as discussed in the paper.\n\n# Experiment setup\n- As discussed in the paper, not all NN quantization algorithms are directly comparable to each other due to variations in the experimental protocol and considered quantization levels [1]. Since it is impossible to evaluate on all different experimental setups, following the recent publications [2,3], we compare against directly comparable baselines and we consider the extreme case of fully-quantized networks (ie, all learnable parameters are quantized). \n\n[1] Guo, Yunhui. \"A survey on methods and theories of quantized neural networks.\" CoRR (2018).\n[2] Bai, Yu, Yu-Xiang Wang, and Edo Liberty. \"Proxquant: Quantized neural networks via proximal operators.\" ICLR (2019).\n[3] Ajanthan, Thalaiyasingam, Puneet K. Dokania, Richard Hartley, and Philip HS Torr. \"Proximal Mean-field for Neural Network Quantization.\" ICCV (2019).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rke-y25fiB",
                "reply_to": "H1li8fEhYH",
                "title": "Summary of contributions and response to other comments {Response to R3 [2/2]}",
                "comment": "Below, we first summarize our contributions and then address the comments regarding convergence analysis and choice of projection.\n\n# Main contributions of our MD method\n- We would like to clarify that the main focus of the paper is to show that MD is a suitable framework for NN quantization and introduce a practical MD algorithm for NN quantization. To this end, our main contributions are summarized below:\n- We introduce the first practical MD based algorithm for NN quantization with time-varying mirror maps and demonstrate superior empirical performance against directly comparable baselines.\n- As MD updates are prone to numerical instability, we introduce a numerically stable version of MD and show that the popular STE method is an implementation method for MD under certain conditions on the projection.\n\n# Convergence of MD in the nonconvex setting\n- As mentioned in our submission, convergence analysis of MD in the nonconvex setting is an active research area [3,4] and MD has been recently shown to converge in the nonconvex stochastic setting under certain conditions [5]. This, together with empirical convergence plots (in Fig. 2) justifies the use of MD for NN quantization. We have appropriately cited [5] in the revised version and we believe convergence analysis of MD for NNs could be a completely new theoretical paper in itself.\n\n# Choice of projection\n- As stated in Theorem 3.1, if the projection is invertible and monotonically increasing, a valid mirror map exists and the corresponding MD algorithm can be derived. Moreover, to ensure fully-quantized networks we require the projections to be parameterized by an annealing hyperparameter $\\beta$ (Example 3.1). Nevertheless, choosing a projection that is guaranteed to yield improved quantization performance is an open problem.\n\n[3] Zhou, Zhengyuan, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter Glynn. \"Mirror descent in non-convex stochastic programming.\" CoRR (2017).\n[4] Zhou, Zhengyuan, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W. Glynn. \"Stochastic mirror descent in variationally coherent optimization problems.\" NeurIPS (2017).\n[5] Zhang, Siqi, and Niao He. \"On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization.\" CoRR (2018).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgAEhcMjH",
                "reply_to": "H1li8fEhYH",
                "title": "PQ is not based on MD and our MD method significantly outperforms PQ {Response to R3 [1/2]}",
                "comment": "Thank you for the feedback and we appreciate that the reviewer finds that our MD method is suitable for NN quantization.\n\nIn this reply, we clarify the novelty and significance of our MD method compared to ProxQuant (PQ) [1]. Meanwhile, responses to other comments will be provided in a subsequent reply.\n\n# Summary\n- Our main contribution of the paper is to show that MD is a suitable framework for NN quantization and introduce a numerically stable MD algorithm for NN quantization with superior performance compared to directly comparable baselines.\n- In this regard, we find the statement that our MD method is a \u201cnatural extension of PQ\u201d (ie, proximal gradient method or in general gradient descent where the $L_2$ norm is used) to be misleading and the differences are as follows. \n\n# MD vs PQ\n- The main and important difference between our MD method and PQ is that MD allows gradient descent to be performed on a more general non-Euclidean space (refer to Sec. 2) whereas PQ does not. To see this, we first give the update equations of PQ and MD below:\n- PQ: $\\tilde{x}^{k+1} \\gets x^k - \\eta g^k$ where $x^k = \\text{prox}(\\tilde{x}^k)$ and $g^k = \\nabla f(x)|_{x = x^k}$. Here, $x^k, \\tilde{x}^k \\in R$. (refer to Alg. 1 in [1]) \n- MD: $\\tilde{x}^{k+1} \\gets \\tilde{x}^k - \\eta g^k$ where $x^k = P(\\tilde{x}^k)$ and $g^k = \\nabla f(x)|_{x = x^k}$. Here, $x^k \\in B$ and $\\tilde{x}^k \\in B^*$, where $B^*$ is the dual space of $B$. (refer to Eq. 22 in the paper) \n- Notice that, PQ assumes the point $x^k$ and gradient $g^k$ are in the same space. Then only the formula $x^k - \\eta g^k$ is valid. This would only be true for the Euclidean space [2]. However, MD allows gradient descent to be performed on a more general non-Euclidean space by first mapping a primal point $x^k\\in B$ to a point $\\tilde{x}^k \\in B^*$ in the dual space via the mirror map. Such an ability is extremely beneficial in many problems (eg, simple constrained optimization) and it enabled theoretical and practical research on MD for the past three decades. Therefore, as mentioned in the paper (page 7) PQ is not based on MD.\n- Furthermore, it is clear from our experiments that MD significantly outperforms PQ (up to 20% in some cases when fully-quantized, refer to Table 1) demonstrating the importance of optimizing on a non-Euclidean space based on our MD framework.\n- Even though PQ hinted at the connection to the dual averaging version of MD and STE, it does not analyze the conditions on the projections under which corresponding valid mirror maps exist. This is important to show STE as a numerically stable implementation method for MD and such a link was previously lacking in the literature.\n- We have added this discussion in the revised version of the paper (page 7) to improve clarity.\n\n[1] Bai, Yu, Yu-Xiang Wang, and Edo Liberty. \"Proxquant: Quantized neural networks via proximal operators.\" ICLR (2019).\n[2] Bubeck, S\u00e9bastien. \"Convex optimization: Algorithms and complexity.\" Foundations and Trends\u00ae in Machine Learning (2015).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1li8fEhYH",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #3",
                "comment": "This paper proposes a neural network (NN) quantization based on Mirror Descent (MD) framework. The core of the proposal is the construction of the mirror map from the unconstrained auxiliary variables to the quantized space. Building on that core, the authors derive some mapping functions from the corresponding projection, i.e. tanh, softmax and shifted tanh. The experimental result on benchmark datasets (CIFAR & TinyImageNet) and basic architectures (VGG & ResNet-18) showed that the proposed method is suitable for quantization. The proposed method is a natural extension of ProxQuant, which adopted the proximal gradient descent to quantize NN (a.k.a $\\ell_2$ norm in MD). Different projections in NN quantization lead to different Bregman divergences in MD. \n\nHowever, the authors do not analyze the convergence of the MD with nonconvex objective function in NN quantization neither how to choose the projection for mirror mapping construction. Moreover, it is better to discuss with [Bai et al, 2019] to clarify the novelty of the proposed method. So I concern about the novelty and the theoretical contributions \n\nYu Bai, Yu-Xiang Wang, Edo Liberty. \nProxQuant: Quantized Neural Networks via Proximal Operators. ICLR 2019.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1gG6JwAYS",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #1",
                "comment": "A good paper that uses the Mirror Descent paradigm for learning quantized networks.  \nThough Mirror Descent is not their original idea, but using it in the context of learning quantized network is novel and interesting.  \nEmpirically, they showed better results than existing method, with comparisons with reasonable baselines including using relaxed projected gradient descent.  \n\nOverall, I don\u2019t have much concerns, but here are some more specific comment/questions (most relates to writing)\n\nIn the intro, it would be great to mention some past success on using MD, as opposed to just saying it\u2019s well-known. Also you mention MD can be used for more than quantization, but compression in general, it\u2019d be better to add that discussion, or remove this sentence. \n\nIn the beginning of Section 2.1, it'd be easier for the readers to make clear that the primal space corresponds to the quantized weights and the dual space corresponds to the unconstrained space in the rest of the paper.\n\nAt the top of page 3 you describe MD for the first time, but it\u2019s unclear to me how y^0 is handled.\n\nThe end of section 3 and section 4 talk quite a bit about STE, maybe it'd be clear if the authors can provide a concise description.\n\nAs someone not super familiar with NN quantization, this work seems like a good contribution.  My only possible concerns would be somehow comparisons to existing methods are not comprehensive enough (if this will be pointed out by the other reviewers)\n\n",
                "rating": 8,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "0cn6LSqwjUv": {
        "paper_id": "nips_2022_0cn6LSqwjUv",
        "paper_title": "RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling",
        "paper_abstract": "AI-for-science approaches have been applied to solve scientific problems (e.g., nuclear fusion, ecology, genomics, meteorology) and have achieved highly promising results. Spatial precipitation downscaling is one of the most important meteorological problem and urgently requires the participation of AI. However, the lack of a well-organized and annotated large-scale dataset hinders the training and verification of more effective and advancing deep-learning models for precipitation downscaling. To alleviate these obstacles, we present the first large-scale spatial precipitation downscaling dataset named RainNet, which contains more than 62,400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation downscaling. Specifically, the precipitation maps carefully collected in RainNet cover various meteorological phenomena (e.g., hurricane, squall), which is of great help to improve the model generalization ability. In addition, the map pairs in RainNet are organized in the form of image sequences (720 maps per month or 1 map/hour), showing complex physical properties, e.g., temporal misalignment, temporal sparse, and fluid properties. Furthermore, two deep-learning-oriented metrics are specifically introduced to evaluate or verify the comprehensive performance of the trained model (e.g., prediction maps reconstruction accuracy). To illustrate the applications of RainNet, 14 state-of-the-art models, including deep models and traditional approaches, are evaluated. To fully explore potential downscaling solutions, we propose an implicit physical estimation benchmark framework to learn the above characteristics. Extensive experiments demonstrate the value of RainNet in training and evaluating downscaling models. Our dataset is available at https://neuralchen.github.io/RainNet/.",
        "paper_acceptance": "Accept",
        "meta_review": "This paper describes SPDNet, a dataset for spatial precipitation downscaling. \n\nExperiments are provided using a fairly wide set of alternative methods - 14 models (including Kriging which is a widely used standard method in the meteorological community) - as well as a novel architecture proposed by the authors. The authors also extended SRGAN, EDSR, ESRGAN from Single Image Super Resolution (SISR) methods to Video Super Resolution (VSR) methods. While the level of innovation on the neural architecture side of the work is not extreme, clear value is provided in terms of contributions to neural architecture development. Reviewers felt that the dataset itself, the wide variety of models examined and the large set of evaluation metrics offers value to the community and that this dataset could help bring more interest to the problem domain. \n\nDuring the discussion period it was made clear that \"All relevant codes and datasets are open-source for research purposes\" and that \n\"The dataset and the code are not proprietary. We will build a dedicated github repository and website for users to easily use our datasets and codes.\" It is important that this is indeed is fully executed by the authors. \n\nThree of four reviewers recommended acceptance. \n\nFor all these reasons the AC recommends acceptance.",
        "meta_review_title": "Meta Review of Paper690 by Area Chair KSxd",
        "reviews": [
            {
                "review_id": "7hoTfc2gAyo",
                "writer": "author",
                "reply_to": "lnVp28VEsCi",
                "title": "To Reviewer WVYT",
                "comment": " We sincerely thank you for the review and comments.\n\nAlso thank you for acknowledging the value of our dataset.\n\nAfter discussions in our team, we thought that we should reduce the discussion of metrics and focus on metric that are very familiar to the computer field (such as RMSE).\nWe will add more content to introduce the dataset and benchmark itself so that this paper focuses on our propsed dataset and model.\nMore dataset-related details, as well as model design and training details, will be covered in the main text.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "lnVp28VEsCi",
                "writer": "official_reviewer",
                "reply_to": "iyh5uAQH1sE",
                "title": "Thanks for your response",
                "comment": " I'm sorry for the late response. The authors addressed some of my questions given the limited time. Thanks.\n\nIn the response to Q1, the authors also provide detailed explanation on the necessity of real low-res data and empirical study on the corresponding improvement. The response seems reasonable to me.\n\nAs for Q3 and Q6, the evaluation of performance on precipitation related tasks is still an open problem. E.g., DeepMind's Nature paper resorted to meteorologists for human evaluations due to the discrepancy between evaluations from experts and scores. It's not appropriate to include the intuitive designs of PEM/PDEM as one of the major contributions in this paper.\n\nOverall, the dataset and the corresponding benchmark are valuable. I suggest that the authors focus on them and remove the PEM/PDEM part in the paper.  The value of the dataset and benchmark will not be diminished by not proposing \"novel\" metrics. The proposed method does not necessarily have to outperform baselines in all concerned metrics.\n\n[1] Ravuri, Suman, et al. \"Skilfull precipitation nowcasting using deep generative models of radar.\" Nature 597.7878 (2021): 672-677.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Smp8psilatv",
                "writer": "author",
                "reply_to": "jjMUDMTFaxK",
                "title": "To reviewer 5jsP",
                "comment": " Dear reviewer 5jsP:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5wwckMiMQ2N",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To reviewer WVYT",
                "comment": " Dear reviewer WVYT:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses and results, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ndu1tUMJVku",
                "writer": "author",
                "reply_to": "WBmpyaytXL",
                "title": "To reviewer 9mB6",
                "comment": " Dear reviewer 9mB6:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest,\nAuthors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4N-CL_8pR09",
                "writer": "author",
                "reply_to": "9vOAx1JCX0r",
                "title": "To reviewer aP2Q",
                "comment": " Thanks for your valuable reply!\n\nQ1: Can we get a model that is better at domain specific scores by selecting the candidate models with PEM/PDEM and not MSE?\n\nA1: Thank you for your question. In spatial precipitation downscaling tasks, domain researchers typically use the metrics introduced in our paper to evaluate/select models instead of directly using RMSE [1]. The RMSE here is the pixel-level average error over all frames (i.e. 720 frames in a month), this averaging loses structural and dynamic information [1], which are properties of most interest to domain researchers. \"We could expect better PEM/PDEM performance when better RMSE performance is observed, but it might not always be the case.\"  For example, the model can reconstruct some frames very well and others very poorly (this often happens in heavy rain situations, e.g., hurricanes, continuous heavy rain, they occur almost every year), and the model can also get decent RMSE values, in this case, the model exhibits poor temporal consistency and dynamics. However, these issues can be captured by PDE/PDEM, which are more fine-grained metrics.\nIn other words, similar RMSEs may have different PDEs and PDEMs, for example, RCAN (RMSE\\times 100:0.325, PEM: 0.227, PDEM: 0.558) and EDVR (RMSE X100:0.329, PEM: 0.180, PDEM: 0.476) ). So simply using RMSE may cause models selection to fail.\nIn fact, CPMSE has similar functionality to RMSE.\nTherefore, it is entirely feasible to use PDE/PDEM directly for model selection or evaluation.\n\n[1]. Ekstr\u00f6m, Marie. \"Metrics to identify meaningful downscaling skill in WRF simulations of intense rainfall events.\" Environmental Modelling & Software 79 (2016): 267-284.\n\nQ2: For example, there might be other domain-specific scores that are missing in the benchmark, how should we incorporate these scores in the PEM / PDEM? \n\nA2: Thank you for your question. It can be added to PEM/PDEM by first normalizing and then weighting the metric to be added.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9vOAx1JCX0r",
                "writer": "official_reviewer",
                "reply_to": "nf3SWnPfY5",
                "title": "Thanks for the rebuttal",
                "comment": " Thanks for the rebuttal.\n\nRegarding Q1, I can understand the necessity for including a few domain-specific score functions such as  MPPE, HRRE, CPMSE, AMMD, HRTS, CMD. However, it is not clear why we need PEM / PDEM at this stage given that they are very consistent with the simpler MSE score. For example, there might be other domain-specific scores that are missing in the benchmark, how should we incorporate these scores in the PEM / PDEM? In fact, people may later adopt these metrics for model selection. Can we get a model that is better at domain specific scores by selecting the candidate models with PEM/PDEM and not MSE?\n\nRegarding Q2, thanks for agreeing to add vision transformers in the benchmark.\n\nRegarding Q3, thanks for the reply. The event types can be useful for further analyzing whether the SR algorithms are robust for different domains (i.e., meteorological events).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZIIO_BtU8bA",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To Reviewer WVYT part2",
                "comment": " Q3: There are no clues about the effectiveness of proposed novel compound metrics PEM and PDEM. It would be much more convincing to conduct empirical studies to prove that models achieving better PEM/PDEM demonstrate better ability on addressing some concerned issues.\n\nA3: Thanks for the comment. Here we are trying to make the metrics more applicable to meteorology society while also containing variables (e.g., RMSE) that are familiar to the computer science society. The metrics Precipitation Error Measure (PEM) and Precipitation Dynamics Error Measure (PDEM) are weighted over a series of metrics with clear physical meaning (reconstruction metrics: MPPE, HRRE, CPMSE, AMMD, and dynamic metrics: HRTS and CMD) and have been applied in downscaling research in meteorology society for a long time (may not in the same abbreviation). We\u2019ve mentioned these in the supplementary (section 3. Metrics) and have added an explanation to the main text. In supplementary section 3, we also discussed how each metric is calculated and what other literature employs the metric. This explains why better PEM/PDEM demonstrates a better ability to address downscaling problems (from a meteorology sense). For example (line 30-33 in supplementary), \u201cThe mesoscale peak precipitation error (MPPE; mm/hour) is calculated as the difference of top quantile between the generated/real rainfall dataset which considering both spatial and temporal property of mesoscale meteorological systems, e.g., hurricane, squall. This metric is used in most of these papers (for example [15, 10, 2, 6, 11, 14] (refs in our paper) suggest the quantile analysis to evaluate the downscaling quality)\u201d. To be noticed, in meteorology society, researchers tend to evaluate one downscaling algorithm with not a single variable but multiple variables together. However, it is always important to condense the information when bridging two fields. Here we weighted these variables into two to make comparing machine learning models easier for computer science society. \n\nQ4: Missing training details: Selected baselines are not designed for precipitation data. It is necessary to (at least slightly) modify and tune the models for fair comparison. However, there is no information about these details except for a single statement \"we also adjust the hype parameters of these models for better performance\" in Supl. Sec.5.2.\n\nA4:  Thanks for the question. The parts that need to be adjusted for these models include two parts\uff1a\n1. The hyperparameters required for training, which we have explained in line 309\\~313 of our paper. It is worth pointing out that typically SR models use a learning rate of 1e-4\\~5e-4, but we found that using 1e-3 is better for our task.\nWe have added more detailed instructions in Section 6.1 of the new version of our paper;\n2. The adjustment of the model, including the adjustment of the input channel and upsampling rate. We adjust the number of channels of input data for SRCNN, SRGAN, EDSR, ESRGAN, DBPN, RCAN, EDVR, RBPN, and our model to 1. We set the input data channel to 5 for SRGAN-V, EDSR-V, and ESRGAN-V. We set the upsampling rate to 3 for all models. \n\n\nQ5: Formatting errors.\n\nA5:  Thank you for your thoughtful suggestions. We had corrected \"e.t.c\" to \"etc.\". Careful corrections have been made to the language of our paper. \"L\" denots low-resolution and \"H\" denots the high-resolution. \"T\" represents the frame number. A detailed explanation has been added to Section 5 of the new version of our paper. We have revised the writing of Eq.1. We had highlighted 6 commonly used metrics in Table 1 of the new version of our paper.\n\nQ6: The qualitative results shown in Figure 4 are hard to distinguish. Could the authors provide more evidence to demonstrate that the models achieving better PEM and PDEM generate better predictions?\n\nA6: Thank you for your thoughtful suggestion. For this task, visualization is only an auxiliary means. The field of meteorology directly uses the quantitative metrics mentioned in our paper to measure the quality of model predictions, as described in A3. The two metrics PEM and PDEM describe overall performance over a period of time, while PDEM describes dynamic properties that are difficult to capture in static pictures. PEM and RMSE are usually positively correlated. More reflected in the visualization is the level of RMSE. It is worth mentioning that PSNR/SSIM/LPIPS visual effects are often indistinguishable in image super-resolution tasks. To improve the distinguishability of qualitative analysis results, we marked the PEM and RMSE corresponding to the visualization results to facilitate readers to distinguish. Furthermore, the discriminative regions in the visualization are marked with red boxes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iyh5uAQH1sE",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To Reviewer WVYT part 1",
                "comment": " Thank you very much for your interest in our work and for your golden suggestions. \n\nQ1: What are the advantages of using the real data collected by two different systems for training super resolution models than using simple downsampling algorithms.\n\nA1: Thanks for the question. When considering real-world meteorological problems, the downscaling algorithm trained on data collected from two different systems will be more helpful. As we mentioned (lines 57-59), \u201cContrary to image data, the proposed real precipitation dataset \u2026 shows the physical characters (e.g., temporal misalignment, temporal sparse and fluid properties, etc., that challenge the downscaling algorithms.\u201d The down-sampled dataset doesn\u2019t reflect these real-world problems. It is necessary to emphasize that the difference between high-resolution observation data and low-resolution observation data in the real downscaling task [1] is not simply the difference in resolution, but the difference in observation methods (e.g., satellite and radar). This situation is like different degenerate kernels (e.g., unknown and bicubic) in image super-resolution. SR models trained on bicubic degenerate datasets (e.g., DIV2K-bicubic) suffer severe performance degradation on the in-the-wild raw data [2,3,4]. On the other hand, many parts of the world are covered by multiple-resolution observations of metrological variables. How to unify them and how to organize them become an important question. When it comes to the two systems mentioned in this dataset, NLDAS (lower-resolution) covers 1980-now, and StageIV (higher-resolution) covers 2002-now. Developing a downscaling algorithm to transfer NLDAS to StageIV allows researchers to extend higher-resolution observations of metrological variables to a longer period, which helps to understand the climate change effect on precipitation. We\u2019ve added further explanations to the main text to explain the advantages of using the real data collected by two different systems.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\n[2]. Ji, Xiaozhong, et al. \"Real-world super-resolution via kernel estimation and noise injection.\" proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020.\n\n[3]. Hussein, et al. \"Correction filter for single image super-resolution: Robustifying off-the-shelf deep super-resolvers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[4]. Xu, Yu-Syuan, et al. \"Unified dynamic convolutional network for super-resolution with variational degradations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nQ2: The author should compare these two approaches empirically, e.g., demonstrate that models trained with real data are able to reconstruct better high resolution data and hence boost the performance on downstream tasks.\n\nA2: Thank you for your constructive comments. We use the bicubic method (Widely used to synthesize data) to downsample the high-resolution data (624*999) from 2002.7 to 2016.11 to low-resolution data (208 \u00d7 333), so that we generate a synthetic dataset. We employ this dataset to train our model from scratch, and use the original data from 2017.7\\~2017.11 as the test set. We report the test results in the table below:\n\n|  Approach   | MPPE &darr;  | HRRE &darr;  | AMMD &darr;  | CPMSE &darr;  | HRTS &darr;  | CMD &darr; | PEM &darr; | PDEM &darr; | RMSE X100 &darr; |\n|  ----  | ----  | ----  | ----  |----  | ----  | ----  | ----  | ----  | ----  |\n| Ours (real data)  | 4.198 | 221.859 | 0.191 | 1.890 | 7.723 | 9.568 | 0.197 | 0.441 | 0.312 |\n| Ours (synthetic data)  | 5.187 | 311.212 | 0.232 | 3.121 | 9.953 | 12.282 | 0.259 | 0.568 | 0.399 |\n\nIt can be seen from the table that the performance of the model trained on the bicubic synthetic dataset (row #3) is severely degraded. Therefore, the model trained with the real collected data has a great advantage in the task of spatial precipitation downscaling, and also confirms \"A1\". We have added the above experiment to the Sec.6.1 of the new version of our paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nf3SWnPfY5",
                "writer": "author",
                "reply_to": "yXth6737L-j",
                "title": "To Reviewer aP2Q",
                "comment": " Thank you very much for your interest in our work and for your valuable comments.  This would be an important work bridging meteorology and computer science. In this paper, we propose the first large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [a]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[a]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nQ1: It seems that RMSE is itself a very good summary metric. Thus, it is not clear why we will still need PEM / PDEM.\n\nA1: Thanks for the comment. We agree RMSE is an excellent summary metric and which is also very familiar to computer science society. For meteorology society, researchers usually use metrics that consider many kinds of meteorological phenomena. The metrics Precipitation Error Measure (PEM) and Precipitation Dynamics Error Measure (PDEM) are weighted over a series of metrics with clear physical meaning (reconstruction metrics: MPPE, HRRE, CPMSE, AMMD, and dynamic metrics: HRTS and CMD) and have been applied in downscaling research in meteorology society for a long time (may not in the same abbreviation). We could expect better PEM/PDEM performance when better RMSE performance is observed, but it might not always be the case. To make this dataset practical for computer scientists and meteorologists, here we introduce both PEM/PDEM and RMSE systems to benchmark the algorithms. \nFor details on calculating PEM/PDEM, we\u2019ve mentioned these in the supplementary (Section 3. Metrics) and have added an explanation to the main text. In supplementary Section 3, we also discussed how each metric is calculated and what other literature employs the metric. This explains why better PEM/PDEM demonstrates a better ability to address downscaling problems (from a meteorology sense). For example (line 30-33 in supplementary), \u201cThe mesoscale peak precipitation error (MPPE; mm/hour) is calculated as the difference of top quantile between the generated/real rainfall dataset which considering both spatial and temporal property of mesoscale meteorological systems, e.g., hurricane, squall. This metric is used in most of these papers (for example [15, 10, 2, 6, 11, 14] (refs in our paper) suggest the quantile analysis to evaluate the downscaling quality)\u201d. To be noticed, in meteorology society, researchers tend to evaluate one downscaling algorithm with not a single variable but multiple variables together. However, it is always important to condense the information when bridging two fields. Here we weighted these variables to two to make it easier to compare machine learning models and easier for computer science society to follow.\n\n\nQ2: Currently, the state-of-the-art image super-resolution model is based on vision Transformers (e.g., SwinIR) and the author need to reference the latest progress in this area.\n\nA2: Thank you for your constructive comments. We will definitely add the state-of-the-art transformer-based SR models (e.g., SwinIR) trained on our dataset to the benchmark models.\n\nQ3: The author mentioned that the dataset contains lots of different events such as hurricane, squall. Are the sequences in the dataset marked with the event type?\n\nA3: Thanks for the comment. Yes, we have provided event annotations such as hurricanes, squall lines for relevant frames.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bc26pvVJeJA",
                "writer": "author",
                "reply_to": "jjMUDMTFaxK",
                "title": "To Reviewer 5jsP",
                "comment": " Thank you very much for your interest in our work and for your valuable comments. This would be an important work bridging meteorology and computer science. In this paper, we propose the ***first*** large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [1]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nFor the technical novelty concern. \n\nOur work demonstrates its novelty in two aspects: \n\n1.The first large-scale open-source dataset for precipitation downscaling that is based on real measured data as described above, which will greatly help bridge the DL/ML community with meteorological science, while promoting the development of AI-for-Science.\n\n2.Novel benchmark model structure design and performance. Existing VSR methods generally include motion estimation modules, which are composed of modules (e.g., PCD in EDVR, Projection Module in RBPN, etc.) with strong video dynamics assumptions. As mentioned in our paper, the assumptions do not match precipitation downscaling. Unlike them, our implicit dynamic estimation module (IDEM) is a low inductive-bias module (e.g., transformers outperform CNNs), it only contains N-2 (N is the input adjacent frames, 5 frames in our model setting) weight-sharing small networks, so that IDEM can explore the inherent laws in the precipitation data without constraints/assumptions. In addition, self-attention, as a low inductive-bias operator, has achieved huge performance improvements in computer vision tasks (e.g., image classification, object detection, etc.). The low inductive-bias setting allows self-attention to fully explore the inherent laws within the data without being constrained by data assumptions [2]. At the same time, the self-attention operator also exhibits stronger generalization ability. Analogously, this is also the potential reason why our IDEM works better on the precipitation dataset. The results in Table 1 (in our paper) show the superiority of our model. Furthermore, our IDEM module also shows very competitive performance on the VSR data set: Vid4(4\u00d7) Average RGB PSNR 25.85 (EDVR Average RGB PSNR 25.83, DUF Average RGB PSNR 25.79). We will add more details (novelty analysis and performance analysis in VSR task) about the proposed model in Sec.5 and Sec.6.\n\n[2]. Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12873-12883.\n\nQ1: Eastern coast of US has been selected for data collections. What about other regions ?\n\nA1: Thank you for your constructive comments. There several reasons for selecting the eastern coast of US. \n1. Compared with other regions in the world, the US has systematic and complete observational data (NLDAS (lower-resolution) covers 1980-now, and StageIV (higher-resolution) covers 2002-now) of various resolutions from different observational systems (e.g., satellite, weather radar, etc.).\n2. Compared to the eastern US, the West Coast has very little precipitation, which is not helpful for our task, so we discarded the West Coast to reduce the redundancy of the dataset.\n3. In our future work, we will expand to more regions of the world.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8DMtDBXurHN",
                "writer": "author",
                "reply_to": "WBmpyaytXL",
                "title": "To Reviewer 9mB6",
                "comment": " Thank you very much for your interest in our work and for your golden suggestions. This would be an important work bridging meteorology and computer science. In this paper, we propose the first large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [1]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nQ1: SPDnet is not a very good name\u2026\n\nA1: Thank you for your constructive comments. SPDNet is a straightforward name derived from shorthand for \"Spatial Precipitation Downscaling\".\nWe believe a good name is very important to our work, so we will look for a better one.\n\nQ2:  \"It says in the checklist that the code and the data are proprietary. This needs to be clarified. What\u2019s the point of publishing a dataset if it cannot be used by others?\"\n\nA2:\nThanks for the comment. \n***All relevant codes and datasets are open-source for research purposes.***\nWe apologize for the error in filling out the checklist, we have changed the item \"Do you include licenses for code and datasets? [No] Code and data are proprietary\" to \"Do you include licenses for code and datasets?[ Yes] see Section 4.2\".\nThe dataset and the code are not proprietary.\nWe will build a dedicated github repository and website for users to easily use our datasets and codes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WBmpyaytXL",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "Official Review of Paper690 by Reviewer 9mB6",
                "comment": " There is not much to say here. The paper organizes a large precipitation dataset from both high res and low res sources and illustrates some baselines. The problem addressed is both interesting to ML (esp. to those interested in mixing physics and ml), and important. The question then, is; given that this data can be obtained directly from the original sources, is a new organization of it necessary and does it warrant a publication in an ML conference. Although I expect other reviewers will disagree, I have a positive view. Publishing this paper will probably increase interest in ML in this set of problems, and some in the community will find the dataset useful. See above SPDnet is not a very good name\u2026 It says in the checklist that the code and the data are proprietary. This needs to be clarified. What\u2019s the point of publishing a dataset if it cannot be used by others? ",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "jjMUDMTFaxK",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "Official Review of Paper690 by Reviewer 5jsP",
                "comment": " This paper presents a large scale dataset for spatial precipitation downscaling which contains more than 62, 400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation\ndownscaling. The precipitation maps  collected in the dataset  cover various meteorological phenomena such as hurricane and  squall. The data are organised in time series of maps, with 720 maps/month. Comprehensive metrics are also provided to evaluate the performances of models.  This paper is well written and brings comprehensive dataset for spatial precipitation. However  the technical novelty is low.\n Eastern coast of US has been selected for data collections. What about other regions ? The dataset lacks precipitation maps for several regions",
                "rating": 6,
                "confidence": 2
            },
            {
                "review_id": "yXth6737L-j",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "Official Review of Paper690 by Reviewer aP2Q",
                "comment": " The paper proposed a large-scale spatial precipitation downscaling dataset named SPDNet. The dataset contains more than 62400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, and covers more than 9 million squre kilometers of land area. The author also introduced 6 metrics that evaluate different aspects of the downscaling models, and 2 summary metrics that combine these 6 individual metrics. The author viewed the task as a video super-resolution problem and compared 14 methods. From the experimental results, the overall performance of the video super-resolution (VSR) models are better than Single-Image Super-Resolution (SISR) models.\n Stengths:\n\n1. The paper proposed the first large-scale precipitation downscaling dataset. This is an important scientific problem and a large-scale dataset can help move the area forward. In addition, the author pointed out the unique characteristics of the task such as temporal misalignment, temporal sparse, and fluid properties.\n2. The paper proposed 6 metrics for evaluating the models, including 4 reconstruction metrics that focuses on evaluating if the predicted high-resolution precipitation map matches the ground-truth, and 2 dynamic metrics that evaluate the dynamics of the predicted precipitation (via first order dynamics).\n3. The paper compared 14 models, including the Kriging method that has been widely used in the geospatial community, and other SISR and VSR methods. The author also extended SRGAN, EDSR, ESRGAN to be VSR methods.\n\nWeaknesses\n1. It seems that RMSE is itself a very good summary metric. Thus, it is not clear why we will still need PEM / PDEM.\n2. Currently, the state-of-the-art image super-resolution model is based on vision Transformers (e.g., SwinIR) and the author need to reference the latest progress in this area.\n 1. The author mentioned that the dataset contains lots of different events such as hurricane, squall. Are the sequences in the dataset marked with the event type?\n 1. The paper will be limited regarding the coverage of the baseline methods. However, it is difficult to cover all the latest image super-resolution methods so it is acceptable.\n\n",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "1mMfRqp4pty",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "Official Review of Paper690 by Reviewer WVYT",
                "comment": " This paper proposed a dataset consists of precipitation image sequences named SPDNet for spatial precipitation downscaling as well as a novel implicit dynamics estimation driven model. The proposed model as well as baseline models are evaluated on SPDNet with task specific metrics.  **Strengths**\n1. **Valuable dataset**: The large scale dataset SPDNet is in high resolution and in sequence, which is a valuable contribution to data-driven meteorological research.\n2. **Benchmark**: The authors have evaluate SOTA super resolution models on the proposed dataset ,and thus provide a good benchmark.\n\n**Weaknesses**\n1. **Necessity of low resolution data**: The authors claim in Introduction that the data obtained by simulated degradation is different from the real data collected by two different systems. However, there is no further discussion on it. It is insufficient to argue that their approach is better than obtaining low resolution data by simply downsampling the high resolution data. The author should compare these two approaches empirically, e.g., demonstrate that models trained with real data are able to reconstruct better high resolution data and hence boost the performance on downstream tasks.\n2. **Unconvincing evaluation**: There are no clues about the effectiveness of proposed novel compound metrics PEM and PDEM. It would be much more convincing to conduct empirical studies to prove that models achieving better PEM/PDEM demonstrate better ability on addressing some concerned issues.\n3. **Missing training details**: Selected baselines are not designed for precipitation data. It is necessary to (at least slightly) modify and tune the models for fair comparison. However, there is no information about these details except for a single statement \"we also adjust the hype parameters of these models for better performance\" in Supl. Sec.5.2.\n4. **Formatting errors**: There are some language errors like \"e.t.c.\" $\\rightarrow$ \"etc.\", and misleading notations in mathematical expressions such as missing brackets in Eqn.1, missing description of \"L\", \"H\", \"T\" in line 250. Scores of 6 commonly used metrics in Table 1 should also be highlighted. 1. What are the advantages of using the real data collected by two different systems for training super resolution models than using simple downsampling algorithms? If the two domains need to be similar, directly downsampling the high resolution data is both cheap and effective. If the two domains need to differ a lot from each other, it would be better to categorize the task as domain transferring instead of \"spatial precipitation downscaling\".\n2. The qualitative results shown in Figure 4 are hard to distinguish. Could the authors provide more evidence to demonstrate that the models achieving better PEM and PDEM generate better predictions?  Listed in **Weaknesses**",
                "rating": 4,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "ygWoT6hOc28": {
        "paper_id": "iclr_2021_ygWoT6hOc28",
        "paper_title": "Regression Prior Networks",
        "paper_abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD2), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD2 to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "awTxN3m2fOi",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "A simple extension of Prior networks models to regression.",
                "comment": "This paper extends Prior networks models, previously introduced for classification, to regression problems.  Prior networks are neural networks whose main target is to \"modelling uncertainty in classification tasks by emulating an ensemble using a single model\".  Standard Prior networks models output the parameters of a Dirichlet probability distribution. This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes. This hierarchical approach allows to better capture uncertainty. The presented approach extends this framework to regression tasks. So, instead of returning the parameters of a Dirichlet distribution, it returns the parameters of a Normal-Wishart distribution, which then defines a probability distribution over Normal distributions, and, in turn, each Normal distribution defines a probability distribution over the value of the target variable.  \n\n\nPros:\n* The presented approach is sound and addresses a relevant problem, which is modelling uncertainty for regression problems. \n* A method for distilling an ensemble model into a single model while maintaining accuracy is also proposed. \n* The proposed approach does not incur in computational and memory overheads like standard deep ensembles. \n* This work properly approaches technical difficulties (such as employing numerical stable precision parametrizations of the Normal-Wishart distribution) that arise in this kind of problems.\n\nCons: \n* The presented approach does not introduce any novel idea or insight. It's a relatively simple extension of a previously published method. \n* The empirical results do not show a clear advantage of the presented approach wrt previously published proposals. \n* The advantage of having a small computational and memory overhead is not properly evaluated with other proposals which also have a small  computational and memory overhead [1] (although this proposal has not been defined for regression problems, the adaptation to regression is as simple as the adaptation of the DeepEnsembles models employed in this work). \n\n\nI can not recommend the acceptation of this paper because I find the originality of the work quite limited. Although the extension of prior networks to regression task is mot really straightforward because of technical issues related to the problem of learning the parameters of a Normal-Wishart distribution. The general strategy to do that exactly matches the previous steps employed when introducing prior networks.  In consequence, this work does not provide any new relevant insight into the problem of modelling uncertainty and learning models with well-calibrated predictions. \n\n\nMinor comments:\n- Eq (14): T parameter is not defined. Temperature? \t\n- Typo at the end of Page 5: [-25,20] --> [-25,-20]\n- ENSM is defined after Table 1. \n- Fix the following reference:\nAndrey Malinin and Mark JF Gales. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness. 2019. \n\nPost-rebuttal:  I thank  the authors' effort for the improvement of the manuscript following the comments of the different reviewers. I think the overall quality of the paper has really improved. But, after many thoughts, I still think there is a limited novelty in this paper. I have increased my score to 5. But I can not recommend this paper for publication. \n\n  adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1]  Wen, Y., Tran, D., & Ba, J. (2020). BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. arXiv preprint arXiv:2002.06715.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "_GrpdkEvnoU",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Official Blind Review #1",
                "comment": "Prior Networks (Malinin & Gales, 2018) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks. This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity. The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data, UCI datasets and monocular depth estimation. \n\n_Strengths_:\n- The paper is well-written and clearly structured.\n- Most design choices are justified.\n- Simple idea (in a good way!) which seemed to work well, shown by the evaluation.\n\n_Weaknesses_:\n- Most of the work seems to be heavily based on Prior Networks (Malinin & Gales, 2018). Even Section 2.1 seems to be exactly like the Subsection in the paper about Prior Networks. This paper mainly focuses on an extension to the regression task. Therefore, the contribution / novelty of this paper is incremental. However, I still think the authors did a good job to present a general distillation method for regression task. Therefore, I would consider the novelty a minor weakness.\n- I am on the fence about specifying the OOD dataset for learning with the loss in Eq. 8. I believe it is difficult to decide what kind of model to use for generating the OOD dataset, thus, the model choice can lead to large differences in performance. This is not really discussed. Further, the models trained have more data available for training, I believe it is not quite fair to compare against models which only have been trained on in-domain-data.\n- There are no comparisons to other approaches for distillation of regression tasks. I understand, that this paper wants to show a viable general approach for regression distillation, however, this work is not the first one to do so and therefore should consider existing work.\n\n_Overall assessment_: For me, this paper is borderline. The weaknesses, especially the OOD dataset used for training and the lack of comparisons in the evaluation are concerns. However, I like the idea and the execution so therefore, I would recommend a weak accept (6).\n\n_Detailed comments and questions_:\n- OOD data: I have seen that you have an ablation for the degree of regularization on the OOD dataset. However, what about different OOD data? Why choose KITTY and not a different dataset? Were there any large difference in performance?\n- Table 3: I notice that NLL performance of distilled models are better than the actual ensemble, how can this be?\n- OOD detection for monocular depth estimation: Did you also trained the comparing models with the OOD data, e.g. DD?\n- Comparing models: Have you consider comparing your model to other ones, e.g. [1, 2]? This could improve your paper and approach to show that it also consider existing work on regression distillation.\n\n_Post-rebuttal_:\nI really appreciate the authors adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1] Chen, G., Choi, W., Yu, X., Han, T. and Chandraker, M., 2017. Learning efficient object detection models with knowledge distillation. In Advances in Neural Information Processing Systems (pp. 742-751).\n[2] Saputra, M.R.U., de Gusmao, P.P., Almalioglu, Y., Markham, A. and Trigoni, N., 2019. Distilling knowledge from a deep pose regressor network. In Proceedings of the IEEE International Conference on Computer Vision (pp. 263-272).",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "uJpIqmq2Bp5",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Final revision",
                "comment": "We added a final revision to fix a mistake in eq. 13 and fix a few typos.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yG1VHxH7Mzi",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Updated Manuscript Text",
                "comment": "Dear Reviewers,\n\nAs per the reviewers' and public comments, we have provided additional comparisons to Deep Evidential Regression and Mixture-Density Distillation, as we have described in our previous post. Now we have made changes to the text to reflect the reviewers' other comments. We hope that these changes sufficiently address all of your concerns.\n\nWe describe the changes section by section:\n\nIntroduction (minor changes)\n1. Modified second paragraph to mention evidential approaches\n2. Modified final paragraph to correctly point to roots of idea\n\nRegression Prior Networks (lots of changes)\n1. Sections styles changed for extra space\n2. Added clarifications into discussions of RKL loss function, including role of OOD data, and effect of beta.\n3. Added a discussion of EnD and MD-EnD to ensemble distribution distillation section\n4. Added a final \"Related work\" section, which discusses Deep Evidential Regression and efficient ensemble methods (batch ensemble.\n\nSynthetic Experiment\n1. X-axis range in images widened, so that it is clear from figure C that knowledge uncertainty rises sharply\n\nUCI Experiments (minor tweaks)\n1. Section reworked, experimental protocol clarified.\n2. Added reference to appendix which C3 which discusses PRR\n\nMonocular Depth Estimation (SIGNIFICANTLY reworked for clarity)\n1. Clarified the depth estimation performance metrics and made a forward reference to appendix D1, which they are described.\n2. Added experiments on DER and MD-EnD to both tables 3 and 4\n3. Restructured discussion of OOD experiments. Behaviours explains.\n4. Added forward reference to an additional set of OOD detection experiments in the appendix.\n5. Added forward reference to examples of IN/OOD data so that it is easy to see *exactly* what the models are trying to discriminate between.\n6. Expanded discussion about the difficulty of choosing appropriate OOD data, which highlights that EnD^2 is the superior approach, as it doesn't suffer from this difficulty.\n\nConclusion (Minor clarification at the end)\n\n\n\nWe thank all the reviewers for their effort!\n\nSincerely,\nAuthors\n\n\n\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wWmPr0CWGN-",
                "reply_to": "aLiVwZ4XDHC",
                "title": "Thank you for the stimulating discussion!",
                "comment": "We've updated the paper and are about to make a post describing all the updates. We've added a discussion about BatchEnsembles and the Multi-head distillation method.\n\nWe appreciate the effort you put into this discussion!\n\nMany thanks,\nAuthors\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aLiVwZ4XDHC",
                "reply_to": "V-QYbIwm34",
                "title": "Reply to Response to R4",
                "comment": "Thanks again for your nice replay!\n\nI think my concerns, in terms of comparison, with other related works are already addressed. The comparison with multi output heads is also reasonable and enough for me. Even though, I think it would be worth to also include the discussion of other methods like BatchEnsembles.\n\nRegarding novelty and relevance of the work, I still have the concerns I rose in my original review. But, I promise I will give new thoughts in the light of other reviewers and all your responses. \n\nThanks for the fruitful discussion. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EmqRkAeB_sh",
                "reply_to": "ZoIDbGzIXZw",
                "title": "Response to reviewer 2",
                "comment": "Thanks for you for your comments!\n\nWe explored the effect of gamma in the depth estimation setting in the appendix, table 11. The effect of beta is primarily to make the the expected (under Normal-Wishart) negative-log-likelihood be a tight bound to the NLL of the expected (vs expected NLL). High beta makes the bound tight and the training to be more accurate. We will add this this to the discussion and make it more clear.\n\nRegarding a discussion of Dirichlet Prior Networks - unfortunately, the space is rather limited to be able to discuss everything in detail. We will try to improve the discussion of RPNs such that it is more self-contained. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yQTOTbRfj0t",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "New results in manuscript",
                "comment": "Dear All,\n\nWe tried to provide new results in the open review system, but it turns out that it is very poor (and inconsistent across write/preview and what it actually posts) at representing tables. Thus, we have updated TABLE 3 and TABLE 4 in the manuscript, where we have added results for:\n\nDeep Evidential Regression (ArXiv 2019 version) (will only be displayed during rebuttal)\n\nDeep evidential Regression (NeurIPS 2020 pre-proceedings version) [2] \n\nMixture Density Distillation (only NYU Depth V2 so far, KITTI still training...) [3,4]\n\nNOTE - we are still in the process of updating the text. This update is purely intended to demonstrate updated results.\n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" ArXiv. 2019 (version 1)\n\n[2] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[3] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al). \n\n[4] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V-QYbIwm34",
                "reply_to": "QPCxH9YJxku",
                "title": "Response to R4",
                "comment": "We are very happy to provide a discussion of various alternative approaches to making ensembles computationally cheaper. Indeed, we\u2019ve found 2 papers on a similar approach to distilling an ensemble into a single model [1,2] by having multiple output heads, where each head is meant to replicate the behaviour of a particular ensemble member. We believe that this is as close a baseline as we can make - it is almost identical in compute to EnD^2, it's also a distillation approach, and it also attempts to preserve ensemble diversity. We have provided these results  in TABLE 3 and TABLE 4 of the updated manuscript. Generally, this works a little better for predictive quality than EnD^2, and worse for OOD detection. Results on Kitti for MDD are not ready yet, but are being calculated.  Do you find these results sufficient? \n\nRegarding BatchEnsembles - we\u2019ve had a closer look, and we currently don\u2019t actually understand how it is cheaper *at run time*. While it is true that a BatchEnsemble model has about as many parameters as a single model *on disk*, at *run time* it trades of increased use of GPU memory (batch is replicated) for efficient use of said GPU. Thus, it may be faster than sequential evaluation of an explicit ensemble, but it certainty is not more memory efficient at run time. We will certainly cite, mention and discuss this range of works. However, implementing it is non-trivial -the libraries you\u2019ve sent us are in Tensorflow, not Pytorch, so we cannot directly carry them over. \n\nIf you insist, we CAN promise to implement BatchEnsembles and add this into the camera ready paper (if this paper is accepted). We would definitely keep this promise, as it would be quite embarrassing to make it publicly and then break it.  \n\n[1] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[2] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wP-LoPyTYVB",
                "reply_to": "UjYwY1rS0MR",
                "title": "Experimental results",
                "comment": "We initially tried to add table here, but unfortunately it turns out that the system ignores formatting and just dumps numbers. Results are presented in TABLE 3 and TABLE 4 of updated manuscript. \n\nFor all versions of DER (2019 ArXiv and 2020 Neurips) we used a weight of 0.1 on the evidence regulariser. For DER 2020 we checked the implementation of the student NLL against the pytorch version and made sure that everything is correctly parameterised.\n\nThe results show a few things. \n\nFirstly, the old version of DER (ArXiv 2019) doesn't work well, both in terms of predictive performance, and in terms of uncertainty estimation. Which expected, due to the error on the loss.  \n\nSecondly, the new version of DER (NeurIPS 2020 preproceedings) works much better. In terms of predictive performance it is comparable to a single probabilistic DenseDepth model , though still with a minor degradation. In terms of OOD detection performance of models trained on NYU- it  yields rather competitive performance, marginally worse than the ensemble, and outperformed by EnD^2.\n\nThirdly, on KITTI OOD detection the LSUN OOD data is OOD not only because it is indoors, but also because it represents images which are very close to the camera, relative to images seen in Kitti, which features a range of depths. Here, all models, except RPN + RKL, interpret the OOD data as being in-domain using measures of total uncertainty. Using measures of ensemble diversity (knowledge uncertainty), ensembles, RPN-RKL and EnD^2 are able to detect OOD images successfully. Notably, DER does not seem to be able to. The reason for this is that the evidence regulariser biases the DER model tol yield high evidence in regions of low absolute error, and low evidence in regions of high absolute error. As a result, regions which are closer (bottom half of kitty images) always have higher evidence. As LSUN OOD is very close to the camera, the DER model yields high evidence. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UjYwY1rS0MR",
                "reply_to": "v2KrN3U9lHY",
                "title": "Response to A. Amini",
                "comment": "Thanks for your comment! \n\nRegarding Evidential Deep Learning, particularly [2] - we think it is a rather elegant alternative interpretation for uncertainty estimation, rooted in Dempster-Schafer Theory of evidence, which yields a model which is structurally identical to a Dirichlet Prior Network. However, we are sceptical of the principal claim that this is a reliable single-model uncertainty estimation approach which doesn\u2019t require OOD data or indeed any other approach to enforcing a particular behaviour for OOD inputs which has an understandable mechanism of action. However, that is only our opinion - a rigorous large-scale validation is necessary and clearly would be a useful future direction of investigation. \n\nRegarding your paper (congrats on getting into NeurIPS!). We see our work as being very much an extension and verification of the ideas proposed in [3]. We became aware of your work around the same time as we began ours, however, to the best of our knowledge (until your comment) it was a submission at ICLR2020. Furthermore, upon examination at the time, we determined that the loss function (expectation of square error given samples from the Normal-inverse-Gamma) had an error in the derivation (equations 7-9, derivation 7.1.2 eq. 22-23 in the appendix). As a result, we had no grounds on which to believe in the validity of the results. Looking at the ArXiv submission now - it has not been updated within the last year and still contains the error . \n\nWith respect to the experimental setup - while we both use UCI (which is standard) and NYU Depth v2, our evaluations are quite different. We have used a standard architecture, provided detailed performance comparisons to baselines in depth estimation [4,6], and analysed the properties of several uncertainty measures via ROC-AUC against a range of OOD datasets.\n\nHowever, we have now found your new NeurIPS2020 version in the pre-proceedings (which were released after the ICLR2021 submission deadline), and we see that the mathematical error has now been fixed. In fact, an altogether different loss function (NLL of the student distribution) is used in addition to the evidence regularizer. The results are largely the same. \n\nWe\u2019ve implemented DER both as it is on ArXiv and as it is in the NeurIPS2020 pre-proceedings, and present the results in the next post. Unfortunately, a direct number-for-number comparison to your work is not possible, as there are no summary performance results for your model, and figure 4B contains a range for RMSE which is about 20 times smaller than what is reported in the depth estimation literature [4,6] (you\u2019ve probably scaled something differently). Furthermore, you use a different OOD dataset which seems to be very easy to separate out, as all models achieve a ROC-AUC of about 0.99. \n\nWe will add these results to our paper (omitting the old DER), and cite your work (and the original Evidential work), in our paper. We shall upload an updated version shortly. \n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018.\n[3] Uncertainty Estimation in Deep Learning with Application to Spoken Language Assessment, Malinin, 2019. PhD Thesis\n[4] High Quality Monocular Depth Estimation via Transfer Learning (Alhashim & Wonka, 2018)\n[5] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[6] https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2 ,\n[7] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QPCxH9YJxku",
                "reply_to": "7Qx49nQWUAt",
                "title": "Reply to Authors comments",
                "comment": "Thank you for reply! \n\nI agree deep ensembles can be considered as an upper bound. My point is that there are alternative methods like BatchEnsembles, Rank-1 BNNs, SNGP, etc. (this repo https://github.com/google/uncertainty-baselines contains the references and high-quality open source implementations of all of them) which could be easily adapted for regression and which have much lower time and memory complexity than ensemble methods. In my opinion,  at least one of them should be considered here as relevant baseline, because  only comparing wrt deep ensembles gives the impression that your method is the only available alternative that provides  a big reduction in time and memory complexity  wrt deep ensembles. I think it is fair to show (or at least discuss) that there are other approaches that can be employed here to strongly reduce the memory and time complexity of deep ensembles. \n\nThe lack of novelty of this paper, as acknowledged by other reviewers, puts much more pressure in the empirical evaluation. As I said before, there are well-established prior works which directly address the high memory and time complexity of deep ensembles that can be easily adapted to regression and which, in my opinion, should be considered by this work. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "v2KrN3U9lHY",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Some missing related work",
                "comment": "Thanks for submitting this work! In line with many of the reviewer comments regarding novelty, I was also wondering about the relation of the proposed contribution to published evidential deep learning (EDL) approaches [1,2]. Namely, published at NeurIPS this year, Deep Evidential Regression [1] also proposes learning a 1D Normal-Wishart distribution directly to infer representations of uncertainty specifically in the continuous regression domain as well (not classification). The proposed contribution presented here, like [1], also provides experimental results on nearly identical tasks from UCI and on monocular depth estimation. Also, note that deep evidential networks are structurally identical to prior networks (PN), with the only differences being in their respective objective functions (PN additionally require OOD data to train with, EDL does not). Given that a preprint of [1] appeared over a year ago and is now peer-reviewed/published, as well as the foundational work done in the classification domain [2] is over two years old now, I think it would be very helpful for the authors to cite these papers and and discuss their contributions relative to these works. \n\nI also hope this will help orient reviewers to the context for this submission and perhaps to some contributions that may have been missed. \n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZoIDbGzIXZw",
                "reply_to": "FRAmiZWGSy",
                "title": "Response to Authors",
                "comment": "Thank you for your reply.\n\nWhat I was missing is a high-level description of prior networks. A background section on already existing prior networks for e.g. classification could be nice to include.\n\nRegarding the parameters, I was referring to the gamma parameter in Eq. (8) and the beta parameter in Eq. (11). ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "on7otAuRWqC",
                "reply_to": "6l-gyq0bIrO",
                "title": "Reply to Comments of Reviewer 3",
                "comment": "Thank you for your detailed comments! Allow us to address them:\n\nSEC 2.2 - \n  A. Z is indeed a constant that doesn't depend on the parameters of the model. We will make this clear. \n  B. I'm afraid that this isn't the case. The OOD loss doesn't regularise the choice of beta. Rather the OOD loss is supposed to inform the model of regions beyond which it has no understanding of the data. Clearly, this requires one to decide on and choose an OOD dataset, which is non-trivial. \nC. p(y | mu, Lambda) represents a Normal distribution sampled from the Normal-Wishart. \n\nSEC 2.3 - Yes, this is what I mean - the dataset can be seen an empirical distribution to which we minimise KL, or equivalently, maximise likelihood. Phi represents the parameters of the model into which we are distribution-distilling the ensemble. \n\nSEC 3.  ENSM is the Deep Ensemble. Respectfully, the behaviour of the estimates of data uncertainty out of domain is not relevant - data uncertainty is only important in-domain. Indeed, we cannot give any guarantees on the behaviour of data uncertainty in the OOD region. What we actually care about is that estimates of *knowledge uncertainty* increase as we move further out of domain, which is the case (though perhaps not so easy to see from the picture). We will update the picture to make this clearer.\n\nSEC 4   \nA. We will make the experimental protocol clearer in this section. We will add the description of the Prediction Rejection Ratio in the appendix. It shows what part of the best possible error-detection performance our algorithm covers. \n\nB. UCI datasets are very common datasets for evaluation in related works, that\u2019s why we decided to add them despite their simplicity.\n\nC. To obtain train-OOD-data for RKL, we used factor analysis with increased noise and latent variance. This is a simple generative model. We trained it on in-domain data and added noise to the latent variables to generate out-of-domain examples for RKL. This generative model is simple and appropriate for table data, while GANs are not usual for table data. Also, UCI datasets have few examples and small feature spaces, therefore it could be hard to train GANs on them.\n\nD. For the evaluation of OOD-detection performance, we took parts of other UCI datasets as OOD data. We made sure that the OOD-data comes from different domains and feature distributions are different. We felt that this was the best we could, as, to the best of our knowledge, there has been no established research on OOD detection for tabular datasets.\n\nSEC 5\n\nPerformance metrics in table 3 are usual for Monocular Depth Estimation. They describe model performance from different sides and are usually shown in all papers on this topic. A good description of these metrics can be found in the original Monocular Depth Estimation paper \u201cDepth Map Prediction from a Single Image using a Multi-Scale Deep Network\u201d by Eigen et al., in section 4.3. \n\nDelta 1,2,3 shows a percent of predictions such that the maximum of two fractions: (a) between predictions and targets, (b) between targets and predictions is less than corresponding thresholds: 1.25, 1.25^2, and 1.25^3. Rel stands for absolute relative error and log10 for RMSE between logarithms of predictions and targets. These losses show different properties of the model: deltas help to understand confidence intervals of the model, Rel shows the ratio between prediction error and target, and log10 shows error in the log-space. \n\nWe will add the definition of these metrics to the text and  attempt to simplify table 3 as much as possible.\n\nWe fully understand where you are coming from regarding table 4 and figure 3. We will rewrite this section and make it more understandable, it was hard to fit everything into a given space.\n\nRegarding Table 4 and the behaviour of the NWPN (RPN+RKL) model - we hypothesise this is the result of the interaction between the in-domain and OOD training data. It was very hard to get the models to appropriately train. Likely because discrimination between ID/OOD is a very global task (global scene understanding), while depth estimation requires more local data. The tasks are therefore anti-correlated in training. In contrast, EnD$^2$ doesn't suffer from the same problems and only relies on ID training data.\n\nThus, what we aim to show is that: 1) EnD$^2$ can appropriately replicate and surpass the ensemble's OOD performance. 2) NWPN (RPN+RKL) can sometimes do near-perfect OOD detection, but isn't as reliable in this particular task with this choice of OOD data. \n\n\nIn Figure 3 the left image is from KITTI and the right image is from NYU datasets. Using these images we aim to show that error of a prediction correlates with increased uncertainty of the model. Additionally, we wanted to show how the uncertainties of the ensemble and EnD$^2$ model compare, and we can see that the EnD$^2$ model consistently yields higher uncertainties, as it over-estimates the support of the ensemble.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "FRAmiZWGSy",
                "reply_to": "KMA7obppU9F",
                "title": "Reply to Reviewer 2 Comments",
                "comment": "Thank you for your comments! We will now address your comments point-by-point:\n\n1. Regarding alternative Bayesian baselines:\n\nThe approach we use to generate ensembles  - Deep Ensembles [4], are already the current go-to SOTA Bayesian approach to uncertainty estimation [1,2,3]. Approaches like Dropout, while also capable of generating ensemble, are shown to be consistently inferior. Variational Inference is typically even worse and has never been successfully scaled to complex tasks such as Depth Estimation, to our knowledge.\n\nOur favoured proposed approach - Ensemble Distribution Distillation for regression, allows us to take a SOTA DeepEnsemble (which is the baseline relative to which we compare) and distill it into a single model, generally preserving most of the ensemble\u2019s gains. This allows us to replicate both the ensemble\u2019s predictive performance as well as uncertainty measures at the computational and memory cost of a single model. Thus, suffer a minor reduction in predictive quality (and no loss in the quality of uncertainty estimates) for an M-fold (where M is the ensemble size) reduction in computational and memory cost relative to the ensemble baseline. \n\n       [1] Can you trust your model\u2019s uncertainty? Evaluating predictive uncertainty under dataset shift.\n\n       [2] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning.\n\n       [3] Deep ensembles: A loss landscape perspective.\n\n       [4] Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n\n\n2. Regarding additional training parameters - Could you please be more specific, so that we could address your concerns in detail?\n\n3. Regarding the difficulty of understanding the paper - Are there particular changes you would like us to implement which you think would make this paper more accessible? ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7Qx49nQWUAt",
                "reply_to": "awTxN3m2fOi",
                "title": "Reply to Reviewer 4 Comments",
                "comment": "Thank you for your review! Please allow us to address your concerns:\n\n1. Regarding empirical results:\n\n Could you please elaborate what you would see as a clear advantage? In terms of inference-time compute and memory the M-fold (where M is the ensemble size) advantage over Ensembles is clear. In terms of predictive performance - we outperform single models, and get close to the ensemble. Replicating the ensemble\u2019s predictive performance completely is an upper bound. In terms of OOD Ensemble-Distribution Distilled  RPNs outperform the ensemble. If there some specific comparison you would like us to provide which would convince you?\n\n2. Regarding BatchEnsemble:\n\nBatchEnsembles are interesting, however an efficient implementation of BatchEnsembles is non-trivial and there is no available code in pytorch (The original work was done in Edward). A naive implementation would be as expensive during inference as DeepEnsembles, if not more so, as it may require a larger ensemble to reach the same performance. If you insist, we will explore this approach, but this will likely be infeasible within the time-frame of the rebuttal period. \n\nP.S. Thank you for finding the minor errors. We will fix them. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DYGxsFPHho-",
                "reply_to": "_GrpdkEvnoU",
                "title": "Reply to Review 1 comments.",
                "comment": "Thank you for your review! Allow us to address your concerns on a point-by-point basis. \n\nREGARDING WEAKNESSES\n\n1. We agree with your concerns regarding the choice of OOD dataset. Defining an appropriate one for classification tasks is already non-trivial - doing so is even more challenging for regression. This is why we place greater emphasis on Ensemble Distribution Distillation - it does not require an OOD dataset and yields superior predictive performance relative to RKL-trained Regression Prior Networks.  \n\nWe will use the extra page to present a discussion regarding difficulties of using an OOD dataset, and will shortly upload an updated manuscript. \n\n2. With regards to regression distillation, we would like to point out that previous work has examined the distillation of a *single model  into a single model*. In our work we consider distillation of *an ensemble of probabilistic models into a single probabilistic model*. Limited prior work has examined this scenario, and it is difficult to provide a sensible baseline . We have attempted to do so through Ensemble Distillation (EnD), though it seems this approach also has its limitations. \n\nIt is, in general, not entirely clear whether combining an ensemble of probabilistic models is better done as an arithmetic or geometric mixture. A full analysis of ensembles of probabilistic regression models deserves an investigation of its own. Furthermore, to our knowledge, probabilistic ensemble distillation for regression has been a generally under-explored area. If you could point us to a more appropriate baseline, we would be happy to consider it!\n \t\nWe will add a detailed discussion of this issue into section 2.3 and upload an updated manuscript shortly. \n\nREGARDING DETAILED COMMENTS\n\n1 We were limited in the compute we had available for this project and decided to focus on the ablation study we did, rather than swapping out OOD datasets. In general, for Depth Estimation, we would like to place greater emphasis on RPNs trained through EnD$^2$, rather than RPNs trained via RKL on OOD datasets.\n\nIndeed, one of the conceptual reasons for not further exploring choice of OOD datasets for RPN+RKL is that we believe (and show) that RPNs+EnD$^2$ to be the superior approach. \n\nWe will clarify this point in an updated manuscript we will shortly upload. \n\n2. We believe this is a result of the fact that the EnD$^2$ will overestimate the support of the ensemble (as a natural consequence of ML training). As a result, it will be less over-confident. \n\n3. We didn\u2019t. To be clear - we intended our main comparison for Depth Estimation to be Ensembles vs  EnD$^2$ . Note that RPNs trained via RKL on OOD data in section 5 suffer degraded predictive performance. On the other hand, RPNs trained via  EnD$^2$ show better predictive performance (relative to EnD, Single models and RPN+RKL). \n\n4. : Thank you for pointing out this work. However, as previously stated, these papers consider the distillation of single model into single model, and thus cannot be used as a meaningful baselines. However we will cite them when discussing the nature of regression distillation and highlighting how our work is different.  \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "e8Fzb_fYVd",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Addressing concerns regarding novelty",
                "comment": "Dear Reviewers,\n\nAll of you have expressed concerns regarding the novelty and originality of our work. We would like to address this issue and explain why we think this merits a paper. \n\nIn our interactions with other researchers, and especially with industrial ML practitioners, we noticed that many people thought that the correct extension of Prior Networks to regression tasks would be to take a non-probabilistic regression model and place a Normal distribution over the target variable. As is clear from our work, this is not correct. Thus, one of the main motivations for this paper was to address this common misunderstanding and show that the correct way to extend Prior Networks and Ensemble Distribution Distillation to regression tasks.\n\nIn order to convey our message as clearly as possible we explicitly structured the paper around the parallel between Dirichlet and Normal-Wishart Prior Networks to make it absolutely self-evident what the correct approach is. In this regard we seem to have succeeded a little too well, as all of you note how the extension is straightforward and incremental. We would respectfully ask you to consider that this extension is not as evident to the majority of the ML community as we make it seem in this work. Notably, since the publication of the original paper on Dirichlet Prior Networks (Malinin and Gales, 2018), to our knowledge, Prior Networks have not been extended to regression, despite the popularity of the approach for classification. Thus, the value of our work is in extending a powerful uncertainty estimation approach for classification to regression, resolving a common misconception, and clearly presenting the mathematical basis for this extension. \n\nWe address your remaining concerns on a point-by-point basis and will shortly upload an updated manuscript.\n\nSincerely,\nAuthors\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KMA7obppU9F",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "An nice paper based on incremental work",
                "comment": "Summary of the Paper:\n\n        This paper introduces regression prior networks. These are models that aim at capture predictive uncertainty, both epistemic and aleatoric, in the context of regression problems. Regression prior networks can also be used to compress an ensemble of predictors into a single model while keeping the benefits of the ensemble. That is, better predictive performance and uncertainty estimates. The method is validated on several problems from the UCI repository and compared with ensemble methods.\n\nSpecific details:\n\n        I believe that this is a nice paper that illustrates an appealing method for uncertainty estimation in the context of neural networks. My main concern, however, is that it builds heavily on previous work. In particular, prior networks have already been proposed for classification and they have also been used to distill (compress) an ensemble. There is hence not much novelty here, only the extension to regression problems since, previously, only classification problems have been addressed. The use of prior networks for ensemble distillation is also not new. All this questions the novelty of the proposed approach.\n\n        The extension to regression seems to follow very closely the work already carried out for classification. The only difference is that a Normal Wishart distribution is used instead of a Dirichlet distribution.\n\n        The experiments carried out are extensive and consider different tasks involving prediction accuracy and out of distribution data detection. My main concern, however, is that no comparison is carried out with alternative methods to estimate prediction uncertainty such as those of Bayesian neural networks using variational inference or dropout. The authors should comment on the advantages of their method with respect to these techniques.\n\n        The method proposed is also complicated and has several training parameters. The authors give specific values for them, but it is not clear the motivation for them or the sensitivity to their values.\n\n        The paper is clearly written but heavily relies on previous work, making the reading difficult for someone who is not familiar with it. The paper is not self-contained.\n\n        Summing up I believe that this could be an interesting contribution for the conference, suffering from a reduced amount of novelty.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "6l-gyq0bIrO",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Clarifications needed",
                "comment": "This paper addresses interpretable uncertainty quantification for data driven models. In particular, the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only. The author contribution is thus clearly stated and positioned w.r.t. prior arts and tackle a non-trivial issue.\n\nIn the classification setting, the Dirichlet distribution is pretty much the universal model for the parameters of multinomial distributions. For regression, i.e. continuous r.v., there is no such universal solution and the authors chose to focus on outputs that have a normal distribution. The parameters of this latter are assumed to be normal-Wishart. Although, the proposed method is de facto non-applicable to other types of distributions, it can be argued that this already covers a majority of situations. \n\nThe paper is rather well organized and seems technically sound. This said, a few mathematical details are missing and, most importantly, the experiments are not very convincing. These concerns, also with other minor remarks are detailed below, section by section.\n\nsec 2.2\n\nMaybe give the explicit definition of Z to clarify that is does not depend on network parameters.\nThe presence of the OOD loss term in (8) is a bit artificial as it boils down to regularizing because of the choice of beta. Is this choice systematic ?\nIn (9), how is p(y | mu, Lambda) computed ? Is it a T distribution ?\n\n2.3\n\n(12) lacks clarity : dataset is equal to an empirical distribution... Do you mean p hat is a sum of Dirac ?\nWhat does phi represent ?\n\n3\nThe acronym ENSM is not explained. I believe this corresponds to the deep ensemble. \nThe Prior Networks achieve a form of disambiguation but the quality of it is a bit disappointing compared to ENSM. In particular, data uncertainty raises quickly for out-of-domain inputs. \n\n4\n\nThe presentation of the experimental protocol in 4 lacks clarity thereby impairing the interpretation of the results. The definition of the unconventional performance criteria [(Malinin et al. 2020] must be recalled (at least in an appendix). \nIn addition, as honestly mentioned by the authors, these datasets may not offer sufficiently rich problems to provide interesting comparisons. Besides, the way that OOD data is generated does not seem to necessarily produce inputs that are not covered by the in-domain distribution. Perhaps, the authors could use a \"bad GAN\" to obtain such data points, I mean a GAN where the generator and the discriminator would co-operate instead of being adversaries. If it converges, the generator would produce synthetic inputs that are easy to discriminate, thus far from true inputs. \n\n5\nWhile the dataset used in this section is more challenging, the experiment description is confusing. Again, performance criteria are not sufficiently explained and the general message becomes cryptic. Table 3 is overly complicated, I think RMSE is fairly enough to depict regression performances. Moreover, the definition of some columns are missing.\nIn Table 4, the performances of the methods seem quite unstable. For example, NWPN works fairly well for a given dataset configuration for one knowledge uncertainty criterion but fails miserably using another criterion on the same data.\nOn Fig 3, from what dataset are these image coming from ? Why are these or that object presumably \"unknown\" to the model ?\nI think the whole section deserves some re-writing.\n\nFinal remark : there are a few English mistakes that should be wiped out. \n\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "9U4gLR_lRP": {
        "paper_id": "nips_2022_9U4gLR_lRP",
        "paper_title": "Logit Margin Matters: Improving Transferable Targeted Adversarial Attack by Logit Calibration",
        "paper_abstract": "Previous works have extensively studied the transferability of adversarial samples in untargeted black-box scenarios. However, it still remains challenging to craft the targeted adversarial examples with higher transferability than non-targeted ones. Recent studies reveal that the traditional Cross-Entropy (CE) loss function is insufficient to learn transferable targeted perturbations due to the issue of vanishing gradient. In this work, we provide a comprehensive investigation of the CE function and find that the logit margin between the targeted and non-targeted classes will quickly obtain saturated in CE, which largely limits the transferability. Therefore, in this paper, we devote to the goal of enlarging logit margins and propose two simple and effective logit calibration methods, which are achieved by downscaling the logits with a temperature factor and an adaptive margin, respectively. Both of them can effectively encourage the optimization to produce larger logit margins and lead to higher transferability. Besides, we show that minimizing the cosine distance between the adversarial examples and the targeted classifier can further improve the transferability, which is benefited from downscaling logits via L2-normalization. Experiments conducted on the ImageNet dataset validate the effectiveness of the proposed methods, which outperforms the state-of-the-art methods in black-box targeted attacks. The source code for our method is available at https://anonymous.4open.science/r/Target-Attack-72EB/README.md.",
        "paper_acceptance": "Reject",
        "meta_review": "In this paper, the authors propose novel method to improve transferability of targeted adversarial attacks by enlarging the margin between targeted logit and non-target logits.  Experiments on ImageNet with different methods demonstrated the effectiveness of the method. However, as is pointed out by the reviewers that there exist high overlap between the paper and the existing works, which significantly hinders the novelties of the paper. The paper are expected to clarify the novelty and provide more comprehensive evaluations. \n\n\n",
        "meta_review_title": "Meta Review of Paper1795 by Area Chair PGox",
        "reviews": [
            {
                "review_id": "04OYiCm6jg",
                "writer": "official_reviewer",
                "reply_to": "7eZgZ-rivOg",
                "title": "Thanks for the authors' response.",
                "comment": " I would like to thank the authors for their response and have checked the revised version. I agree with Reviewer WQZv that the change of the current version is falling into a major revision. In particular, I would like to highlight the high overlap between the previous submitted manuscript and the existing work. Therefore, I remain my previous rating and still vote for reject.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "VacpnSA35Nc",
                "writer": "author",
                "reply_to": "jNXvtEGBFMIS",
                "title": "Summary of Revision",
                "comment": " Thanks for your comments and constructive feedback. We have uploaded a revision to address the concerns. The notable changes are below.\n\n1. We revised the introduction section to clarify the distinction between the Logit [30] and moved the logit margin figure (Fig. 1) to the introduction to better illustrate the motivation and contribution of this study (Section 1).\n\n2. We thoroughly rewrote the related work section to address the similarity issue pointed out by the reviewer qX4X (Section 2).\n\n3. We added the experiments on the varied targets and T=10/20 in the combining logit calibrations. Besides, the suggestion for achieving a better-targeted attack is added (Section 4).\n\n4. To better present the tables, we reported the average targeted transfer success rate with three digits at most for Tables 1, 2, & 3 instead of the average number of successfully attacked samples with four digits (Section 4).\n\n5. We carefully polished the manuscript and corrected some typos and grammar mistakes.\n\n6. More experiment results during the rebuttal period are added into the supplementary.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sT14tZIeoddW",
                "writer": "author",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Summary of Revision",
                "comment": " We thank the reviewers for their positive comments and constructive feedback. We have uploaded a revised manuscript based on the reviewers\u2019 feedback and have highlighted changes from the original submission in blue. We summarize the notable changes below.\n\n1. We revised the introduction section to clarify the distinction between the Logit [30] and moved the logit margin figure (Fig. 1) to the introduction to better illustrate the motivation and contribution of this study (Section 1).\n\n2. We thoroughly rewrote the related work section to address the similarity issue pointed out by the reviewer qX4X (Section 2).\n\n3. We added the experiments on the varied targets and T=10/20 in the combining logit calibrations. Besides, the suggestion for achieving a better-targeted attack is added (Section 4).\n\n4. To better present the tables, we reported the average targeted transfer success rate with three digits at most for Tables 1, 2, & 3 instead of the average number of successfully attacked samples with four digits (Section 4).\n\n5. We carefully polished the manuscript and corrected some typos and grammar mistakes.\n\n6. More experiment results during the rebuttal period are added into the supplementary.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jNXvtEGBFMIS",
                "writer": "official_reviewer",
                "reply_to": "sIAr738LGW9",
                "title": "Official Comment by Paper1795 Reviewer WQZv",
                "comment": " Thanks for providing the response.  I am worried all of these changes are falling more into a major revision and would not be within the limit of the conference. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sIAr738LGW9",
                "writer": "author",
                "reply_to": "OyP0Pw8xB5",
                "title": "Results on another dataset, attacking Google API, and transfer with varied targets",
                "comment": " >**Exp 1:** Experiments on another dataset (e.g., CIFAR-10, MNIST, SVHN)\n>\n>**Response 1:**  During the rebuttal, we conducted the experiments on the CIFAR-10 dataset under the untargeted attack setting based on the code provided by [a]. The ResNet-18 is used as the white-box model for crafting the perturbation by training with the I-FGSM for 20 iterations. The DenseNet, GoogLeNet and SENet18 are black-box models. Table 1 reported the fooling rate of attacking the 10,000 images in the CIFAR-10 testing set. \n>\n>From Table 1, we can find that the fooling rate continually increases along with the T in the white-box attack. In transfer black-box attacks, the best fooling rates are obtained at T=5 or T=10, and the fooling rate will decrease when further increases T. These results also can validate the effectiveness of logit calibration in non-targeted attacks on a small dataset.\n>\n>[a] Enhancing Adversarial Example Transferability with an Intermediate Level Attack, *ICCV 2019*.\n\nTable 1: The transfer untargeted fooling rate of training with ResNet-18 and testing by the DenseNet-121, GoogleNet and SENet-18 on CIFAR-10.\n|       | ResNet-18*| DenseNet-121 | GoogLeNet   | SENet-18 |\n| -     | :-:  | :-:  | :-:  |:-:   |\n|T=0.5  |89.77 |50.23 |37.43 |51.04 |\n|T=1    |91.61 |50.78 |37.30 |51.20 |\n|T=2    |91.39 |51.14 |37.60 |51.65 |\n|T=5    |92.01 |55.56 |41.77 |55.74 |\n|T=10   |94.04 |54.76 |42.41 |55.10 |\n|T=20   |94.20 |53.33 |41.31 |54.11 |\n\n>**Exp 2:** A real-world attack on the Google Cloud Vision API.\n>\n>**Response 6:** We randomly select 100 images and compute the attacking performance of the ensemble of four CNNs using the same evaluation protocol in [30]. The results are as follows. We can find that the results of the Logit and CE (T=5) are very similar. But the Margin-based calibration performs worse than Logit and CE (T=5).\n\nTable 2: Non-targeted and targeted transfer success rates (%) on Google Cloud Vision.\n|       | Logit | CE (T=5)   | Margin |\n| -     | :-:  | :-:  |  :-:   |\n| Targeted    |  16| 15  | 12 | \n| Non-targeted| 51 | 53 | 42 |\n\n>**Exp 3:** The targeted success rates for transfer with varied targets.\n>\n>**Response 3:** The targeted transfer success rate with varied targets is reported in Table 3, and we can have the following findings. **(1)** The three types of logit calibration methods can improve the targeted transfer success rate over the original CE. The angle-based calibration has the best performance. But, we notice that the margin-based calibration doesn't work well in this setting. **(2)** The Temperature-based (T=5, 10) and the Angle-based calibrations can outperform the Logit loss by a large margin, especially the Angle-based calibration.\n\nTable 3: Targeted transfer success rate (%) when varying the target from the high-ranked class to low. (Average of 5 times)\n|      | 2nd  | 10th | 100th | 200th | 500th | 800th | 1000th |\n| :-:  | :-:  | :-:  | :-:   | :-:   | :-:   | :-:   |  :-:   |\n|Logit | 83.7 | 83.2 | 77.3  | 74.5  | 71.5  | 64.9  |  52.4  |\n|CE    | 77.4 | 58.6 | 34.0  | 26.9  | 23.7  | 16.7  |  7.0   |\n|CE/5  | 91.3 | 88.7 | 80.7  | 77.1  | 75.8  | 70.1  |  58.8  |\n|CE/10 | 89.0 | 87.8 | 82.8  | 81.0  | 79.2  | 73.5  |  62.5  |\n|Margin| 87.4 | 81.7 | 67.4  | 61.3  | 51.6  | 43.1  |  23.0  |\n|Angle | 92.4 | 89.1 | 82.2  | 80.3  | 79.2  | 76.1  |  66.3  |\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "OyP0Pw8xB5",
                "writer": "author",
                "reply_to": "JcXe9C2n82K",
                "title": "Response to Reviewer WQZv ",
                "comment": " Thanks for your comments and valuable suggestions for the presentation. We would like to address your concerns in the following aspects.\n\n>**Comment 1:** Clarified the distinction between [30] in the introduction. \n>\n>**Response 1:** Thanks for your suggestion. We will rewrite this part in the introduction to better clarify the contributions of this study.\n\n>**Comment 2:** To have a better presentation of the tables.\n>\n>**Response 2:** We agree that a better presentation of the tables is needed. Currently, we use tables instead of line graphs that capture progress through iterations mainly due to the results of different calibration methods being very similar, and their lines will largely overlap with others. In the revision, we will report the average targeted transfer success rate with 3digits at most for Tables 1 & 3 instead of the average number of successfully attacked samples with 4 digits, and replace Table 2 with line graphs.  \n\n>**Comment 3:** Why not introduce a single best receipt and present everything else as ablations? \n>\n>**Response 3:** The main reasons are: **(1)** The primary goal is solving the saturated issue in the CE loss for learning better transferable targeted adversarial attacks. Therefore, we first evaluate the effectiveness of different logit calibrations. **(2)** Since the logit calibration works, we then test their mutual effects by combining them jointly. However, we find that the optimal combination is different for different models, and there isn't a universal receipt for them. Consequently, we didn't introduce a single best receipt and present else as ablations.\n>\n>On the other aspect, based on the results in the manuscript and new results in the rebuttal, we might suggest using (T=5 + Margin) or (T=5 + Angle) for CNNs with more layers and the single Margin-based calibration for CNNs with fewer layers. \n\n>**Comment 4:** Why $T=5$ +Margin or Angle in Table 3? \n>\n>**Response 4:** We currently use the same $T=5$ mainly based on the result of ResNet-50, instead of the optimal $T$ for each model. During the rebuttal, we test the performance of $T=10,20$ + (Margin or Angle). The results are reported in Table 1. We can find that the $T$ has a marginal influence in the combination of \"$T$ + Margin\", while largely increasing the performance of \"$T$ + Angle\" of VGG16. \n\n**Table 1.** The comparison of combining logit calibration.\n\n(1) Surrogate model: **ResNet-50**\n|       | Dense121  | VGG16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin  |338.2/698.4/772    |239.6/590/655.4   |33.4/96/111      |\n|T=5 + Angle   |345.2/742.6/823.8  |256.2/664.8/721.6 |35.8/104.6/131.4 |\n|T=10 + Margin |326.8/694.6/772.8  |227.8/593.6/663.2 |129.4/96.8/114.6 |\n|T=10 + Angle  |329.6/697.6/790.6  |244.2/590.2/689.4 |33.6/99.8/128.6  |\n|T=20 + Margin |330/691.6/762.4    |230.8/584.4/658.2 |31.6/95/117.8    |\n|T=20 + Angle  |342.2/686.2/764.6  |247.4/587/666.2   |34.4/97.4/126.8  |\n|Margin+Angle  |344/708.4/781.4    |242.6/601.8/673.8 |35/103.6/125.8   |\n\n(2) Surrogate model: **Denss121**\n|       | ResNet50  | VGG-16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin |192.6/442.6/477.8  |141.2/377.4/408.4 |25.4/74.8/93.6 |\n|T=5 + Angle  |202.6/526.6/619.2  |158.2/450.2/536.4 |23.4/92/127.2  |\n|T=10 + Margin|183.2/441.4/491.2  |136.6/369.4/416.4 |24.4/82.8/91.8 |\n|T=10 + Angle |193.8/472/561.2    |148.2/400.6/470.2 |25.2/82.8/109.8|\n|T=20 + Margin|191/433.8/485.4    |138.8/366.6/414.2 |23.6/78.2/95.4 |\n|T=20 + Angle |199.6/443.8/508.6  |155.4/383.8/437   |24.6/82.4/95.2 |\n|Margin+Angle |198.8/465.6/527.4  |152.2/392.8/445.4 |27/82/99       |\n\n(3) Surrogate model: **VGG16**\n|       | ResNet50    | Dense121   | Inc-v3     |\n| -     | :-:       | :-:         | :-:        |\n|T=5 + Margin  |34.8/101.8/114    |37.2/123.6/145.6 | 3/10.8/13    |\n|T=5 + Angle   |21.6/25/23.4      |23.6/25.6/23.2   | 1.6/1.4/1.6  |\n|T=10 + Margin |31.6/107.2/117.4  |34.4/129.4/149.6 | 2.2/10.4/14.2|\n|T=10 + Angle  |34/62/50.8        |34.8/75/70.2     | 2.4/6.4/5.8  |\n|T=20 + Margin |34.6/100.8/118.2  |33.6/120.2/148.6 | 2.8/12.4/14.4|\n|T=20 + Angle  |32.4/96.6/101     |38.8/119.4/133.2 | 2.8/10.4/12  |\n|Margin+Angle  |33/98.4/111.2     |35.4/126.4/146   | 2.6/12/14    |\n\n(4) Surrogate model: **Inc-v3**\n|       | Dense121  | VGG-16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin |4.8/14.4/16    |6.2/21.2/28.6 | 5/17.2/28.4  |\n|T=5 + Angle  |5.2/16/20.4    |5.8/19.6/31.2 | 5.4/16.6/24.6|\n|T=10 + Margin|5.4/14/19.2    |4.6/18.8/30.4 | 3.2/14.8/23  |\n|T=10 + Angle |5.8/13.4/18.6  |6/19.6/32.2   | 4.6/16.4/25.6|\n|T=20 + Margin|6.4/12.2/19.4  |5/19/29       | 4.8/16.4/26.8|\n|T=20 + Angle |6.4/16.2/20.4  |5.6/19.6/35.2 | 4.8/17/28.8  |\n|Margin+Angle |6.4/14/21      |5.6/17/31.2   | 5.4/15.4/26.2|\n\n>**Comment 5:** Limitation of this study.\n>\n>**Response 5:** Thanks for your valuable suggestions. We will add this information in the revision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IfQv4z1bM6K",
                "writer": "author",
                "reply_to": "bNMs61pCbvC",
                "title": "Response to Reviewer Boef",
                "comment": " Thanks for your comments. We would like to address your concerns in the following aspects.\n\n>**Comment 1:** The influence of different $T$ in CE.\n>\n>**Response 1:** \n>\n>(1) ***A large $T$ for VGG-16 and Inc-V3:*** In the Response to Reviewer SQcN, we guess that the T is related to the model depth, in which a large $T$ is preferred for the CNN models with few layers. Compared with the ResNet-50 and DenseNet-121, VGG-16 and Inc-V3 have few layers, and better performance is obtained using large $T$. \n>\n>(2) ***The results of continually increasing $T$:*** In the supplementary, we analyzed the relation between the Logit loss in [30] and the CE calibrated by a large $T$. The gradient of Logit loss is $\\frac{\\partial L_{Logit}}{\\partial \\phi(\\hat{x})} = - W_t$, and the gradient of CE with a large $T$ is $\\frac{\\partial L_{CE}^T}{\\partial \\phi(\\hat{x})} \\approx - \\frac{W_t}{T}$. Since the I-FGSM only considers the Sign of gradient while neglecting the magnitude, then the optimization of the CE calibrated by a large $T$ is nearly equivalent to the Logit loss. In Table 1 and Figure 1 in the supplementary, we reported the comparison results of $T=50, 100$, and the Logit loss. Their results are very similar to each other, which verifies our analysis between the Logit loss and using a large $T$ in CE.\n>\n>Therefore, the performance of the targeted attack will get saturated when T continually increases since it is nearly equivalent to the Logit loss.\n\n>**Comment 2:** The relation between three calibration methods.\n>\n>**Response 2:** In this study, we investigate temperature-based, margin-based, and angle-based logit calibrations to validate the main hypothesis of our study that \u201cenlarging the logit margins can increase the targeted transferability.\u201d \n>\n>The temperature-based is the simplest one which only calibrates the logits by a constant value of T.  However, the optimal T is different for different models, as shown in Tables 1 & 2. Therefore, we investigate the margin-based and angle-based calibrations to deal with this hyper-parameter issue.  The margin-based method adaptively computed the \u201cT\u201d based on the Top-2 logits of each iteration instead of using a constant value. On the aspect, since $z_i= W_i*x + b$ and the L2 norm of $W_i$ is different for each class $i$, we further perform the calibration by normalizing the classifier weight $W_i$ of each class $i$ and the feature $x$ to the unit length by L2-normalization. This calibration is actually computing the cosine between the $W_i$ and $x$ while without considering their norms. Therefore, we term it angle-based calibration.\n\n>**Comment 3:** The option for the best targeted attack.\n>\n>**Response 3:** Since the best combinations of different models are different, we currently cannot have a universal receipt for each model. Based on the results in the manuscript and new results in the rebuttal, we might suggest using (T=5 + Margin) or (T=5 + Angle) for CNNs with more layers and the single Margin-based calibration for CNNs with fewer layers. \n\n>**Comment 4:** Typos and Grammar mistakes.\n>\n>**Response 4:** We will carefully polish the manuscript.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gLRl60iGOpe",
                "writer": "author",
                "reply_to": "IDrIEbjfT7Y",
                "title": "Response to Reviewer SQcN ",
                "comment": " Thanks for your comments. First, we will revise all tables to have a better presentation in the revision. Then, we would like to address your concerns about the interpretation of Table 1. \n\n>**Comment:** Potential interpretation of the results in Table 1.\n\n>**Response:** We argue the main reason for better results obtained by Margin-based calibration for the VGG-16 is mainly due to the influence of model depth. For the CNN models with fewer layers, a large normalization factor \u201cT\u201d is preferred to achieve higher targeted transferability. In our Margin-based calibration, the denominator \u201cT\u201d (logit margin between the first and second logits) will keep increasing along with the optimization iterations and thus leads to better performance. \n>\n>To further check the influence of depth, we leverage the ResNet-18 with fewer layers as the surrogate model and reported the results in the following Table 1. We also find that a large T and the margin-based calibration are preferred. \n\n>**Table 1.** The average number (#) of successfully attacked targeted samples with the ResNet-18 as the surrogate model.\n|       | Inc-v3         | ResNet-50       | Dense-121       | VGG-16 |\n| -     | :-:       | :-:        | :-:        |:-: |\n|CE     |21/30.4/29.6    |191.8/239.8/259.8|185.8/239.6/246.2|158.6/192.6/190.4|\n|CE/5   |39.2/108/119    |278.2/606.8/636.2|271.8/574.8/615.6|237.2/530/565.8|\n|CE/10  |36.2/111.6/132.4|259.2/597.4/668.6|258.6/571.8/642.2|222.4/530/596.6|\n|CE/20  |38.8/113.8/129.8|251.8/578/641.8  |248.2/543.2/607  |211.4/497.4/571.2|\n|Margin |41/113/130.8    |273/601.4/653.2  |273.2/572.6/629  |234.2/535.4/586.2|\n|Angle  |36.6/81.8/83.6  |271.4/514.8/542.6|280.6/527.6/557.4|239/449.4/462|\n|Logits |37.2/100.2/122  |247.8/556.2/606.8|243.2/536.4/585. |212.4/494.2/548.6|",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7eZgZ-rivOg",
                "writer": "author",
                "reply_to": "eqDmaQ47r91",
                "title": "Authors' Response",
                "comment": " Thanks for your feedback. We would like to address your concerns in the following two aspects.\n\n>**Comment 1:** The related work section.\n>\n>**Response 1:** The structure of our current related work is mainly based on the following considerations. (1) The [1] highly inspired this study, and we followed the academic writing skills of [1] to some extent. (2) The I-FGSM, MI-FGSM, TI-FGSM, and DI-FGSM have been used as the baseline in the experiments. Besides, the optimization of only using the Sign of gradient in the I-FGSM is essential for our analysis of the relation between Temperature-calibration with large T and the Logits loss function. (3) The Po-Trip and the Logits are two main comparison methods, and then we also introduce them in detail.\n> \n>We will rewrite the related work section in the revision to avoid this similarity issue.\n\n----\n\n>**Comment 2:** Marginal Improvement and Contribution.\n>\n>**Response 2:** We totally disagree with your comments that the contribution of this work is limited by only achieving marginal experimental gains. \n>\n>* First, we would like to **recap the primary goal of this study**, which mainly aims to analyze why the widely used CE loss function can not generate adversarial samples with higher targeted transferability. However, previous studies only reveal this issue due to the vanished gradient issue without further analysis. In this study, we take a close analysis of the CE loss and find that the logit margin between the targeted and non-targeted classes quickly gets saturated during the optimization process, hindering the CE's transferability.\n>\n>*  Second, **how to solve this issue**? Based on our analysis, we then explored three different logit calibration methods to deal with the saturated issue of logit margin. The experiment results valid our findings for the problem. Besides, in the supplementary, we further analyze that the Logit loss in [1] is nearly equivalent to the Temperature-calibration with large T. \n>\n>*  Third, **what is not the goal**? We do not intend to beat the state-of-the-art by a large margin. Although the logit calibrations slightly outperform the Logit for most cases, they can significantly increase the performance of the original CE. Besides, we also notice the results of combined logit calibrations in Table 3, which can outperform the Logit by more than 10% when using the ResNet50 and Dense121 as surrogate models. The additional experiment on the difficult transfer with varied targets suggested by the Reviewer WQZv can further show the effectiveness of logit calibration in the targeted attack.\n> \n>Based on the above explanation, we believe our investigation in this study can provide valuable insight for future researchers by using the logit calibration from both attack and defense.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eqDmaQ47r91",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Official Review of Paper1795 by Reviewer qX4X",
                "comment": " This work proposed three different calibration methods, temprature-based, margin-based and angle-based temperature scaling to enlarge the margin between targeted logit and non-target logits to improve transferability of targeted adversarial attacks. This work is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods.\n\n\n[1] \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\".\nZhengyu Zhao, Zhuoran Liu, Martha Larson. NeurIPS 2021. First of all, after comparing the related work in [1] and this work, there is a huge amount of overlapping of the equations or rewriting the sentences. This significantly destroys the overall quality of the work.\n\nSecond, the improvement of the proposed method over [1] is marginal compared to the improvement of [1] over cross-entropy loss.\n\nThird, the contribution of this work is limited. Although the authors proposed different temperature-scaling based methods to improve transferability of targeted attacks, which only achieve limited experimental gains, this work did not provide extra useful insight to this research area.\n\n[1] \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\". Zhengyu Zhao, Zhuoran Liu, Martha Larson. NeurIPS 2021. In general, it leaves me a poor impression when I realize the great similarity between the Related Works in this work and that in [1]. Although the authors try to reframe the sentences, it's still very unprofessional to structure related works with such a strong similarity with another existing work.  See above.",
                "rating": 2,
                "confidence": 5
            },
            {
                "review_id": "IDrIEbjfT7Y",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Official Review of Paper1795 by Reviewer SQcN",
                "comment": " The authors propose a novel and effective method to improve the transferability of adversarial attacks. They increase the logit margins between targeted and non-targeted classes, which can quickly become saturated in cross-entropy loss. Strengths:\n1. The findings are very interesting and the motivation is well-explained.\n2. Comprehensive experiments are presented and the combining logit calibrations have significantly better performance than previous methods.\n\nWeaknesses:\n1. The proposed method has various settings and hyper-parameters. Compared to the simple Logit method, the proposed method needs more effort for tuning or need combining logit calibrations to achieve better performance. This can make the method less attractive to the community.\n3. There is no theoretical analysis to support the empirical findings.\n2. The presentation of the results needs to be improved. All tables contain tons of numbers, which makes it hard for the reader to get the point in a short time. Could authors provide any interpretation of the results in Table 1? For example, why do Margin and Angle have better performance when the surrogate models are VGG17 and Inc-v3, but have lower performance for ResNet50 and Dense121? N/A",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "bNMs61pCbvC",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Official Review of Paper1795 by Reviewer Boef",
                "comment": " This paper designs a new logit calibration method which is inspired by knowledge distillation. The method uses logit calibrations in the CE loss function so that it can improve the targeted adversarial attack with higher transferability than other attack methods with cross-entropy loss. Except for the primary temperature-based method, this paper designs margin-based and angle-based methods to solve different surrogate models and different norms. The strengths of this paper are:\n\nThis paper designs a new cross-entropy (CE) loss function to improve the targeted adversarial attack, which performs better than Logit (NIPS21).\n\nExcept for the temperature-based method, this paper designs margin-based and angle-based methods to solve different surrogate models and norms.\n\nThe weakness of this paper are:\n\nThis paper follows `Zhengyu Zhao, Zhuoran Liu, and Martha Larson. On success and simplicity: A second look at transferable targeted attacks. NeurIPS, 34, 2021.' from academic writing skills and code in specific. However, instead of Zhao et al. designing the Logit loss and using it to generate universal adversarial perturbations, this paper's method does not have any additional functions such as UAP. \n\nMoreover, this paper's method only exceeds the Logit loss by around 10%, which is not a significant improvement. Therefore, this paper lacks novelty.\n\nThe equation 14 seems to have some mistakes. On the left of the equation, would $z_{i}$ be $\\tilde{z}_{i}$?\n\nA few grammar problems in this paper should be improved. For example, in line 275, it should be \"be similar to\"; in line 23, it should be \"Following many approaches\"; in line 27, it should be \"it is vital to explore.\"\n Firstly, for the influence of different $T$ in CE, this paper claims that when the surrogate model is VGG16 and Inc-V3, a larger $T$ obtains better transferability. However, I am curious about is there a limitation of $T$ on VGG16 and IncV3. For example, after the ASR achieves 600, the performance of the targeted attack will decrease when $T$ continually increase. And then, although this method is based on CE, it would be better if the authors designed a new name to describe it. The relationship between temperature-based, margin-based, and angle-based logit calibration is unclear. This paper claims that the margin-based one is designed to face different surrogate models, and the angle-based one is designed to solve the influence of various norms. However, in the experiments, the performance on T=5, T=10, Margin, and Angle does not prove the relation between them. This paper does not evaluate which method is the best for the targeted attack, or in other words, which option should I choose if I need to achieve the highest attack success rate?",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "JcXe9C2n82K",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Official Review of Paper1795 by Reviewer WQZv",
                "comment": " The paper targets improving the transferability of adversarial attack using the logit calibration. Despite the recent success in untargeted black-box attacks, the targeted transferability of adversarial attacks remains challenging. The paper takes a closer look at the vanishing gradient issue in the CE loss function which is commonly used to learn transferable adversarial samples and suggests that the logit margin between the targeted and non-targeted classes quickly gets saturated during the optimization process. So, to improve transferability they aim to enlarging logit margins which consequently reduce saturation and  enable longer optimization and iterations. The paper investigates three different types of logit calibrations including temperature-based, angle-based and margin-based inspired by previous studies and techniques.  Experiment conducted using ImageNet dataset and different methods including ResNet50, DenseNet-121, VGG-16 and Inception-v3. Results are compared to SOTA methods including Po+Trip, Logit, and TTP.  Strength:\n- Quality: The writing quality is very good, very easy to follow, there are areas that can be improved but nothing major.\n- Clarity: Very clear. Easy to understand the motivation and the thought process of different method\u2019s component. \n- Significance and novelty: Novelty is a bit limited and built up mostly on top of previous methods,  but it is also not a weakness because this work attempts to solve an interesting problem and the analysis and results are valuable. The results are somewhat important. Mostly inspired by Logit [30], future researchers might use the suggested logit calibration and increase the number of iterations for targeted attacks.\n\nWeaknesses:\n- The quality of results and presentation of it could be improved significantly. (see below)\n- The distinction between [30] and this paper should be clarified and the introduction in page 2 [line 36-56] can benefit from a re-writing. (see below)\n - There is a large novelty overlap between [30] and the current method. It is proper to get the similarity and distinction discussed upfront. Specifically,  discussion around CE starting line 36 is getting very confusing and blurry going through line 56. Vanishing gradient and the use of Logit loss has been discussed and proposed in previous arts, for example [30]. This gets discussed later at line 145+ and in method, however, I feel the contribution of the current work can get discussed in a more clear way and upfront in introduction. \n\nExperiments are limited and can get improved:\n- First the organization of the results is not optimized or ideal. (1) Following Table 1, 2 and 3 is very hard and replacing this with line graphs that capture progress through iterations would be very beneficial. (2) Overall collective of Table 1, 2, 3 seems to be more exploratory and ablation tables rather than the main results. The main message that I read from these two tables are T=10, 20 and a combination of Margin + Angle or T + Angle can result in the best outcome. So, why not introduce a single best receipt and present everything else as ablations? (3) Also I would suggest sticking with conventional methods such as heat-map to summarized heavy tables, as it is a norm in adversarial attack literature.  [21] have good examples of result presentations.\n\n- One relevant question, is also as Table 2 suggests the best outcome is coming from T=10 or T=20 so why in Table 3 we analyze the effect of combining logits as T=5 +Margin or Angle? If any underlying study suggests this, results should be provided.  \n\nThe results could be strengthened by:\n- Providing experiments on another dataset (e.g., CIFAR-10, MNIST, SVHN) Since the proposed method works well on ImageNet, it could only be a minor concern.\n- Incorporating study of a real-world attack for example the Google Cloud Vision API.\n- Providing targeted success rates for transfer with varied targets.\n I cannot find any specific discussion around the potential negative social impact of this work. Also, the limitation of the method was not addressed in the paper.\n\nTo improve this part, discussion around the benefits of adversarial attack research can get discussed. Potentially this can motivate the AI community to design stronger defenses against transferable attacks, and  in the long run such results can be directly used for social good applications, such as protecting privacy. On the contrary, there are applications that can benefit from transferable attacks in a harmful manner to damage the outcome of any AI system, e.g. imaging a scenario that someone uses such attacks to intrude with the outcome of a medical AI device.  \n\nI also suggest that authors discuss the limitation of the work, failure cases and processing time.\n\nOverall, I enjoy reviewing this paper and looking forward to reading the authors' responses. \n",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "3h1iwXmYVVJ": {
        "paper_id": "nips_2021_3h1iwXmYVVJ",
        "paper_title": "Implicit Regularization in Matrix Sensing via Mirror Descent",
        "paper_abstract": "We study discrete-time mirror descent applied to the unregularized empirical risk in matrix sensing. In both the general case of rectangular matrices and the particular case of positive semidefinite matrices, a simple potential-based analysis in terms of the Bregman divergence allows us to establish convergence of mirror descent---with different choices of the mirror maps---to a matrix that, among all global minimizers of the empirical risk, minimizes a quantity explicitly related to the nuclear norm, the Frobenius norm, and the von Neumann entropy. In both cases, this characterization implies that mirror descent, a first-order algorithm minimizing the unregularized empirical risk, recovers low-rank matrices under the same set of assumptions that are sufficient to guarantee recovery for nuclear-norm minimization. When the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a first-order approximation to mirror descent, in which case we obtain an explicit characterization of the implicit bias of gradient flow as a by-product.\n",
        "paper_acceptance": "accept",
        "meta_review": "All reviewers are satisfied by the authors' response and agree that this is well-written paper with solid contributions, though some of the results could be considered incremental given the existing work in [13,35]. The authors are encouraged to provide additional discussion on the points raised by the reviewers.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "tkVV-C3McwC",
                "writer": "official_reviewer",
                "reply_to": "iHUCezSWYTL",
                "title": "Final Score",
                "comment": " Dear authors,\n\nthank you for your responses. I have now raised my score to 7. \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vl3AQRH5Ops",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "Official Review of Paper10104 by Reviewer dunA",
                "comment": "The authors analyze the convergence of  mirror descent  in matrix sensing for specific selection of mirror maps. Specifically, they prove that when the mirror map is the spectral hypentropy or spectral entropy then MD converges to global minimizers of the empirical objective minimize quantities related to the nuclear, the Frobenius or the Von Neumann entropy.   \nStrengths\n- The main contribution of the paper is the derivation of Theorems 1 and 2 which establish convergence of mirror descent (MD) to global minimizers of the empirical loss of the matrix sensing problem that minimize certain quantities related to nuclear norm, Frobenius norm and von Neumann entropy. These results are interesting and provide an insight into the interplay between the geometries induced in mirror descent and the implicit regularization phenomena that are brought up. \n- Theorems 3 and 4 present recovery guarantees for the proposed mirror descent algorithm, the first of this kind in the framework of implicit regularization.\n\nWeaknesses\n- The contribution of these results to already existing work is somewhat limited given Theorem 1 of [19], which proves that MD converges to minimizers of the Bregman divergence. That being said, though the authors provide an alternative proof and focus on specific mirror maps, the added knowledge seems not to be  that significant.  The authors should make more clear the advantage of their proof as compared to the one followed in Theorem 1 of [19]. \n- From a practical point of view, the selection of the mirror maps gives rise to computationally expensive SVD steps per iteration of the algorithm. That being said, it is so clear what are the advantages of an implicitly regularized algorithm compared to an explicitly regularized one.\n\nMinor comments \n\n- The first 3 paragraphs of Section 1.1  present the contribution without explicitly referring to the added knowledge to the existing literature. The authors should rewrite that part and clarify what point is new and what is not.\n- Typos: \n - line 105 : numerical\n- line 109 : large\n- line 146: identity\n-line 164: descent\n-line 345: regularization Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "x-_fYE0xKIy",
                "writer": "official_reviewer",
                "reply_to": "EA-IM1AqCgn",
                "title": "Final Score",
                "comment": " Thank you for your response. After reading the other reviewers' assessments, I would like to keep my score.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WjkPYw-MVXj",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "Official Review of Paper10104 by Reviewer sWYY",
                "comment": "In this manuscript, the authors study the trajectory of mirror descent to optimize the unregularized empirical risk functional to solve low-rank matrix estimation problems. They characterize the trajectory of mirror descent with different mirror maps and showcase an implicit bias towards certain structures in solutions. In particular, when equipped with either the spectral hypentropy map or spectral entropy map, they show that mirror descent converges to a global minimizer that minimizes a particular quantity depending on the singular values of the resulting matrix. This quantity interpolates between the nuclear norm and Frobenius norm for the spectral hypentropy map and is a linear combination of the nuclear norm and von Neumann entropy for the spectral entropy map. They also show algorithmic guarantees for mirror descent to solve matrix sensing and matrix completion problems that operate with sample complexities on par with traditional nuclear norm minimization approaches, without explicit regularization. Finally, a connection between gradient descent over the parameterization $UU^T - VV^T$ and mirror descent is shown and toy experiments show mirror descent and gradient descent behave similarly.  # Originality\n\nThere has been a flurry of interest in understanding implicit bias in matrix factorization problems, and this work provides an interesting perspective by studying the solutions chosen by mirror descent methods for various mirror maps. Similar results have been reported in [13], but the proof techniques in this work (direct analysis of mirror descent trajectory) and [13] (KKT conditions) differ from one another. The precise formulae presented on which minimizers are chosen appear to be novel as well. The shown equivalence between exponentiated gradient and mirror descent is nice, but not entirely novel as it appears in [35] for the vector-valued case.\n\n# Quality\n\n- Strengths: - The paper is well-written. - To the reviewers knowledge, applying mirror descent to solve matrix factorization problems and analyzing its implicit bias towards low-rank solutions appears novel. Moreover, the characterization of their preferred solutions holds in fairly general settings (e.g., no assumptions on the measurement matrices $A_i$ aside from the existence of a $0$ loss matrix). - The results on recovery guarantees for matrix sensing/completion hold with sample complexity on par with convex nuclear norm minimization techniques. - The connection between exponentiated gradient and mirror descent is interesting and extends previously known results to show the convergence of mirror descent to small nuclear norm solutions in the small initialization scale regime. \n\n- Weaknesses: - Mirror descent would not be a practical algorithm to use due to computational complexity and seems to perform essentially the same as standard gradient descent - No convergence rates are provided in the theoretical analysis of applying mirror descent to matrix sensing/completion. The results only establish convergence to a neighborhood of the underlying low-rank matrix. Moreover, these neighborhoods depend logarithmically on the parameters $\\beta$ (for hypentropy) and $\\alpha$, the initialization size, for the entropy map so that to achieve $\\epsilon$ precision, one would need $\\beta$ or $\\alpha$ to behave like $\\exp(-1/\\epsilon)$. In addition, in the matrix completion setting, a term depending on the sample complexity appears in the estimation bound, which would require $m = \\Omega(nn\u2019\\log^2(n\u2019))$ to make small, but the theorem operates in the $\\Omega(r (n + n\u2019)\\log^2(n\u2019))$ regime. Hence, while the sample complexity in the theorem does scale in the same way as nuclear norm approaches, the resulting error bounds could potentially be quite large even for exponentially small $\\beta$ or $\\alpha$. Could the authors comment on this point and, perhaps, why the error bound scales this way? My main concern is that the bound may be vacuous when operating in the traditional nuclear norm sample complexity regime.\n\n# Clarity\n\nOverall, the paper is well-written and easy to follow. Related work seems to be cited well and the authors do a good job of discussing their results in relation to others on implicit bias in matrix factorization. Here are a couple of typos I found:\n- pg 1 line 33 \u201cwhen then\u201d -> \u201cwhen the\u201d, pg 3 line 105 \u201cnumercal\u201d -> \u201cnumerical\u201d, pg 3 line 109 \u201clargeg\u201d -> \u201clarge\u201d\n\n# Significance\n\nThis work shows that mirror descent in matrix estimation problems exhibits an implicit bias towards certain structured solutions, and precisely characterizes this structure. I think that this does inspire research into further analyzing the role of mirror maps in other estimation problems and in analyzing the potential implicit bias of other first order algorithms. There are no clear societal impacts of this work. The authors discuss some of the downsides and limitations of their approach (namely, the computational cost of applying mirror descent). Another limitation that should also be discussed is the potential looseness of the error bounds.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "sElTzDoMJs",
                "writer": "official_reviewer",
                "reply_to": "r5t6-nXXcP4",
                "title": "Response to authors",
                "comment": " Dear authors,\n\nThank you for the response. After reading the other reviews and author responses, I have decided to raise my score. I think that the paper provides a nice contribution in showcasing further evidence of mirror descent\u2019s implicit bias, and has the potential to inspire future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mQxDVw6EGJE",
                "writer": "author",
                "reply_to": "8diaieCu2Y",
                "title": "Response to Reviewer oUMM",
                "comment": " We thank the reviewer for their feedback. The primary focus of our paper was indeed to understand how the implicit bias of mirror descent can be leveraged to solve matrix recovery problems rather than to argue that mirror descent is a computationally competitive algorithm for this problem. Typically, algorithms based on implicit regularization (i.e. mirror descent or gradient descent with full-rank factorized parametrization) cannot match the computational efficiency of gradient descent with low-rank factorized parametrization (see lines 354-356). Please find our responses to the questions below.\n\n1. We thank the reviewer for pointing this out and we have made the dependence on $\\beta$ more explicit in Theorem 1. We remark that in Theorems 3 and 4, where an upper bounds on $\\beta$ is assumed, the step size $\\eta$ can be chosen independently from $\\beta$.\n\n2. Indeed, a smaller value for $\\beta$ leads to a slower convergence of mirror descent (initially). This can be perhaps most directly seen from Proposition 5, which shows that $\\beta$ corresponds to the initialization size in the exponentiated gradient algorithm defined in (15) (under the assumption that the sensing matrices are symmetric and commute). Hence, a small value for $\\beta$ means that the algorithm defined in (15) does not move much initially, as both $\\mathbf{U}_t$ and $\\mathbf{V}_t$ start from small initial values and are updated multiplicatively. While a rigorous analysis of the convergence speed of mirror descent was outside the scope of our paper, we remark that precise convergence speed guarantees showing a similar behaviour have previously been established for mirror descent (in the vector-case) for the problem of sparse phase retrieval in [35].\n\n3. We believe that there is indeed a tradeoff between computational complexity and statistical error controlled by $\\beta$, similar to the tradeoff established in the context of sparse phase retrieval in [35]. We point out that the logarithmic dependence on $\\beta$ and $\\alpha$ is likely not sharp as we observed in our experiments (we observed similar results for the dependence on $\\beta$, and we omitted those experiments given the similarity to the experiments in Section 6 and Appendix C).\n\n4. The analysis of the evolution of the Bregman divergence is a general tool that can be employed to study the trajectory of mirror descent. Beyond characterizing the implicit bias of mirror descent, it has been previously used to establish the convergence of mirror descent for a general class of non-convex optimization problems [39], and, more specifically, the potential-based analysis of mirror descent has been used to establish minimax-optimal rates of convergence in noisy sparse phase retrieval [36], for instance. We remark that in previous related works that establish convergence speed guarantees, e.g. [19, 35, 36], a sample complexity that scales quadratically in the respective notions of sparsity in matrix sensing and sparse phase retrieval is needed, while our results only require a linear sample complexity, but do not establish any convergence speed guarantees for the estimation error. We will add a discussion on this point in the main paper, and we hope that our work can inspire future research in this direction, possibly bridging this gap and establishing rigorous convergence speed guarantees when only a sample complexity that scales linearly in the respective notion of sparsity is assumed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iHUCezSWYTL",
                "writer": "author",
                "reply_to": "vl3AQRH5Ops",
                "title": "Response to Reviewer dunA",
                "comment": " We thank the reviewer for their feedback. Please find our responses below.\n\n1. We believe that the reviewer is referring to Theorem 1 of [13]. The main advantages of our proof technique over the KKT optimality conditions-based proof of Theorem 1 in [13] are 1) the fact that convergence of mirror descent does not need to be assumed, and 2) that this type of potential-based analysis of mirror descent is also applicable to more general non-convex problems (e.g. [39]) and can, in principle, be used to establish convergence speed guarantees (e.g. [35, 36]), see also lines 88-95. We remark that in previous related works that establish convergence speed guarantees, e.g. [19, 35, 36], a sample complexity that scales quadratically in the respective notions of sparsity in matrix sensing and sparse phase retrieval is required, while our results only require a linear sample complexity, but do not establish any convergence speed guarantees for the estimation error. We will add a discussion about this in the main paper, and we hope that our work can inspire future research in this direction, possibly bridging this gap and establishing rigorous convergence speed guarantees when only a sample complexity that scales linearly in the respective notion of sparsity is assumed. \n\n2. Indeed, mirror descent is computationally more expensive than gradient descent due to an SVD in each iteration of the algorithm. We point out that, as elaborated in lines 344-356 and Section A of the Appendix, the SVD can be avoided in the positive semidefinite case, where we only need to compute matrix exponentials, which, though more expensive than matrix multiplication, are typically cheaper to compute than an SVD.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EA-IM1AqCgn",
                "writer": "author",
                "reply_to": "qDuuo2OrrzW",
                "title": "Response to Reviewer ZKpj",
                "comment": " We thank the reviewer for their feedback. We fully agree with the reviewer and believe that both points are very interesting directions for future research.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r5t6-nXXcP4",
                "writer": "author",
                "reply_to": "WjkPYw-MVXj",
                "title": "Response to Reviewer sWYY",
                "comment": " We thank the reviewer for their feedback.\n\nRegarding the logarithmic dependence of the error bounds in Theorems 3 and 4 on $\\beta$ and $\\alpha$: even though the error bounds in equations (13) and (14) involve a term of the form $\\frac{nn\u2019\\log^2n\u2019}{m}$, both bounds tend to zero for any $n,n\u2019,m$ in the limit $\\beta,\\alpha\\rightarrow 0$. While this would require an impractically small $\\beta$ or $\\alpha$, our error bounds demonstrate that mirror descent can recover a planted matrix $\\mathbf{X}^\\star$ from $\\Omega(r(n+n\u2019)\\log^2(n\u2019))$ samples (exactly in the limit $\\beta,\\alpha \\rightarrow 0$, which is a setting often considered in the literature, e.g. [14, 20, 34]).\n\nWe obtain the logarithmic dependence on $\\beta$ and $\\alpha$ since our error bounds are established by 1) bounding the nuclear norm of the limiting point of mirror descent using Theorems 1 and 2, and 2) using this bound on the nuclear norm to control the estimation error $||\\mathbf{X}_t-\\mathbf{X}^\\star||_F$. In step 1), the upper bound on the nuclear norm inherits the logarithmic dependence on $\\beta$ and $\\alpha$ from the characterization of the limiting point of mirror descent in Theorems 1 and 2 (which is exact).\n\nAs we point out in lines 246-252, error bounds with a polynomial dependence on the initialization size $\\alpha$ as well as convergence speed guarantees have been established using additional assumptions for gradient descent in [19]. We expect that the analysis of [19] can be adapted to mirror descent, since the operations $\\exp$ and $\\log$ leave the eigenspaces of symmetric matrices invariant, and we expect a similar adaptive decomposition of $\\mathbb{S}^n_+$ into a rank-$r$ subspace, to which $\\mathbf{X}_t$ is approximately confined, and a rank-$(n-r)$ subspace where $\\mathbf{X}_t$ remains small, to be possible. In the present paper, our focus was on understanding how the implicit bias of mirror descent can be leveraged to solve matrix sensing and completion, under assumptions matching those required by nuclear norm minimization (in particular assuming a sample complexity that scales linearly in the rank $r$ of $\\mathbf{X}^\\star$ rather than quadratically as in [19]). We will add a discussion about this in the main paper, and we leave an analysis that establishes error bounds that do not depend polynomially on $\\beta$ and $\\alpha$, and convergence guarantees, desirably without additional assumptions compared to Theorems 3 and 4 (in particular with a linear sample complexity), to future work. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qDuuo2OrrzW",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "Official Review of Paper10104 by Reviewer ZKpj",
                "comment": "The paper provides the convergence analysis of mirror descent for matrix sensing to particular minimum norm solutions. The updates that are studied are induced by the hypentropy and von Neumann divergences. The authors analyze multiple interesting cases including matrix completion and the case where the instances satisfy the RIP. They also provide some experimental evaluations to validate their findings.  The paper studies an interesting problem and greatly generalizes the results in [13] and [14]. There are a few remarks about the analysis:\n\n1) Although hypentropy is an interesting divergence, it is only an approximation of the squared Euclidean and KL divergence in the limit cases. A more natural extension is the tempered KL divergence which was partially analyzed in [1]. I believe most of the analysis may carry over.\n\n2) The construction in Eq. (15) is interesting, but not exactly related to the reparameterization of the Exponentiated Gradient Unnormalized (EGU) algorithm. The exact reparameterization for the vector case was discussed in (Amid and Warmuth, 2020) along with regret bounds for the discrete case. It would be interesting to extend the analysis to the matrix case where matrix U is updated using gradient descent.\n\nReference:\nEhsan Amid and Manfred K. Warmuth. \"Winnowing with Gradient Descent\". COLT, 2020. Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "8diaieCu2Y",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "Official Review of Paper10104 by Reviewer oUMM",
                "comment": "This paper studies the implicit regularization of a particular algorithm, mirror descent, for matrix recovery problems.   I think the paper presents an interesting phenomenon that is also present in mirror descent.  I believe the paper only attempts to explain some phenomenon in mirror descent, rather than arguing that the mirror descent method is the correct or a competitive method in solving matrix recovery problems.   \n\nI have a few questions that I would like to hear the author's responses. \n\n1. In theorem 1, does the stepsize eta depends on the parameter beta as well? \n\n2. I wonder how beta affects the algorithmic performance, i.e., the convergence speed of the algorithm? Can the author comment on the relationship between the magnitude of beta and the speed of the mirror descent method?\n\n3. I notice that the bias/difference between X_infty and X* in Theorem 3 and 4 depends logarithmically on the parameter beta. So one wants to choose a very smaller beta to reduce the bias. It seems in Theorem 4, one needs beta to be exp(-n^(1/2)) so the RHS of (13) can be smaller than ||X*||_*. Is there a tradeoff between computational complexity, and statistical error, i.e., the difference between  X_infty and X*,  in choosing beta? \n\n4. I think characterizing the implicit regularization phenomenon is interesting by itself. But I wonder whether there are any broader applications/impacts of the results or techniques used in the paper.  A few limitations are discussed in the end. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "mOO-LfEVZK": {
        "paper_id": "iclr_2021_mOO-LfEVZK",
        "paper_title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering",
        "paper_abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods (e.g., adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, i.e., the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "Two reviewers expressed clear concerns about the paper but the authors did not provide any response. ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "oxM2g9iZ9IY",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "No theory, experiments only",
                "comment": "Results: To defend against adversarial attacks, this work experimentally analyzes the feature distribution of traditionally- trained CNNs for gaining more knowledge about adversarial examples. Two properties, i.e., the non-clustering property and confusing-distance property, of the feature distribution are identified by means of t-SNE visualization and clustering analysis (showing the limitations regarding representativeness) in Figure 1. The authors introduce a loss function which separates out cluster centers of CNN output features, setting them as far as possible - so that model accuracy is preserved while strengthening robustness. They test on two datasets: CIFAR10, MNIST, and show improvements in \"robustness\" of the model. \n\nStrong points: The experiments presented are promising in terms of increasing robustness of the learned models. \n\nWeak points: Experiments are only conducted on two datasets, it's unclear how generalization these results are. Further, there is no theoretical development regarding manifolds in the feature space.  \n\nMinor typing errors: \n\"indication of the clean accuracy\" \n\"using PGD optimizationm,\"\n\"an input images x\"\n\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "DiDrtOmSKyF",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "This paper proposes a manifold aware training strategy to learn compact features and improve the robustness of CNNs.",
                "comment": "This paper proposes to leverage the manifold aware training to learn compact representation. The authors proposes to enforce the learned representation along with generated vectors for different clusters, which implicitly enlarge the margin of the prediction.\nHowever the technical contribution as using the three-term loss to improve robustness is limited. In particular, it's is unclear what the equation (10) and (11) try to prove without a concrete theorem or lemma statement. \n\nFrom the empirical performance, it looks promising from table 2 but it's also quite clear that the TRADES loss BIBO dominates the performance, and without adding this loss, the proposed MAT training cannot achieve high robustness. This is as expected and also render the proposed method less effective.  \nIn addition, TRADES is evaluated on ImageNet and it would be good for the work to evaluate on ImageNet to demonstrate the generalization ability and scalability. It would also be good to explain why without the BIBO loss, the robustness against adaptive attack of MAT is almost 0 which again shows the weakness of the main proposed method.\n\nIt would also be necessary to provide analysis for the properties of the learned representation. For instance, if it is compact features, whether its rank is indeed lower, and whether the entropy of the learned features is indeed low in order to claim the consistent and compact feature representation. \n",
                "rating": 5,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "tD4cYRakmsO",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "Recommendation to Accept",
                "comment": "Summary:\nThis paper tackles the problem of training models that are robust to adversarial inputs. The authors starts by observing that previous models generate embeddings that can both (i) place same-class embeddings in different clusters and (ii) different-class embeddings in close proximity. They then introduce new loss functions that penalize these behaviors and design a training procedure (MAT) around these new losses. Finally, they show favorable performance of MAT compared to state-of-the-art techniques for addressing adversarial robustness.\n\nReasons for score:\nOverall, I vote for accepting. Training adversarially robust models is an important problem, and the paper\u2019s experimental validation that the features of prior methods (TRADES) exhibit (i) non-clustering and (ii) confusing distance motivates the approach they take. The loss functions are explicitly designed to combat these issues, and the experimental results clearly show the favorability of the MAT procedure. In addition, the ablation study of the various components of the loss functions also adds some insight into the results. The paper is also very well written.\n\nCons:\nIt would be of interest to have some theoretical justification for the approach. Regarding the loss functions, it seems that BIBO should be a consequence of penalizing FTC loss and SO loss, and should not be explicitly needed (this is also somewhat consistent with the results of Table 2). Finally, some of the experimental results can be explored further. For example, in the ablation study, some of the experiments perform better without one of the loss functions, and it may help to explain such behavior. \n\nClarity / Typos:\nThe paper is very well written. A couple of minor points:\nFeature compactness - Maybe explain this phrase better in the introduction (explained well in Section 3 introduction)\nEqn 1: Maybe write J(f(x), y) and J(f(x), f(x\u2019)) instead\n",
                "rating": 7,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "zzQzZ9CO3wY",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "The defense evaluation is not correct",
                "comment": "This work proposes a defense that combines prior work on learning features that are compact for samples from the same but dispersed for samples from different classes (MMD by Pang et al.) with (a) a method to find better class centers, (b) a gradient-norm regularization and (c) an adversarial training regularization.\n\nUnfortunately, the reported results on the robustness of the defense are clearly wrong. For one, the core part of this defense by Pang et al. was broken by [1] which is not mentioned here. More importantly, the adversarial attacks employed here are not suited for finding minimal adversarial perturbations against the proposed defense. This can be seen most clearly in Figure 2 (or Table 10 in the appendix): If we allow a perturbation with L-infinity norm of 0.5 on MNIST, then we can always find an adversarial perturbation simply by setting the whole image to a flat gray value of 0.5. In turn, any effective adversarial attack should drive network performance down to at least random baseline performance (10%) for epsilon = 0.5. Instead, the paper reports > 99% accuracy for this value under a PGD attack, which means that PGD is totally ineffective against the given defense and a very different adaptive attack would be needed to accurately measure its robustness. Similarly, in Table 3 the attack success of targeted attacks is often higher than for untargeted attacks, again a clear sign for ineffective attacks. The work also uses an adaptive attack which works better for some versions of MAT but performs similar to PGD in other cases. Hence, the adaptive attack employed here are not good enought.\n\nThe reason why the proposed attacks fail against the defense are probably simple: for one, the attacks optimise a different classificatioon loss then what is actually used by the model. Second, both auxiliary losses may give rise to gradient masking, the most common issue for gradient-based attacks to fail against a defense. I highly suggest the authors study [1] to get familiar with how to engineer strong adaptive attacks.\n\n[1] On Adaptive Attacks to Adversarial Example Defenses, Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry, NeurIPS 2020, https://arxiv.org/abs/2002.08347",
                "rating": 1,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "VdxOoQuC6FI",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "AnonReviewer4 Review",
                "comment": "# Summary\n\nThe authors propose a novel training process called Manifold-Aware\nTraining (MAT) to increase the robustness of the CNN against adversarial\nexamples. The authors compare MAT against the state-of-the-art in\ndefenses against adversarial evasion attacks (i.e., TRADES and MCC) and\nshow their approach outperforms it.\n\n# Strengths\n\n+  Interesting intuition of performing training \"in the\" manifold\n+  Interesting intuition to support SO and BIBO losses\n\n# Weaknesses\n\n-  Lack of comparison with a similar approach\n-  Lack of conclusive remarks / actionable points\n\n# Comments\n\nI praise the authors intuition of exploring the possibility of training\na classifier by exploiting knowledge of the manifold - its immediate\nimplication is that of focusing on lower dimensions of compact features\nthat would be more robust to manipulation (and thus adversarial\nattacks). I also particularly appreciate the threat model and the fact\nthe approach is evaluated in a white-box setting, according to Carlini\net al. (2019). While the authors' intuition is interesting, I wonder how\neasy this is to achieve in practice. In general, we have no knowledge of\nthe underlying manifold and I thus wonder what guarantees this approach\nwould provide. The results seem to show no clear loss-dependent trend\nand I thus wonder whether we can easily draw conclusive remarks. (For\ninstance, should we use SO and BIBO always? From a theoretical\nperspective, it seems so, but experiments seem to show otherwise.)\n\nFigure 1 is interesting as it shows that the representative features of\nsame-class samples are not always similar to one another. Wasn't this\nalready explored in Szegedy et al.? Perhaps not visually, but the fact\nthat objects close in the input space get eventually separated in the\nlatent space across the layers of the CNN is quite known. Also, a\nsimilar approach to the authors' proposal seems to be explored by Crecchi\net al. [1]. It would be interesting to compare and position MAT against\nthis.\n\n## Additional Comments\n\nIn Section 3.2, the authors propose two auxiliary loss functions to\nfurther improve the robustness of MAT. I wonder whether the BIBO loss\nwould just suffice for the purpose, instead of relying on the\nsecond-order loss too. I appreciate the explanation in Section 3.2.3 but\nit would be interesting to understand how one should expect to tune\nalpha and beta accordingly. \n\nResults on CIFAR10 seem less stable than compared to those on MNIST. In\nparticular, there is no trend that shows that relying on SO and BIBO on\na clean dataset provides better results than with a plain FTC loss:\n94%->85%->95%->83%; why the 95%? Is that expected? Similar reasoning\ncan actually be applied to MNIST too when one looks at PGD:\n61%->99%->82->99; why 82%? Is this expected? In contrast, TRADES seem to\nshow an expected trend (even when BIBO loss is considered).\n\nThe authors rely on the library 'foolbox' - my impression was that\ncleverhans [2] represented the state-of-the-art when it comes to\nexperimenting with adversarial ML attacks. What advantages does foolbox\nhave compared to cleverhans?\n\nAlthough off-topic for this work, it would be interesting to understand\nwhether MAT would be beneficial in defending against adversarial attacks\nthat consider realizable attacks (in the problem space). Figure 2 shows\nthe stability of MAT robustness for increasing values of perturbations.\nAdversarial attacks in the problem-space might need to consider\nadditional constraints while being non-necessarily constrained in a\nlp-norm [3].\n\n[1] Crecchi et al. Detecting Adversarial Examples through Nonlinear\nDimensionality Reduction. ESANN 2019\n(https://pralab.diee.unica.it/sites/default/files/crecchi19-esann.pdf)\n\n[2] http://www.cleverhans.io/\n\n[3] https://s2lab.kcl.ac.uk/projects/intriguing/ (IEEE S&P 2020)\n\n### Minor Typos\n\n\"optimizationm\" -> \"optimization\"",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "bmGLlsX_iJl": {
        "paper_id": "iclr_2022_bmGLlsX_iJl",
        "paper_title": "EMFlow: Data Imputation in Latent Space via EM and Deep Flow Models",
        "paper_abstract": "The presence of missing values within high-dimensional data is an ubiquitous problem for many applied sciences. A serious limitation of many available data mining and machine learning methods is their inability to handle partially missing values and so an integrated approach that combines imputation and model estimation is vital for down-stream analysis. A computationally fast algorithm, called EMFlow, is introduced that performs imputation in a latent space via an online version of Expectation-Maximization (EM) algorithm by using a normalizing flow (NF) model which maps the data space to a latent space. The proposed EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results for high-dimensional multivariate and image datasets are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods in terms of both predictive accuracy and speed of algorithmic convergence.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper proposes a data imputation method for MCAR and MAR data by combining EM and normalizing flows.  The paper is clearly written.  The idea is interesting and they show better performance compared to MCFlow and competing methods on ten multivariate UCI data, MNIST and CFAR10 image data.\n\nIssues regarding limited novelty compared to MCFlow was raised.\nIssues regarding the validity of Assumption 2 on the dependencies in the latent space and observation space was also raised.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "11SUrc0VF-e",
                "writer": "author",
                "reply_to": "8LLp1OznxG",
                "title": "Further questions/concerns?",
                "comment": " We thank the reviewer again for detailed read and constructive comment, and hope that you find our response satisfying. If you have any further questions or concerns, we are willing to provide more explanations and experiments. Meanwhile, if our previous response addresses your concerns, we sincerely hope that you may rearrange the score.\n\nWe want to emphasize again the novelty of this work. The latent space of EMFlow operates in a completely different way than MCFlow: the online EM _explicitly_ performs the imputation while _learns_ the parameters of the base distribution during the training. Such design leads to significant improvement on 1) imputation accuracy, 2) computational efficiency and 3) robustness to hyperparameters and initialization compared to MCFlow and other most recent competitive methods.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YwuzdRW-2Ds",
                "writer": "official_reviewer",
                "reply_to": "LiYGAQYepqF",
                "title": "Response to author follow up",
                "comment": " Thank you for the thorough discussion and for additional experiments, particularly clarifying the initialization across methods (both using NN across methods, and random initialization optimization). This paper seems broadly applicable and of interest to the community, there are slight limitations in scalability to larger datasets but overall the method seems well-justified. My score remains positive.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wTQapKwa4sE",
                "writer": "official_reviewer",
                "reply_to": "pjoIZ2kG9jS",
                "title": "Thanks for the response",
                "comment": " I would thank the authors for the effort in the response to my concerns. \nI would say sorry for some of my initial unclear comments, luckily we make it clear in the discussion session and it turns out the discussion converged at some point. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pjoIZ2kG9jS",
                "writer": "author",
                "reply_to": "1l78YK1VJ4T",
                "title": "Further response",
                "comment": " The points made by the reviewer is well taken but given the limited time and space for the paper, we can address all of the concerns in a future work. We have already empirically illustrated the computational efficiency and accuracy of our methods compared to most recent competitive methods in the area, but, clearly, more work is required in this area to elaborate on some of the issues raised by the reviewer.\n\nWe also want to note that EMFlow shares similarities with the identifiable iFlow proposed in [1]. In iFlow, the base distribution $P_Z(\\cdot|\\mathbf{u})$ is a factorized exponential family distribution where the auxiliary variable $\\mathbf{u}$ decides the natural parameters though MLP. It is also noted that the exponential families have universal approximation capabilities. In EMFlow, the base distribution is a Gaussian $P_Z(\\cdot|\\boldsymbol{\\mu}$, $\\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu}(\\mathbf{m})$ and $\\boldsymbol{\\Sigma}(\\mathbf{m})$ is conditioned on the auxiliary missing mask $\\mathbf{m}$ though the EM procedure in Equation (5). Furthermore, iFlow optimizes the marginal likelihood $P_X(\\mathbf{x}|\\mathbf{u})$, which is what we optimize in Equation (11). The difference is that EMFlow conditions $\\boldsymbol{\\mu}(\\mathbf{m})$ and $\\boldsymbol{\\Sigma}(\\mathbf{m})$ on _batches_ of $\\mathbf{m}$, such that the results of iFlow may not be directly applied to EMFlow. But it can be a direction for future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1l78YK1VJ4T",
                "writer": "official_reviewer",
                "reply_to": "kCiL8rm3zBz",
                "title": "A concern about Assumption 2, the dependencies and the correspondence of (,\n) and (,), and applying MAR to the latent space.",
                "comment": " I would thank the authors for the response and for providing the other references. However, here is my concern:\n\n**(I)**\nI knew the difference between ICA and NF. A further elaboration on the identifiability of ICA is to take us on the same page, which means that one should properly utilize the latent space, be careful about the correspondence between the latent space and observation space, and notice what information/conclusions/assumptions can be used/guaranteed not. (_In short, [1] uses the auxiliary variable to have the identifiability which is not the case of the work._ )\nUnfortunately, Assumption 2 is not the case. One should justify it and provide the condition and further discussion for it because it is fundamental and essential for the correctness of the method. For example, what does the method do, and how does it guarantee that Assumption 2 holds.\n\n**(II)** The authors distinguish correspondence from dependencies for the proposed method.\nHowever, I didn't quite get it. For example, what would be the case that $X_i$ and $S_j$ have no correspondence but the dependencies of $X_i$ and $X_j$ implies the dependencies between $S_i$ and $S_j$.\n\n**(III)** The **correspondence** matters which is used in the method. As shown in Eqn. (6), it uses the correspondence of $X_i$ and $S_j$ (in the paper it is $Z_i$), because $z_i = f_{\\psi}^{-1} (x_i)$, I guess if $x_i$ is missing, the method considers $z_i$ as missing as well by using the correspondence. Furthermore, the MAR assumption is applied to $s_m$/$z_m$ and $s_o$/$z_o$ where MAR is an assumption based on $x_o$ and $x_m$. Without the correspondence, how does the MAR hold in the latent space for the corresponding variable $z_o$ and $z_m$?\n\n**(IV)** As for the **dependencies** in the latent space and observation space, which is also assumed in Assumption 2. \nI am not convinced given the figure in [2] such that to believe Assumption 2 holds for the work in general.\nFrom the empirical result, we could say when it holds, it works. However, it is essential to include the justification, discussion, and guarantee/condition of the assumption, e.g., that why it should hold, and when it holds, as mentioned in **(I)**.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kCiL8rm3zBz",
                "writer": "author",
                "reply_to": "7QMhBH83_Bq",
                "title": "Further explanation on identifiability",
                "comment": " We very much appreciate for your efforts to provide additional feedback with more elaboration. We hope you find our further explanation on identifiability satisfying.\n\nFirst of all, although nonlinear ICA and NF share some structural similarities, they have quite different goals: nonlinear ICA is primarily used for source identification and separation, while NF is primarily used to approximate the probability distribution of the data. We also note that recently [1] bridges\u00a0the gap between NF and nonlinear ICA by developing a flow-based model for estimating latent representations with\ntheoretical results on identifiability using equivalence classes.\n\nDue to the crucial distinction between the perceived goals of ICA and NF, identifiability\u00a0of the parameters of the map is not necessarily the focus of this work. The primary goal of EMFlow is to perform imputation using posterior predictive distribution of missing values\nconditioned on the observed values using the NF to model the data distribution, and then use estimated predictive distribution on test data. Thus, identifiability of model parameters and their unique estimation may not be necessary. This is true for almost all NNs and so identifiability of parameters are usually ignored when predictions are the main goals. In fact, EMFlow only assumes a learnable correlation between the inter-feature-dependencies of the data and latent spaces, instead of coordinate correspondence (e.g., the dependencies between $X_i$ and $X_j$ imply the dependencies between $S_i$ and $S_j$ ).\n\nEmpirically, such coordinate correspondence does exist for the current implementation of this work. A nice work to show such correspondence\u00a0is [2] that concludes \"there exists a correspondence between the coordinates in an image and in its learned\nrepresentation\" (see Figure 2 in section 5). However, as explained before, coordinate correspondence is not necessary.\u00a0To address this issue, we carried out additional experiments where a random permutation layer was inserted between the affine\ncoupling layers of Real NVP, such that the coordinate correspondence would be eliminated. The empirical results show\u00a0that the introduction of such random permutations makes no significant impact in terms of accuracy of imputed values. However,\nwe also noticed that the convergence speed becomes slower possibly due to the fact that the learnable correlation becomes more complex.\n\n[1]\u00a0Li, Shen, Bryan Hooi, and Gim Hee Lee. &quot;Identifying through flows for recovering latent\nrepresentations.&quot;\u00a0arXiv preprint arXiv:1909.12555\u00a0(2019).\n\n[2]\u00a0Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. &quot;Why normalizing flows fail to detect\nout-of-distribution data.&quot;\u00a0arXiv preprint arXiv:2006.0854",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7QMhBH83_Bq",
                "writer": "official_reviewer",
                "reply_to": "CLYnDJXAEwm",
                "title": "A further elaboration and discussion ",
                "comment": " I would thank the authors for the response and would like to further discuss my main concerns. Let's see can we converge to the same page.\n\nI would like to further elaborate my point. The formulation of nonlinear ICA is: $x =f(s)$, where $x$ represents for the observed variables and $s$ represents for the noise/source signal. This is what NF is doing by using the change of variable formula: $p_x(x) = p_s(s)| det J_T (s)|^\u22121$. \n\nGiven an observed vector $[x_1, x_2,...,x_m]$, one can get $[s_1, s_2 ,..., s_m]$ with the proposed method. My question is that without certain **identifiability**, how are the dimension information preserved, i.e., should $s_i$ be corresponding to and used for interpreting $x_i$? Furthermore, how the inter-feature dependencies are preserved, i.e., the dependencies between $X_i$ and $X_j$ imply the dependencies between $S_i$ and $S_j$? For example, one non-identifiable case can be that given $p(s)$, there is an equivalent class of $f$; another case can be that given $f$, $p(s)$ is invariant to any rotation of $s$.\n\nMoreover, I didn't get how Rosenblatt transform can deal with my concern regarding the dependencies and identifiability. I would sincerely ask the authors to further explain it for further discussion.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CLYnDJXAEwm",
                "writer": "author",
                "reply_to": "P0QsASnSkq",
                "title": "Response to reviewer Twou",
                "comment": " We thank the reviewer for taking time in reviewing our work, and  appreciate the in-depth comments on the assumptions of this work. However, we believe that there seem to be significant misunderstandings of our work in the review, as outlined below.\n\n1. __\"According to the studies in factor analysis and nonlinear ICA, the latent representation is not identifiable. So it is not straightforward to believe that the dependencies in the latent space are consistent with the dependencies in the observation space\"__\n\n    It is to be noted that the NF framework is distinctly different from that of standard factor analysis, PCA or ICA and their nonlinear generalizations as the NF is built using a sequence of compositions of invertible maps. Thus, the identifiability issues are not confronted by NFs. Moreover, although its not straightforward to explicitly characterize dependence structure in the data space from those in the latent space (even though NF involves complicated invertible maps), the existence of such a map is well known (e.g, via the Rosenblatt transform).\n    \n2. __\"the neighbors in the latent/source variable space are not necessary to be the neighbors in the observation space.\"__\n\n    The NF used in this work (Real NVP) performs element-wise transformation. That is, there is a univariable transformation between $x_i$ and $z_i$ for $i=1,\\ldots,p$. So, the neighbors in the latent space are the neighbors in the data space. \n    \n3. __\"Another fact about NF is that even though the covariance matrix of NF is an identity matrix, ..., Why we could believe that they are corresponding to inter-feature dependencies in the observed data?\"__\n    \n    There is a trade-off between the complexity of the base distribution and the expressiveness of the transformations in NF. For example, to model heavy-tailed distributions, either the base distribution should also be heavy-tailed or the transformation should be more expressive than Lipschitz-continuous affine transformations ([1]). That is, we can have the base distribution to capture some characteristics of the target distribution, rather than letting the transformations to do all the jobs. In this work, we deliberately assume a full covariance matrix in the latent space and which in turn allows it to capture the inter-feature dependency in the latent space. Considering that the transformations are element-wise, we further assume that the inter-feature dependencies in the latent and data spaces can be correlated. Such assumption is verified in the experiments in the sense that the imputed data vectors (transformed from imputed latent vectors) have reasonable accuracy.\n    \n4. __\"more comparison in the experiment section would be required, especially, the works using VAE and the methods without using generative models\"__\n    \n    We added VAE-based MIWAE ([2]) as to our experiments and the results is shown in Appendix G.2. Since the authors of MIWAE didn't release the code on image datasets, we can only tune and test it on MNIST within this limited time frame. The results shows that EMFlow and MIWAE have very close results on UCI datasets, but EMFlow outperforms MIWAE on MNIST. We also note that EMFlow still converges much faster than MIWAE.\n\nReferences\n\n[1] Jaini, Priyank, et al. \"Tails of lipschitz triangular flows.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Mattei, Pierre-Alexandre, and Jes Frellsen. \"MIWAE: Deep generative modelling and imputation of incomplete data sets.\" International Conference on Machine Learning. PMLR, 2019.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rUetbqYQl0",
                "writer": "author",
                "reply_to": "WvlKhCyqo-e",
                "title": "Response to reviewer zdAh",
                "comment": " We\u2019re glad you had a positive impression on our paper. Please find our point-wise responses below to your insightful feedback.\n\n1. __\"time series dataset which also have additional layers of correlation between time points\"__\n    \n    The [Air(Quality) dataset](https://archive.ics.uci.edu/ml/datasets/Air+quality) included in our experiments is itself a time series dataset. But we admit that we didn't test EMFlow systematically on time series datasets. We understand that EMFlow and all other competing methods have an implicit assumption that each observation is independently drawn from an underlying distribution, and thus don't account for the correlations between between time points. To mitigate this issue, we think there might be at least two solutions to explore:\n    * Stack some lagged observations and apply EMFlow (like what is done for fitting AR models).\n    * Develop NFs that estimate time-dependent distributions, i.e., include time as an additional dimension in the data and latent space. There is already some interesting works in this area [1].\n    \n    These are definitely worth pursuing as possible extensions of our proposed EMflow algorithm that accounts for correlation across the vector of observations.\n    \n2. __\"The author did not show the performance in MNAR. Therefore the novelty remains limited\"__\n\n    We admit that EMFlow is not designed for MNAR as we don't explicitly model the missing mechanism. We expect significant change needed for EMFlow to work on MNAR, like including another component to incorporate the domain knowledge of the missing process and learn the missing mechanism. It is well known that handling MNAR data requires strong modeling assumptions that depends on specific nature of data collection process and hence generic version like EMflow (applicable for MAR) or any other imputation methods would not work without domain knowledge. For this reason, we initially focused our application to MAR case for broader applications.  \n    \n3. __\"The author also need to realize the limitation for all EM method comparing to FCS multiple imputation which considering the uncertainty of the imputed value\"__\n    \n    It's true that EMFlow currently only does single imputation and thus the uncertainty of imputed values is missing. But it's possible to obtain the standard errors of the parameters (i.e., the covariance matrix) estimated by EM ([2], [3]), which could be potentially used for multiple imputation in the future work.\n    \n    We also want to note that FCS methods normally need to specify a fully conditional distribution for each feature and sample from them sequentially in each iteration, which prevents FCS's application for high-dimensional data. However, it is also well known that FCs may not always uniquely determine a joint distribution or even lead to improper distributions if some regularity conditions (eg, as required for Hammersley-Clifford-Besag type results) are not met or not checked.\n    \n4. __\"one can not assume similar performance when the method is tested on longitudinal clinical data, unless tested systematically on actual data\"__\n    \n    We added a longitudinal dataset ([Web](https://www.kaggle.com/c/web-traffic-time-series-forecasting)) for in our experiments in Table 1. Although it is not a clinical one and we don't have systematical test on such data, we believe that EMFlow can be applied to longitudinal clinical datasets as long as each subject is assumed to be independent.\n\nReferences\n\n[1] Both, Gert-Jan, and Remy Kusters. \"Temporal Normalizing Flows.\" arXiv preprint arXiv:1912.09092 (2019).\n\n[2] Meng, Xiao-Li, and Donald B. Rubin. \"Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm.\" Journal of the American Statistical Association 86.416 (1991): 899-909\n\n[3] Jamshidian, Mortaza, and Robert I. Jennrich. \"Standard errors for EM estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62.2 (2000): 257-270.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zD1g9lJPSIl",
                "writer": "author",
                "reply_to": "8LLp1OznxG",
                "title": "Response to reviewer L7zK",
                "comment": " We sincerely appreciate your comments and suggestions to strengthen our paper, and hope our responses help to address your concerns.\n\n1. __\"my main concern is that the novelty of the paper seems to be limited\", \"the major difference is that this paper replaces the sampling with the EM algorithm\"__\n\n    We think sampling is part of both EMFlow and MCFlow. That is, a sample is taken from the base distribution in the latent space and then goes though the transformation layers. The major difference is that MCFlow has an \"implicit\" sampler in the latent space, which is a standard MLP. This MLP finds samples of latent vectors by maximizing the log-likelihood of imputed vectors in the data space. \n    \n    On the other hand, EMFlow has an \"explicit\" sampler in the latent space: the online EM explicitly replace the missing parts of latent vectors with the conditional means of the base distribution. Such sampling happens completely in the latent space. To achieve this, we assume a Gaussian base distribution with a general covariance matrix in the latent space that will be learned during the optimization. To the best of our knowledge, Gaussian with unstructured covariance had not been used as the base distribution of NFs for applications other than density estimation (e.g., [1], [2]).\n\n2. __\"I would be very interested to see discussions about how interpretability can be enhanced by this proposed model\"__\n\n    The interpretability of EMFlow comes from the explicit sampler in the latent space (i.e. online EM). In MCFlow, the MLP in the latent space is a black box and the difference between the input ($\\mathbf{z}$) and output ($\\widehat{\\mathbf{z}}$) latent vectors is unclear. In EMFlow, with a probabilistic model in the latent space, we know exactly how $\\mathbf{z}$ is transformed to $\\widehat{\\mathbf{z}}$, and $\\widehat{\\mathbf{z}}$ has higher log-likelihood in the latent space than $\\mathbf{z}$.\n    \n    Such interpretability bring some possibilities of extending EMFlow. For example, there is established work on obtaining the standard errors of the parameters estimated by EM (e.g., [3], [4]). It would be interesting to see how the standard errors based on latent data can be used to estimate the uncertainty of the imputation in the data space by the use of delta-method that can estimate the propagation on uncertainty.\n\n3. __\"Another minor typo\"__\n\n    Thanks for pointing it out and it's fixed.\n\n\nReferences\n\n[1] Laszkiewicz, Mike, Johannes Lederer, and Asja Fischer. \"Copula-Based Normalizing Flows.\" arXiv preprint arXiv:2107.07352 (2021).\n\n[2] Jaini, Priyank, et al. \"Tails of lipschitz triangular flows.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Meng, Xiao-Li, and Donald B. Rubin. \"Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm.\" Journal of the American Statistical Association 86.416 (1991): 899-909\n\n[4] Jamshidian, Mortaza, and Robert I. Jennrich. \"Standard errors for EM estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62.2 (2000): 257-270.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LiYGAQYepqF",
                "writer": "author",
                "reply_to": "CFYFwWY_OqB",
                "title": "Response to reviewer tdhx (part II) ",
                "comment": " 6. __\"Could EMFlow be adapted to work with low-rank covariance estimators?\"__\n\n    Yes, we think low-rank covariance estimator is a promising direction to make EMFlow scale to larger datasets. Instead of assuming a full-rank covariance matrix in the latent space, a low-rank/structured/sparse covariance estimator is more appealing for image datasets. We could possibly adapt some of the earlier works in this area as illustrated by [3] and [4]. But to make such adaption feasible, we expect notable change in the part of EM imputation as NF may no longer be strictly identifiable. \n\n7. __\"Or perhaps the authors could show further analysis of convergence of the covariance matrix estimation during optimization?\"__\n\n     We show the convergence of covariance matrix estimation in terms of Frobenius norm in Appendix G.3. \n\n8. __\"Can EMFlow be adapted to impute categorical data?\"__\n\n    It is another interesting direction of the future work of this paper. One possibility is to replace the RealNVP used in the paper with NFs that directly work on categorical features (e.g. [5], [6], [7]), and adjust the objectives accordingly. \n\nReferences\n\n[1] Ledoit, Olivier, and Michael Wolf. \"A well-conditioned estimator for large-dimensional covariance matrices.\" Journal of multivariate analysis 88.2 (2004): 365-411.\n\n[2] Won, Joong\u2010Ho, et al. \"Condition\u2010number\u2010regularized covariance estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 75.3 (2013): 427-450.\n\n[3] Zhou, Sheng-Long, et al. \"Sparse and low-rank covariance matrix estimation.\" Journal of the Operations Research Society of China 3.2 (2015): 231-250.\n\n[4] Chen, Xixian, Michael R. Lyu, and Irwin King. \"Toward efficient and accurate covariance matrix estimation on compressed data.\" International Conference on Machine Learning. PMLR, 2017.\n\n[5] Hoogeboom, Emiel, et al. \"Integer discrete flows and lossless compression.\" arXiv preprint arXiv:1905.07376 (2019).\n\n[6] Tran, Dustin, et al. \"Discrete flows: Invertible generative models of discrete data.\" Advances in Neural Information Processing Systems 32 (2019): 14719-14728.\n\n[7] Lippe, Phillip, and Efstratios Gavves. \"Categorical normalizing flows via continuous transformations.\" arXiv preprint arXiv:2006.09790 (2020).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CFYFwWY_OqB",
                "writer": "author",
                "reply_to": "22dnWja_Rf",
                "title": "Response to reviewer tdhx (part I)",
                "comment": " Thank you for appreciating the strengths of our work and the constructive suggestions for potential improvement. Please find point-wise responses to your concerns below. We also amended our Appendix in the updated manuscript based on your comments.\n\n1. __\"the paper does not compare to GAN-based imputation methods, which might outperform simpler architectures\"__\n\n    We did compare EMFlow with GAN-based methods such as MisGAN and GAIN, and spent a fair amount of time for tuning them. For UCI datasets and smaller image datasets (compared to CelebA), EMFlow outperforms MisGAN and GAIN as shown in Table 1 and Table 2 in the paper. \n\n2. __\"The tasks in the empirical section, while not trivial, are not the most difficult examples\"__\n\n    The main barrier of applying EMFlow to larger tasks like CelebA is the estimation of covariance matrix in the latent space (as you also mentioned). To have a well-conditioned covariance matrix, a batch size larger than the data dimension is desired, which is impractical for some tasks like CelebA even with the help of super batch introduced in Appendix D. \n\n    We tried some methods for high dimensional covariance estimation ([1] and [2]) but didn\u2019t get very promising results. We admit that it is the current limitation of EMFlow and making EMFlow scale to larger tasks should be the priority of future works. \n\n    But we also want to emphasize that, EMFlow performs very well on datasets of moderate sizes in terms of accuracy and the ease of training (fast convergence, robust to the choice of hype-parameters). EMFlow is also robust to initial imputation (please see our response (5) for further details). \n\n3. __\"The dimensionality and general dataset descriptions are missing from the text and should be included\"__\n\n    The description of UCI datasets was already included in Appendix E, and the dimensions of MNIST and CIFAR-10 are provided in the text of Sec 4.2. \n\n4. __\"EMFlow is able to be seeded with NN imputation, whereas MCFlow is not\"__\n\n   All the methods experimented in this paper (e.g., EMFlow and MCFlow) use the NN imputation as the starting point on image datasets. So, the use of initial NN imputation is perhaps not a significant contributing factor to the final performance difference between EMFlow and MCFlow. \n\n5. __\"Finally, to what degree is the fast convergence a result of the warm-start imputation?\"__\n\n    We added appendix G.3 to investigate the impact of warm-start on the convergence speed. Specifically, we compare the NN imputation and zero imputation as the starting point of imputation on MNIST. We find that the convergence of training loss, test set RMSE and the covariance matrix estimation under both initiation schemes remain similar and relatively insensitive, the initial differences being rather negligible. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "22dnWja_Rf",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "Official Review of Paper404 by Reviewer tdhx",
                "comment": "This paper presents a novel imputation method for high-dimensional datasets that typically serve as benchmarks for machine learning methods. This method (EMFlow) innovates by training a normalizing flow network to map input data samples to a multivariate Gaussian, where imputation is performed via an online version of expectation-maximization (EM), which is commonly used for missing data imputation. EMFlow is applied across regression tasks in datasets in the UCI machine learning dataset repository, and standard image classification datasets, MNIST and CIFAR-10. Empirical results show strong performance for missing data imputation, as well as downstream classification from these imputations, and the model design choices and well-constructed architecture make for easy training and fast optimization convergence. EMFlow builds most closely upon MCFlow, mentioned in the paper. Both models use normalizing flows (NF) to capture the input data distribution, and MCFlow uses a standard MLP to find a latent vector maximizing the log-likelihood of the missing data. EMFlow uses NFs with EM to maximize the probability of the latent vector (under a probabilistic model \u2014 a multivariate normal) corresponding to the missing data. EMFlow has strong empirical results compared to the baseline of MCFlow. While the paper does not compare to GAN-based imputation methods, which might outperform simpler architectures, the lack of comparison seems fair given the ease of training EMFlow relative to a GAN-based method. Particularly looking at convergence traces in Figure 2. Overall the text is clear, well-organized, and easy to follow; there are only minor typos.\n\nMain strengths:\nThe architecture presented here is relatively straightforward, and does not have many hyperparameters, leading to relatively easy training and implementation. The choice of a multivariate gaussian latent space distribution leads easy conditioning and marginalization, and thus lends itself well to the online EM algorithm presented for imputation. The empirical results are strong, and the methodology is appealing for the reasons listed above. While not far from MCFlows, it provides enough of a conceptual and performance improvement to be a significant contribution to that work.\n\nMain weaknesses and areas to address:\nThe tasks in the empirical section, while not trivial, are not the most difficult examples. MisGAN, for instance, highlights results on CelebA, a more complicated learning task, as well as a higher-dimensional dataset than any listed in the paper. It might strengthen the paper to present results on a higher-dimensional dataset like CelebA or others. Towards this point, it might also be worth evaluating the FID score of imputed images on a dataset like CelebA, rather than just reporting classification accuracy or RMSE.\n\nThe dimensionality and general dataset descriptions are missing from the text and should be included. Particularly of importance is to include the dimensionality of the various datasets analyzed to support the claim that EMFlow works well for high-dimensional data.\n\nThe nearest neighbor imputation in MNIST and CIFAR-10 to seed EMFLow is quite reasonable. However, this should be compared to as a baseline as well. It is not clear the difference this warm-start makes in practice. Furthermore, EMFlow is able to be seeded with NN imputation, whereas MCFlow is not. EMFlow produces much better looking and smoother images than MCFlow, which contains far more noise, on CIFAR-10, and these differences should be due to algorithm changes in EMFlow and not NN imputation. The concrete suggestion here is to either report NN imputation as a baseline method on it\u2019s own, or seed EMFlow with a random imputation as a form of ablation experiment. A third type of ablation experiment different from the two mentioned can be left up to the authors. Finally, to what degree is the fast convergence a result of the warm-start imputation?\n\nHigh-dimensional image data are thought to lie on lower-dimensional manifolds. It is unlikely that the covariance matrix ($\\Sigma$) learned in Z-space has full rank. As the data dimensionality continues to scale, it seems this methodology of estimating the empirical covariance will likely not scale as well, even with the robust estimator in (31). Could EMFlow be adapted to work with low-rank covariance estimators? Or perhaps the authors could show further analysis of convergence of the covariance matrix estimation during optimization.\n\nCan EMFLow be adapted to impute categorical data? Imputation is a key and general ML problem. EMFlow is an intuitive framework methodologically and in practice seems easy to train and performs well on real data. It is principled in its approach \u2014 relying on the rigorous EM algorithm as its base. However, EMFlow (albeit fairly due to training complexity) does not compare to what might be state-of-the-art GAN-based imputation methods. It is also unclear how EMFlow would scale to even higher-dimensional datasets given the modeling assumptions.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "8LLp1OznxG",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "Official Review of Paper404 by Reviewer L7zK",
                "comment": "This paper presents a model named EMFlow, which performs data imputation in the latent space using the online EM algorithm together with the normalizing flow models. The normalizing flow models aim to capture the complete data density $p_X$ and the bidirectional mapping between the data space and the latent space, even when the data is only partially observed. The parameters in the latent space are updated using an online EM algorithm. Thanks to the feature-wise mapping, the dependency between features in the data space is carried over to the latent space and hence the imputation can be done. Evaluation using ten UCI datasets, MNIST, and CIFAR-10 datasets show impressive improvement against baseline models and the convergence is faster than MCFlow. The quality of the paper is generally good. The proposed model combines the online EM and flow models to do imputation, which looks to me a quite reasonable design. The model is evaluated using a number of UCI multi-variable datasets and two image datasets (MNIST and CIFAR-10). The performance is generally better than baselines including MCFlow. Because sampling is avoided by using EM, it converges faster than MCFlow.\n\nThe presentation of the paper is quite clear. It is organized, well-written, and quite easy to follow. However, my main concern is that the novelty of the paper seems to be limited, especially when compared with MCFlow; it seems to me that the major difference is that this paper replaces the sampling with the EM algorithm. It is a very reasonable extension, yet it has already been widely studied in various domains and applications.\n\nThe authors claim in the introduction section that EM can be applied in an interpretable way, which motivates the authors to use EM in this paper, but this point is not further discussed in the paper. I would be very interested to see discussions about how interpretability can be enhanced by this proposed model.\n\nAnother minor typo: in the line below Eq. (2), should it be $\\subseteq$ instead of $\\in$ between $\\mathcal{X}^\\prime_i$ and $\\mathcal{X}$? The presentation is clear, the model is theoretically sound, and experiments show impressive improvement against baseline models for most of the dataset and missing rates. However, the novelty is somewhat limited, especially when compared with MCFlow.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "P0QsASnSkq",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "Official Review of Paper404 by Reviewer Twou",
                "comment": "The paper aims at imputing missing data which are MCAR and MAR. \nFor modeling the observed data distribution, it utilizes the framework of normalizing flow of which the latent variable/source variable space is Gaussian.\nBy assuming the consistency of inter-feature dependencies in the latent/source variable space, it applies online EM for the imputation of the latent space variables.\nIn the experiments, the proposed method, EMFlow is compared with GAIN, MisGAN, and MCFlow. Although the imputation of MCAR and MAR scenarios has been studied for decades and in theory is not a problem, its applications in high-dimensional and real-world data with complicated distributions can be interesting from a practical perspective.\n\nThe work adapts the online EM algorithm for the missing data imputation. I quickly go through the online EM extension, which seems sound and no problem.\n\nHowever, I have some concerns regarding the assumptions of NF.\nIt would be necessary to justify the assumptions, especially, why the two assumptions are reasonable to believe.\n\nFirstly, inter-feature dependencies in the observation space are different from the inter-feature dependencies in the latent space. According to the studies in factor analysis and nonlinear ICA, the latent representation is not identifiable. So it is not straightforward to believe that the dependencies in the latent space are consistent with the dependencies in the observation space. Then, why would it be reasonable to do imputation in the latent space?\n\nSecondly, note that according to the property of the change of variable formulation used by NF, the neighbors in the latent/source variable space are not necessary to be the neighbors in the observation space.\nAnother fact about NF is that even though the covariance matrix of NF is an identity matrix, it can still model the data distribution well. Then what are the dependencies modeled by the covariance of the multivariate Gaussian distribution in the latent space? Why we could believe that they are corresponding to inter-feature dependencies in the observed data?\n\nA final minor concern is about the experiments. To have a better view of the comparison and the benefits of the proposed method, more comparison in the experiment section would be required, especially, the works using VAE and the methods without using generative models.\n The work focuses on MCAR and MAR cases and extends NF and online EM for the missing data problem, which can be interesting for applications from a practical perspective. But the assumptions and main idea of the work can be lack justification, i.e., the relationship between the dependencies in the latent space and observation space needs to be elaborated and clarified. Moreover, a more thorough comparison with other related works would be helpful to better evaluate the proposed method.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "WvlKhCyqo-e",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "Official Review of Paper404 by Reviewer zdAh",
                "comment": "The authors propose a novel architecture EMFlow for missing data imputation. The authors also show the results of various experiments with multivariate and image datasets. Finally, the authors report the accuracy of post-imputation classification on image datasets. The study is well designed. however, there are few limitations that needs to be acknowledged.\n\nThe data structure is using imaging data as example. This kind of dataset having high neighborhood correlation and imputation using EM with latent space will be less challenging comparing to time series dataset which also have additional layers of correlation between time points. Perhaps need to show the performance on those datasets.\n\nThe author did not show the performance in MNAR. Therefore the novelty remains limited.\n\nThe author also need to realize the limitation for all EM method comparing to FCS multiple imputation which considering the uncertainty of the imputed value. The study and the expected results are acceptable; however, this framework was tested on imaging dataset, which has unique characteristics. for example, one can not assume similar performance when the method is tested on longitudinal clinical data, unless tested systematically on actual data. ",
                "rating": 6,
                "confidence": 5
            }
        ],
        "label": "train"
    },
    "NbaEmFm2mUW": {
        "paper_id": "nips_2021_NbaEmFm2mUW",
        "paper_title": "Hierarchical Skills for Efficient Exploration",
        "paper_abstract": "In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level policy pre-training with a new benchmark suite of  diverse, sparse-reward tasks for bipedal robots. We alleviate the need for prior knowledge by proposing a hierarchical skill learning framework that acquires skills of varying complexity in an unsupervised manner. For utilization on downstream tasks, we present a three-layered hierarchical learning algorithm to automatically trade off between general and specific skills as required by the respective task. In our experiments, we show that our approach performs this trade-off effectively and achieves better results than current state-of-the-art methods for end-to-end hierarchical reinforcement learning and unsupervised skill discovery.\n",
        "paper_acceptance": "accept",
        "meta_review": "The paper proposes a new way of hierarchical goal-based learning. There have been multiple examples of hierarchical strategies where higher levels set goals for the lower levels. However, often such goals are set in a quite low dimensional space, either because the state space is low dimensional to begin with or because a subset of dimensions, e.g. corresponding to the COM coordinates, are pre-specified to be the relevant ones. The paper proposes that which dimensions are relevant for the goal are task dependent, and so it should be up to the higher level policy to choose which dimensions are relevant for goal-setting. In this way, the higher level policy can make the goal for a skill more general or specific, allow a better trade-off between these factors. \n\nThe reviewers had mixed opinions initially, but additional results from the authors convinced some reviewers to update their scores, resulting in a unanimous accept recommendation. Summarizing their opinions:\n- Originality: the proposed approach is interesting & original. The authors also propose an original and interesting new suite of benchmarking tasks. \n- Technical quality: Initially, the opinions were mixed on quality, as some reviewers deemed important baselines to be missing. With the provision of additional HRL baseline, the reviewers were satisfied on quality. \n- Relevance and significance: The problem is relevant for NeurIPS. The results were not super surprising (e.g., the 'full goal space' baseline also worked pretty well across tasks), however, reviewers pointed out the paper might be an important step towards solving sparse reward task. \n- On clarity, the reviews were a bit mixed, from 'difficult to parse' to 'clear and well explained', with specific issues pointed out for possible improvement. \n\nOverall, the paper proposes an original new method and (taken into account the new results), sufficiently evaluates it in the context of relevant baselines. The paper could certainly be improved further, but I think as is it will be an interesting addition to the NeurIPS program. \n\nI had one additional minor comment: In the introduction, it is mentioned that \"In the large body of work ... on HRL ... relies explicitly or implicitly on prior knowledge that low-level skills should control the center of mass\" (lines 22-25). While I agree that this is a common assumption, I don't think it's true for the all current HRL methods as seems implied here (e.g. the option-critic lets the agent control any dimensions, feudal networks do use a subspace but the subspace is learned), and in particular, the reference given for this statement in line 25 are mostly methods where the state consists only of the COM, thus, it's inevitable that a HRL method would control the COM rather than a particular assumption. The HIRO paper, for example, would be a better example. \nHIRO: \"Data efficient reinfocement learning\", Nachum et al.\nOption-critic: \"The option-critic architecture\", Bacon et al. \nFeudal Networks: \"Feudal networks for reinforcement learning\", Vezhnevets et al. \n ",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "lyHk1Dz1QOZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "Official Review of Paper5859 by Reviewer ECxq",
                "comment": "This work proposes a benchmark task with bipedal robots instead of locomotion tasks, where the bipedal robots are low dimensional and perform various movements. It also proposes a hierarchical reinforcement learning method with three levels: a policy that specifies a goal space (a set of features to operate on), a policy to specify the goal configuration given that goal space, and a low level policy to reach the desired goal configuration. It learns the low level policies through unsupervised learning, and then optimizes off policy the high level options by optimizing the value function.  Abstract: It is not clear throughout the abstract what the \"inductive bias\" is. It is not clear how the \"the potential to facilitate exploration\" is an inductive bias. since this is the primary distinction on which this work centers, this makes the abstract confusing.\n\nIntroduction:\n24 While it is true that much of the work in exploration with hierarchical reinforcement learning deals in navigation or navigation-like domains where the center of mass of the agent is particularly useful, there also exists a large body of work which deals with exploration related to object manipulation where this assumption is not the case. Thus, the generalization expressed here is misleading, since it highlights that \"the work that benefits from center of mass knowledge has an implicit bias towards center of mass knowledge.\"\n\n30 While this is true of policies, it may not be the case that low level skills are more widely applicable, since individual skills, low level or high level might be limited to a a very small set of behavior. It would probably be more appropriate to refer to this in the context of policies.\n\nRelated work:\n77 While re-usability across different high level actions is a significant benefit, this underscores the idea that one of the large benefits of Hierarchical reinforcement learning is for exploration. Even if a primitive is only used for one task, it might be that the learning procedure is able to exhibit gains in performance or sample efficiency even without generalizability. While it is likely that navigation tasks as a test domain does introduce implicit biases, it is not clear from the description in this section why this is the case.\n\nHierarchical skill learning: \nThe \"introduction\" section for this component lacks a heading, and it is not clear exactly what it is expressing. Is this part a description of the low level policies? A description of the state space or of general terms? At this point it remains unclear what exactly the high level policy comprises, and why there are task specific features and additional objects separated out to be accessed only by the high level policy. While these components gain some clarity later, the ordering makes this confusing.\n\n107 It is not clear up to this point if the argument is being made for more specific, or less specific policies. However, while such a low-level policy as reaching a goal configuration after short time is not navigation related, it is certainly very specific to robotic motion.\n\n3.1 While it is reasonable to train low level skills over different sets of features, perhaps the most important property related to these would be how they are chosen. It is not clear if these features are selected as random samples or specified. However, random sampling seems like a suspect way, as if the number of features exceeds even a small amount the number of possible sets explodes exponentially.\n\nTraining the low level policies only by unsupervised pre-training also seems like it could introduce issues. In particular, while the low level space might afford many bad ways of controlling the agent, there are probably only a limited number of useful ways to control the agent. There should be a tradeoff between using the higher level policies to specify what is learned at the lower levels, and simply exploring with the lower level policies, but at present the former appears to be completely ignored.\n\n3.2 Equation numbers would be much appreciated in this section, especially since there are clear changes being made to the typical bellman equation/value function, not the least of which is in the notation of taking the feature mean. \n166 While matching sign seems like it should have an effect on optimization, by negating the log of the features, this seems to change the meaning of the equation. \n\n170 It is not entirely clear how the equation in line 164 arrives at the one in 170. In particular, it seems that the loss is the negative expected value, but then the log|F| component has disappeared.\n\n173 A more in-depth description of how the \\alpha and \\beta loss terms are defined is necessary. At this point, it is simply provided as a given without clear explanation, especially since the intuitive meaning of H^f, H^g is not made clear.\n\nBenchmark Environments:\nWhile these environments are provided as one of the clear contributions of this work, they are described fleetingly. It would be useful to note why properties like center of mass or other normal navigation \"implicit biases\" do not apply to these cases. \n\nExperimental results:\n211 With only 5 features, these proposed domains do not actually differ too significantly from other mujoco locomotion tasks that are more commonly given. As highlighted before, the number of features also seems necessary for this method to work since the subset sampling would explode exponentially otherwise.\n\n227 It is still unclear what emphasis is being made about \"no single skill\" In particular, it should be expected in any skill learning framework that one skill does not dominate, otherwise there would be no point in using the framework at all.\n\nFigure 4: it is hard to parse the results, in particular those of SD*. Is this meant to outperform the proposed method?\n\n5.2 The proposed baselines, while interesting, do not always capture a fair comparison. In particular they are all single level skill learning methods except for DIAYN, but in this case DIAYN-C does not appear to be ideal for this case.\n\nFigure 6: It is not entirely clear why SAC would completely fail for the given tasks. It would be useful to see a comparison against SAC where it is able to learn at least some useful behavior, or find another baseline that does give valid results. There should exist algorithms which function on humanoid walker.\n\nOverall, this work proposes an interesting way of selecting features to control over and an effective 3 level hierarchy which has encouraging results. While the idea of selecting features has been proposed, it has not been shown in a multi-level hierarchy. However, the writing is sufficiently difficult to parse such that it is difficult to determine how exactly this method is novel from existing work, except that it encodes more specific information for the tasks. It is also difficult to determine from the writing exactly what features of the proposed method contribute to the success. Furthermore, the experiments are limited because they test on a new domain against baselines that do not seem like fair comparison on that domain. \n\nOriginality: marginal\n\nQuality: marginal\n\nClarity: poor\n\nSignificance: below marginal\n This work has a limited discussion of limitations, particularly how closely the method is tied to the task being performed. It has no discussion of societal impact, though robotics has a large variety of effects on modern society so a discussion could have occurred.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "37c7nyaQeAt",
                "writer": "official_reviewer",
                "reply_to": "0uHnse0kLue",
                "title": "Follow up on response",
                "comment": " Hello Authors, your responses provided valuable insight and some assurance on the clarity of the paper. The expanded list of baselines also addresses one of my primary concerns, and I'm willing to raise my score one point.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-vHeKD8czT2",
                "writer": "author",
                "reply_to": "PpHS5gLIwlY",
                "title": "Did our response address your concerns?",
                "comment": " Hello Reviewer gM3y, we would be thankful if you can confirm that we addressed your concerns in our response, and let us know if any issues remain. To summarize, in our response we:\n- pointed to the video in the supplementary material, which contains comparisons of behaviors of HSD-3 and several baselines\n- clarified the number of seeds (which is present in the main paper but was missing for Table 1).\n- will cite Heess et al., 2016 and Hasenclever et al., 2020 in the related work section.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0uHnse0kLue",
                "writer": "author",
                "reply_to": "cX3Jz9UXZJY",
                "title": "Did our response address your concerns?",
                "comment": " Hello Reviewer ECxq, we were hoping that you can confirm whether we addressed your concerns in our response, and let us know if any issues remain. In summary, our response:\n- clarifies the notion of inductive bias we use in the paper, and the derivation of the value and policy gradients based on SAC\n- explains why we consider the comparison to existing baselines to be fair\n- clarifies our contributions in terms of novelty and our proposed benchmark suite\n\nAs discussed in our [response to Reviewer opgb](https://openreview.net/forum?id=NbaEmFm2mUW&noteId=UmJN7xl4T4R), we will also expand the list of baselines considered with end-to-end HRL algorithm (HIDIO) for the camera-ready version and add an experiment concerning exploration behavior.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dXoCjd_9KIj",
                "writer": "author",
                "reply_to": "jSswNNRiJ_Z",
                "title": "Additional HIDIO results",
                "comment": " We thank the reviewer again for this discussion. Sweeping over the hyper-parameters provided in the HIDIO paper (discrimator input: {state, state-action, action, state-difference}, latent vector dimension {8, 12}, rollout length {25, 50, 100}, replay buffer length per parallel actor {50000, 200000}, 3 different runs (seeds)) on the Hurdles task resulted in a few combinations which achieved small positive returns during evaluations. In the table below, we list the maximum evaluation return of those combinations over the course of training (8 million steps). For comparison, both SD and HSD-3 achieve returns above 12 consistently. \n\n| discr input      |   latent dim |   rollout length |   replay buffer length |   run |   max return |\n|:-----------------|-------------:|-----------------:|-----------------------:|------:|-------------:|\n| state_difference |           12 |              100 |                 200000 |     0 |         0.18 |\n| state_difference |           12 |              100 |                 200000 |     1 |         0.02 |\n| state_difference |           12 |              100 |                 200000 |     2 |         0.02 |\n| state_difference |           12 |               50 |                 200000 |     1 |         0.68 |\n| state_difference |           12 |               50 |                 200000 |     2 |         0.34 |\n| state_difference |            8 |               50 |                  50000 |     1 |         0.38 |",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_WUliaG3gV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "Official Review of Paper5859 by Reviewer opgb",
                "comment": "The authors propose a 3-level hierarchical method, one that operates on a feature space, another that operates on a feature-conditioned goal space, and finally a low level policy that outputs actions in the environment conditioned on everything above and the current state. They pretrain the policies in a pretraining environment before applying it to the task at hand. The authors also introduce a new suite of environments to test hierarchical RL. The issues brought up by the authors about current HRL methods are enlightening and the method is a novel contribution. Results are fine, with subtle/no improvements over the baselines on some tasks, and substantive improvements on others. I believe the paper has some issues with baseline comparisons and tasks, which I hope will be addressed in the rebuttal. As such, I am currently learning towards not accepting the paper.  ## Paper Strengths\n**Paper Framing/Originality**\nPointing out the issues with existing HRL algorithms\u2019 focus on learning skills that are relevant mainly for controlling center of mass is an interesting contribution. Furthermore, the algorithm that results is novel, while also incorporating ideas from modern papers such as the DYNE step-conditioned critic. The inclusion (and supposed release) of the benchmark environments are a benefit to the community as some of them seem like solid tasks to test hierarchical algorithms on.\n\n**Results Significance**\nI think the paper does a good job of demonstrating the importance of goal-space separation with results that demonstrate significant improvements over baselines on some of the tasks. Furthermore, it\u2019s rare to see evaluations over 3 seeds, while the authors evaluate on 9 seeds in total. The inductive bias experiment on HIRO in the appendix (sec G) was very enlightening.\nThe experiment shown in Figure 3 is also very comprehensive and informative.\n\n**Clarity**\nOverall, the writing is generally clear and mathematical derivations are pretty well explained. \n\n## Paper Weaknesses\n\n**Sparse-reward tasks**\nI think more analysis on similar environments with different robots should be given. Tmake general claims about this method, there should be examples of many robots, not just two. Furthermore, the demonstrating an experiment like Figure 3 for another robot (perhaps the Humanoid that\u2019s already included) would be helpful.\n\n**Experiments**\nThere should be comparisons with some works which learn skills without explicit assumptions of skill/task types and do not require pre-training skills, e.g. HIDIO (Hierarchical Reinforcement Learning By Discovering Intrinsic Options, Zhang et al. 2021), HiPPO (Sub-policy Adaptation for Hierarchical Reinforcement Learning, Li et al. 2020). These comparisons would further demonstrate the advantages of having the initial pretraining environments (which the comparison to SAC-HIRO already contributes to) and explicit goal space learning when compared with more modern HRL methods, both of which have demonstrated improvements over HIRO in their experiments. In a sense, HIRO is the only true SOTA HRL baseline in this paper so this would need to be addressed.\n\nThere\u2019s little explicit analysis that studies how exploration is explicitly affected by these skills, despite it being stressed in the introduction. This can be partially remedied by adding more analysis in Section 5.1 regarding exploration specific to each environment (I think Sec 5.1 is also just generally lacking more analysis). \n\n\n**Clarity**\nI think that an example of the feature set F should be given earlier in the paper, it\u2019s confusing to learn bits and pieces about feature sets F throughout section 3 before getting an example (perhaps at L108). This also makes the distance-based reward (L135) clearer when introduced.\n\nYou should bold or highlight the best performances for each column in Table 1. That would improve clarity here greatly.\n\n\n**Minor Issues**\nSome grammatical hiccups throughout the 3rd paragraph of the introduction, making it a little harder to read.\n\n\nFigure 4: \u201caveraged over 0.5M environment steps\u201d this should be 5M steps\n\nL271: \u201cfor SG\u201d -> \u201cfor SD\u201d? Or for subgoals?\n\nAppendix L597, equation:  F -> |F|\n\n## Questions for the authors\n\nHow do you anticipate extending this to visual environments? \n\nWhy are you normalizing the entropy of $\\pi^g$ by $|F|$? Isn\u2019t the size of $F$ fixed?\n\nIt seems that many of the fixed skill experiments achieve performance nearly on par with full HSD-3. Why is this the case when it seems in Figure 5 many skills are needed?\n\n\n--UPDATE--: raised score from 5 to 6 after response\n Yes.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "jSswNNRiJ_Z",
                "writer": "official_reviewer",
                "reply_to": "UmJN7xl4T4R",
                "title": "Thanks for the results",
                "comment": " Thanks for your response. Please do update this response whenever you get more results, but I am now satisfied that you have addressed my main complaint and will be raising my score in response, assuming these results will be finalized and added into the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cX3Jz9UXZJY",
                "writer": "author",
                "reply_to": "oXMz_sONu6H",
                "title": "Follow-up to Response to Reviewer ECxq",
                "comment": " We'd like to follow up on our response and verify whether we clarified the questions of Reviewer ECxq. We are happy to participate in further discussion if any question persists.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UmJN7xl4T4R",
                "writer": "author",
                "reply_to": "9vmtQqxXN0F",
                "title": "Results for HIDIO and Exploration Behavior",
                "comment": " We thank the reviewer for following up on the additional experiments.\n\n### HIDIO Baseline\n\nSo far, we have run HIDIO on 4 out of 7 tasks (we left out GoalWall and Stairs for now since these are more difficult, and HurdlesLimbo). The results for different discriminator features are as follows (mean values over 3 seeds, after 5M environment steps, comparable to the numbers in Table 1 of our paper):\n\n```\nDiscriminator feature  | Hurdles | Limbo | Stairs | PoleBalance\n================================================================\nState                  | -0.12   | -0.09 | -0.02  | 81.69  \nAction                 | -0.98   | -0.90 | -0.99  | 128.25\nStateAction            | -0.09   | -0.01 | -0.04  | 112.01\n```\n\nNegative returns are obtained if the Walker falls over in the course of an episode. We've used the hyper-parameters for the Pusher/Reacher experiments in the HIDIO paper, and are currently sweeping the parameter ranges given in D.1.3 on the Hurdles task. We will follow up with the results from the sweep within the next two days.\n\n### Analyzing Exploration\n\nWe ran an experiment with both (non-hierarchical) SAC and HSD-3 on the GoalWall environmet (one of the most challenging tasks of the benchmark suite), and counted the number of unique states using the SimHash method from http://arxiv.org/abs/1611.04717. After 5M steps, HSD-3 visited roughly twice as many unique states compared to SAC (400k vs. 200k, https://imgur.com/a/qqHVW6P). We will generate these curves for the remaining methods and tasks for the updated version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9vmtQqxXN0F",
                "writer": "official_reviewer",
                "reply_to": "X5cBjZt1HE2",
                "title": "Results table for other HRL",
                "comment": " Thanks for the response.\n\n> We thank the Reviewer for pointing out further opportunities to clarify our contributions. We are currently performing further training runs with HIDIO and will follow up with a results table shortly.\n\n\n> Furthermore, we will attempt to visualize the state space covered over time by counting ``unique'' states via pseudo-counts (using a hash function) for all the algorithms in Table 5 (and HIDIO, if possible).\n\nDo you have a follow-up with these results? I would appreciate if the authors are able to add these results table as we are now in the last week of reviewer discussion and this would be taken into consideration in my evaluation of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UcT1T4U4Hvx",
                "writer": "author",
                "reply_to": "uXbayW9K0D",
                "title": "Response to further questions of Reviewer tdmg",
                "comment": " These are all good questions, and we are happy to answer them. \n\n1. For the first point raised, we refer to our answer to Reviewer opgb below ([link](https://openreview.net/forum?id=NbaEmFm2mUW&noteId=X5cBjZt1HE2)).\n\n2. We thank you for pointing out the ALLSTEPS reference, which does indeed eschew motion capture data, but in exchange for a dense, carefully designed reward function and task curricula. We will include it in our related work section.\n\n3. A forward reward alone is not sufficient for either GoalWall or PoleBalance, and HSD-3 achieves good results in both of them (with the Walker robot). The focus on sparse-reward environments is motivated by the fact that as tasks grow in complexity, coming up with suitably shaped rewards requires an increasing amount of effort. Naturally, deriving good rewards from e.g. demonstrations is another way to tackle complex tasks, but this direction is orthogonal to our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "X5cBjZt1HE2",
                "writer": "author",
                "reply_to": "-YGT4kQGiiN",
                "title": "Response to further question of Reviewer opgb",
                "comment": " We thank the Reviewer for pointing out further opportunities to clarify our contributions. We are currently performing further training runs with HIDIO and will follow up with a results table shortly.\n\nRegarding analysis of exploration behavior, we would first like to stress that our sparse-reward tasks require efficient exploration for success in the first place. The results in Figure 3 can be directly related to the learning curves in Table 5 (Appendix). We admit that full learning curves would provide further insight into how different goal spaces affect learning in each environment, and we will add such curves for the results in Figure 3 in the Appendix. Furthermore, we will attempt to visualize the state space covered over time by counting ``unique'' states via pseudo-counts (using a hash function) for all the algorithms in Table 5 (and HIDIO, if possible).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uXbayW9K0D",
                "writer": "official_reviewer",
                "reply_to": "YyaCJgN6Gwq",
                "title": "further questions",
                "comment": " Thanks for your response. Some questions remain:\n\n1. I think reviewer opgb raises a good point about analysis of how analysis is affected. It will be good if the authors can comment on this point. And I may adjust my score based on the response to that question.\n\n2. In the response to reviewer ECxq, the authors claim \"Our work demonstrates skill learning and usage for a Humanoid robot without supervision from motion capture data, which has not been shown previously outside of navigation tasks.\" That is not true. Please refer to literature in computer animation. For example, ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills, by Xie et al can do more challenging locomotion tasks presented here without motion capture.\n\n3. While I understand the scope of the paper is to solve sparse reward tasks, but my question remains, most of the tasks presented can be solved via very simple forward progress reward (this is not a complicated inductive bias). And the only task that I couldn't figure out a simple dense reward alternative is not solvable by the proposed approach. I think this limitation should be addressed, e.g, in the appendix.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-YGT4kQGiiN",
                "writer": "official_reviewer",
                "reply_to": "PxmK2het2rL",
                "title": "Response to authors (1)",
                "comment": " Thanks for clarifying my questions and addressing some of my concerns. Regarding some of your comments, I have a few things to point out.\n\n> Regarding HiPPO, the experiments in the original paper (https://openreview.net/forum?id=ByeWogStDS) are performed in environments where PPO works well already, while non-HRL methods seem to fail almost completely in our tasks. We hence did not consider it as a baseline.\n\nI don't think that its experiments being performed in environments where PPO already works well implies that it'll fail on your tasks. In HIDIO's experiments, in which I believe they run an unmodified version of HiPPO, HiPPO performs well on some tasks where flat policies fail. In fact, some of the tasks in HiPPO are semantically similar to the ones evaluated in your environment (there's a \"Block Hopper\" and \"Block Half-Cheetah\" in HiPPO). However, I think it's OK to just include one modern hierarchical baseline, which HIDIO satisfies. Specifically regarding HIDIO:\n\n> In quick preliminary experiments on Hurdles, Stairs, and GoalWall, using the actions as input to the discriminator, HIDIO did not make meaningful progress in 5M steps apart from learning to not fall over. We will do additional runs using the parameter ranges and variants listed in the paper and add it as a baseline for the camera ready.\n\nCan you include a preliminary table of results in a future response to this comment? I understand it's hard to setup your environment tasks and perform a full hyperparameter search for any method with an unfamiliar codebase in a short amount of time, so feel free to just put preliminary results, but not having the extra baseline is my primary concern and I would like to see it fully addressed. \n\n\nFurthermore, the authors did not address this concern of mine:\n> There\u2019s little explicit analysis that studies how exploration is explicitly affected by these skills, despite it being stressed in the introduction. This can be partially remedied by adding more analysis in Section 5.1 regarding exploration specific to each environment (I think Sec 5.1 is also just generally lacking more analysis).\n\nCould you address this in a response? I think Section 5.1 should be expanded upon, or given limited space, an additional pointer to the appendix added and extra analysis inserted there. It's very interesting as is.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PxmK2het2rL",
                "writer": "author",
                "reply_to": "_WUliaG3gV",
                "title": "Response to Reviewer opgb",
                "comment": " We thank the reviewer for their valuable feedback and insightful comments. We will address the potential weaknesses that were pointed out individually:\n\n**Sparse-reward tasks:** In general, we view our benchmark task as a first installment and to be potentially extended with further tasks and robots in the future as we agree that methods should be benchmarked in as many scenarios as possible. For this work, we limited our focus on bipedal robots of different complexity. The experiment on individual goal spaces (Figure 3) has also been performed for the Humanoid, although we limited the number of candidate goal spaces to those including translation in X direction. The results are provided in Figure 8 in the Supplementary and paint a similar picture (no single best goal space across tasks).\n\n**Experiments:** The extended results in the Supplementary (Table 5) include an additional end-to-end baseline (Switching Ensemble, from http://arxiv.org/abs/1909.10618). While it doesn't learn a high-level policy, it has been shown to be on par with learning a set of discrete low-level options in http://arxiv.org/abs/1909.10618. Regarding HiPPO, the experiments in the original paper (https://openreview.net/forum?id=ByeWogStDS) are performed in environments where PPO works well already, while non-HRL methods seem to fail almost completely in our tasks. We hence did not consider it as a baseline. HIDIO was published only recently (ICLR 2021). In quick preliminary experiments on Hurdles, Stairs, and GoalWall, using the actions as input to the discriminator, HIDIO did not make meaningful progress in 5M steps apart from learning to not fall over. We will do additional runs using the parameter ranges and variants listed in the paper, and add it as a baseline for the camera ready.\n\n**Clarity:** We provide an example of our skills in the introduction, but will also pick it up again in Section 3 for clarity. We will also address the minor issues that the reviewer helpfully pointed out.\n\n**Questions:**\n- *Visual environments:* Image observations don't directly allow for definitions of goal spaces as in our work. We think that investigating whether these goal spaces can be learned would be worthwhile, as a means to both remove the remaining prior of manual goal space in our method, and to tackle more complex state spaces such as images. Another possibility would be unsupervised keypoint extraction, e.g., as in \"Unsupervised Learning of Object Keypoints for Perception and Control\" (http://arxiv.org/abs/1906.11883), and to construct goal spaces around those keypoints. For the particular environments in our paper, it would also be possible to restrict pre-training tasks to proprioceptive inputs and supply the high-level policy with image observations of downstream tasks, such as in \"Hierarchical Visuomotor Control of Humanoids\" (https://openreview.net/forum?id=BJfYvo09Y7).\n- *Normalization of entropy:* The size of \\mathcal{F} and the total number of goal space features is fixed, but the subsets F \\in \\mathcal{F} differ in size. For example, for F={xpos}, there is only one continuous action for \\pi^g, while for F={xpos,zpos,left_foot} there are 4 actions (left_foot corresponds to two actual features). We normalize the entropy by |F| to not bias the Q-function towards large feature subsets, which would be the case for the standard SAC formulation.\n- *Many skills achieve good results:* All our individual skills are goal-based policies in a continuous goal space and can be used to express a relatively large range of different motions on their own. For example, it's possible to find goal sequences to move forward in X direction by just controlling a single foot (Fig. 3). This is because the skill policy needs to balance the agent to not fall over, and other features, like the torso position, are not constrained to specific values in this case. Finding these goal sequences can however be challenging, which is demonstrated by the mediocre performance of HSD-Bandit. HDS-3 is free to switch between different skills (i.e., goal spaces), and in Figure 5 we demonstrate that semantically meaningful switching sequences can arise. Here, we don't place any constraints on finding a small set of best goal spaces.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YyaCJgN6Gwq",
                "writer": "author",
                "reply_to": "tIkES810p5r",
                "title": "Response to Reviewer tdmg",
                "comment": " We thank the reviewer for their valuable feedback and comments. We will address the smaller issues pointed out in the write-up and provide detailed answers to the listed cons and questions below:\n\n**Comutational requirement for pre-training:** For the Walker, we use 10M steps for pre-training a shared policy for all goal spaces considered. With uniform sampling over goal spaces, this leads to an expectation of about 322k environment steps per goal space, or skill. Learning a single skill (e.g., moving towards an X-position) in an isolated manner with SAC required significantly more samples in preliminary experiments, and this only increases for more complex skills. Considering the fact that we learn skill policies for 31 goal spaces simultaneously, we think that the resource requirements for pre-training are reasonable.\nFor clarity, we will also attach per-skill learning curves with the average rewards reached (in training) to the Appendix.\n\n**Task design:** We agree that designing these tasks is non-trivial, and we consider the investigation into further tasks (especially for Humanoid robots) as a worthwhile future endeavor. We believe that the current suite represents a good first step with 7 varied environments.\n\n**Performance on Gaps:** This task is indeed very challenging since a positive reward is only observed if the robot reaches the next platform. If it touches the Gap (slightly lower than the floor), it receives the same -1 reward as for falling over.\n\n**Performance gain over non-HRL with forward reward:** So far, we found that in dense-reward settings (Hurdles, Limbo, HurdleLimbo, Gaps, Stairs with a reward corresponding to the translation in X-direction per step), the final performance of our method and other HRL approaches we investigated is lower than for plain SAC. We believe this is mainly a limitation of using fixed low-level skills. In the future, it would be interesting to study how to adapt the low-level skills on downstream tasks, or how to use the HRL policy purely for exploration to get good initial traces; we believe this to be out scope for this submission though, as the focus is on improved exploration in sparse-reward settings.\n\n**SAC on Stairs:** Only one out of 9 seeds managed to make meaningful learning progress on the Stairs task, which is not enough to significantly move the average reported in Figure 4. It can be noticed in Table 1, and we will add a corresponding remark in the discussion of results in 5.2 for clarity.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PpHS5gLIwlY",
                "writer": "author",
                "reply_to": "tSEJJQ_Rrh",
                "title": "Response to Reviewer gM3y",
                "comment": " We thank the reviewer for their thoughtful feedback and comments. We kindly refer the reviewer to the detailed video presentation in the Supplementary ZIP archive (file hsd3.mp4) and will add a corresponding reference in the main body. We show rollouts of the pre-training stage and downstream tasks for both robots, and compare the behavior of HSD-3 to several baselines.\n\nWalker experiments were run with 9 seeds each (for methods with pre-trained skills we use 3 pre-training and 3 high-level training seeds). A lower bound of the number of episodes can be derived from the time limit: for the Walker, 5M steps correspond to at least 5000 episodes (time limit 1000) for all tasks except GoalWall (at least 20000 episodes, time limit 250). All environments implement early termination if the robot falls over, so in practice the total number of episodes will be higher.\n\nWe will add a motivation for the selection of our goal space features, and will also reference Heess et al., 2016 (skill learning with a navigation prior) and Hasenclever et al., 2020 (highlighting importance of in-domain motion capture data (as a prior), and proposing joint skill learning and downstream task training as a remedy). We chose the s^+ notation to highlight the fact that it represents additional information introduced by downstream tasks.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oXMz_sONu6H",
                "writer": "author",
                "reply_to": "lyHk1Dz1QOZ",
                "title": "Response to Reviewer ECxq",
                "comment": " We thank the reviewer for the extensive commentary and suggestions. We group the issues that were raised and respond to them individually.\n\n**Inductive bias:** What we call inductive bias is the prior that is used for exploration on downstream tasks, and hence represents a priori knowledge about what comprises useful behavior in a given environment. Settling for more generality will make pre-trained skills applicable to a larger class of environments, but exploration will be more challenging. Hence, a trade-off arises. For example, controlling the center of mass is effective in navigation environments but not helpful for kicking a soccer ball. This would intuitively require control of the robot's extremities, which is again not useful for exploration in navigation tasks. We will try to present this in a clearer manner in the introduction.\n\n**Novelty:** We clearly work out the role of priors in pre-trained skills, and propose a novel three-level architecture to effectively tackle the trade-off that arises when introducing these priors. Our work demonstrates skill learning and usage for a Humanoid robot without supervision from motion capture data, which has not been shown previously outside of navigation tasks.\n\n**Baselines:** The failure of SAC is explained by the sparse-reward nature of our tasks that require agents with effective exploration capabilities. Occasional SAC runs do make progress on the Stairs task, and after a longer training time on Hurdles (Table 5 & 6). In Table 5, we also give results for the Switching Ensemble (from http://arxiv.org/abs/1909.10618), which improves exploration for SAC and finds effective solutions occasionally. We believe that this answers the reviewer's point on providing a simple baseline that works at least sporadically. DIAYN-C embeds a fixed number of discrete skills in a continuous space, and can hence interpolate between them (http://arxiv.org/abs/1807.10299); we regard it as a superior formulation of DIAYN. In contrast to the baselines, our method is novel in its usage of multiple skills, with each one implemented as a goal-based policy. We are not aware of any similar multi-skill algorithm that would be applicable to our scenario. SD* is considered a topline because it requires exhaustive evaluation on a downstream task -- it is the best goal space, selected a posteriori.\n\n**Benchmark Tasks:** While locomotion is an integral part of the majority of tasks, it is not sufficient to perform well across all of them. This is demonstrated in Figure 3: controlling the center of mass roughly corresponds to controlling X,Y and Z features which does indeed work well on 4 out of 7 tasks, but works poorly on the other 3. The Gaps task, for example, clearly requires control of at least one foot. Further, we can't completely follow the connection drawn by the reviewer between the number of goal space features and the similarity to existing MuJoCo locomotion tasks. The tasks are defined irrespective of the goal space features, and the features have been selected to enable a variety of behaviors for Walker robots (position of torso and the two lower appendages).\n\n**Unsupervised pre-training; guiding skill learning with high-level policies:** In this work, we focused on the scenario of first acquiring pre-trained skill policies, and then utilizing them in unseen downstream tasks. In preliminary experiments, running HSD-3 with uninitialized skills (i.e., from scratch) did not work well; perhaps this could be mitigated with additional inductive biases on which skills should be learned at what point in time, or with a CoMic-like setup (http://proceedings.mlr.press/v119/hasenclever20a.html).\n\n**Section 3.2:** The equations are taken from the SAC paper (https://arxiv.org/abs/1812.05905v2) and adapted to our setting. In the interest of brevity, we do not include the full derivation of the SAC losses but refer to the original paper instead. We will motivate the presence of two temperature loss terms in the camera-ready version; the main idea is that the two high-level policies are sufficiently different and benefit from independent entropy regularization.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tIkES810p5r",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "Official Review of Paper5859 by Reviewer tdmg",
                "comment": "This paper presents a hierarchical framework for training locomotion policies for tasks with sparse reward. Low level policies are trained to achieve a selection of subgoals and high level policies are trained by exploring in the goal spaces. Challenging locomotion tasks are introduced to show the effectiveness of the proposed methods.  Originality: This paper presents a three level hierarchical policy with a low level policy generated via training on general tasks. This is new to me.\n\nQuality: The paper is technically sound to me.\n\nClarity: Paper is well written.\n\nSignificance: The result is not very impressive since most tasks can be solved via a simple forward progress reward. But it presents an important step towards solving tasks with sparse reward using a hierarchical policy.\n\nPro:\n\n1. The training of low level policies based on goal spaces specified by position of torso and foot.\n\n2. A three level hierarchical policy and an extension of SAC to accommodate the resulting action distribution.\n\n3. Challenging sparse reward tasks for locomotion.\n\nCon:\n\n1.  The computation required for training even the low level skills are significant. It will be nice to provide more details on the low level training. The low level skills do not seem challenging to me (at least for the walker) so it seems 3 days on 2 GPUs is a lot.\n\n2. Tasks such that designing a simple dense reward (such as the forward progress reward) is none trivial will really demonstrate the strength of hierarchical framework. e.g., the GoalWall task presented. More in Question 2 below.\n\nQuestion:\n\n1. It is not clear to me why Gap will be more challenging than other tasks. Maybe the reward design needs to be modified? e.g., a negative reward for touching the gap can cause the robot want to terminate as soon as possible. Some failure mode of the gap task will help illustrate the issues.\n\n2. Most of the tasks can also be solved via a simple forward progress reward (with the exception of GoalWall, which is difficult to solve in the current framework anyway). It will be interesting to see what is the performance gain the proposed approach has over baseline methods under this setup.\n\n3. In Figure 6, SAC makes no progress on Stairs, but that is not the case in Table 5 in the Appendix. The video also shows SAC makes some progress on Stairs. Yes, limitation is discussed.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "tSEJJQ_Rrh",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "Official Review of Paper5859 by Reviewer gM3y",
                "comment": "This paper argues that, in the context of pre-trained low level skills, there is a trade-off between generality and learning speed. It makes two contributions: firstly, it proposes a benchmarking suite of sparse reward tasks that require different motor skills to study this trade off. Secondly, the paper proposes a hierarchical skill learning algorithm that attempts to trade off generality and learning speed directly. The proposed algorithm outperforms reasonable baselines.  This paper argues that, in the context of pre-trained low level skills, there is a trade-off between generality and learning speed. It makes two contributions: firstly, it proposes a benchmarking suite of sparse reward tasks that require different motor skills to study this trade off. Secondly, the paper proposes a hierarchical skill learning algorithm that attempts to trade off generality and learning speed directly. The proposed algorithm outperforms reasonable baselines.\n\nThe proposed benchmark environments are a number of obstacle courses (e.g. stairs, gaps, hurdles) as well as a pole balancing task. The tasks are set up for the 2d walker as well as a 3d humanoid.\n\nThe proposed algorithm pre-trains skills first defining a set of features. Then the agent is trained to control subsets of the full feature set. The subsets and target features are randomly sampled during pre-training. Selecting the feature set induces and the target features induce an inductive bias. I would have like to see more discussion of how the features and target feature distributions are selected. As an aside, I think for more complex embodiments, I think using demonstrations or mocap data to define the targets would be an interesting approach. To reuse the learned skills, the skills are frozen and a hierarchical policy is learned that first selects the subset of features to control and then specifies a target. \n\nI think the related works section is reasonably comprehensive but should cite\n\n    Heess et al.  \u201cLearning and Transfer of Modulated Locomotor Controllers.\u201d arXiv Preprint arXiv:1610. 05182.\n\nas an early example of low level controllers in deep RL. Another relevant paper is\n\n    Hasenclever et al. \u201cCoMic: Complementary Task Learning & Mimicry for Reusable Skills.\u201d ICML 2020. http://proceedings.mlr.press/v119/hasenclever20a.html. \n\nthat studies (among other things) the same trade-off between generality and learning speed in the context of skills acquired from mocap data.\n\nIn a first experiment, the performance with different fixed goal spaces is compared. The results indicate that no single goal space works best on all tasks: there is indeed a trade-off between speed and generality. However it's worth noting that the full goal space works pretty well across tasks. In the other experiments, the proposed methods performs favourably relative to baselines. \n\nOverall, I think this is a nice paper and I am leaning towards acceptance.\n\nMinor comments:\n- page 4, section 3.1 last paragraph: I'm a little confused by this. If you don't reset why call the experience segments episode? Isn't it just one long episode at that point?\n- Figure 1: I found the $s^+$ notation confusing. Why not $s^t$\n- I would quite like to see some videos of the resulting skills and policies. This is something that other papers in this space provide\n- Table 1: How many seeds and episodes do your experiments correspond to?\n- Figure 6: typo \"are now show\"\n The experimental results with the humanoid are limited in terms of the goal spaces and the full goal space baseline appears to work just as well if not better. This suggests that substantially more work is needed to scale up to more complex embodiments. This is acknowledged by the authors.",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "test"
    },
    "YYHXJOawkPb": {
        "paper_id": "iclr_2022_YYHXJOawkPb",
        "paper_title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning",
        "paper_abstract": "Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit \u201ceffective robustness\u201d and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit  effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.",
        "paper_acceptance": "Reject",
        "meta_review": "Thank you for your submission to NeurIPS.  The reviewers are quite split on this paper, but some remain substantially negative even after discussion.  I'm a bit more optimistic about the paper: the observed increase then decrease in ER during fine-tuning _does_ strike me as a fundamentally interesting phenomenon, and I believe that papers that present such phenomena can be valuable contributions even without more fundamental \"explanations\" of the observations.  My recommendation, therefore, ultimately rests largely on the fact that I think (as is honestly evidenced by the reviews to a large degree), the presentation and contextualization of these results can be substantially improved in a future revision of the paper.  Specifically, the fact that several reviewers found the results obvious and/or not sufficiently substantiated suggests that the basic premises here are still failing to land.  I would strongly suggest revisions that clarified these points in a resubmission.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "AJfZMLnWW0t",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "Official Review of Paper3428 by Reviewer XPbn",
                "comment": "In this paper, the authors conduct a thorough empirical investigation of effective robustness during fine-tuning and have several observations: 1. models pre-trained on larger datasets in the middle of fine-tuning, as well as zero-shot pre-trained models, exhibit high amounts of effective robustness, but the effective robustness vanishes at convergence; 2. the effective robustness increases with the larger size, more diversity, and higher example difficulty of the dataset; 3. models that have effective robustness make different predictions than standard models and are able to correctly classify examples that no standard models get right. Besides, they discuss several potential solutions to mitigate the problem of vanishing of effective robustness during fine-tuning, but find that none of them are able to maintain high effective robustness at high in-distribution accuracy.  I think this paper has the following strengths: \n\n1. I think identifying models that have effective robustness and understanding their properties is an important and interesting problem. This paper has some empirical observations under this direction. \n\n2. Enough details are included for the experiments. \n\n3. Overall, the paper is well-written and the related work is properly discussed. \n\nHowever, I think this paper has the following weaknesses: \n\n1. My major concern is that the contribution is not very significant. The authors have some empirical observations, but those observations are not very useful and don't help us understand the problem better. For the models in the middle of fine-tuning, although they exhibit a high amount of effective robustness, the accuracy of those models on the in-distribution dataset is not high and thus such kinds of models may not be useful. Also, when the fine-tuning converges, the models have high accuracy on the in-distribution dataset but don't have effective robustness. Thus, the models obtained via fine-tuning don't have clear advantages over previous models. Besides, although the authors discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy, none of those methods work. So such a discussion may not be useful.\n\n2. They only have some empirical observations, but don't have analysis for them. For example, they only show that the effective robustness generally increases throughout fine-tuning, peaks, and then gradually disappears towards the end of fine-tuning empirically, but don't analyze or explain why such a phenomenon exists. It is unclear whether such a phenomenon is general or it just exists on some datasets. It seems Figure 3 in the paper shows that such a phenomenon doesn't exist on ImageNet-R and ObjectNet when using ImageNet as in-distribution. \n\n3. Some claims are not well supported by results. For example, the authors claim that the pre-trained models in the middle of fine-tuning, as well as zero-shot pre-trained models, represent an entire class of models that exhibit high amounts of effective robustness. I think this claim may not be true. There might be other training methods that could lead to better effective robustness and also high accuracy. The authors only explore the methods of fine-tuning and zero-shot evaluation. Thus, it is hard to claim that they represent an entire class of models that exhibit high amounts of effective robustness. The claim that the models with effective robustness make different predictions than standard models and are able to correctly classify examples that no standard models get right, is also not well supported by the results. They only select 4.8% of images that none of the testbed models get correct and show that the model that has effective robustness with the best in-distribution performance gets 10% of these examples correct. I think such results cannot support the claim that the models with effective robustness are able to correctly classify examples that no standard models get right since only 10% of these examples are predicted correctly by the model that has effective robustness with the best in-distribution performance. Also, it seems these results could not demonstrate that the models with effective robustness have prediction diversity. \n\n4. Some observations may already be known to the community. For example, the observation that the effective robustness increases with the larger size and more diversity of the dataset seems obvious. \n\n I think this paper doesn't make enough contributions and the claims are not well supported by results. Also, they don't provide analysis for the observations and the observations may not be helpful in understanding the problem. Thus, I think this paper is not ready for publication. \n\n***[Post Rebuttal]***\n\nAfter reading the rebuttal, I think my major concerns still remain: the contributions are not very significant and the findings may not be useful. I still think that the models studied in this paper are not enough to represent all models that exhibit ER. The authors need to explore other kinds of models that have ER (and also have high accuracy). Thus, I keep my original rating and think the paper is not ready for publication. ",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "kqHAA0FV4bI",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "Official Review of Paper3428 by Reviewer AB8m",
                "comment": "This paper highlights important variables impacting the effective robustness (ER) of a pre-trained, fine-tuned model. \nThe authors identify that increasing model size, dataset size, and example difficulty improves the ER of a pre-trained, fine-tuned model.\nThe experiments suggest that the zero-shot component of CLIP plays a significant role in the high value of ER CLIP achieves.\nThe investigation of ER on dominance probability shows that models with high ER have high dominance probability.\nThe authors also present a negative result showing that several reasonable approaches to maintaining high ER while fine-tuning fail. Strengths:\nThe paper is very clearly written and has a thorough experimental section validating the authors' claims.\nThe authors have a thorough selection of experiments that validate their claims.\n\nWeaknesses:\nOne weakness of this paper is that the authors do not properly define fine-tuning. While its meaning is implicit, fine-tuning is a key concept in this paper, so having a clear definition of the term seems necessary. This is especially true when considering multiple fine-tuning steps such as when fine-tuning a fine-tuned model BiT-M-21k on CIFAR-10.\n\nThe authors use a pre-trained or randomly initialized model on a large dataset, fine-tune on a smaller dataset and measure OOD accuracy on an analogous dataset to the fine-tuned dataset. It would help if the authors give some examples of when such a training procedure would be useful. Usually fine-tuning is carried out on the distribution that the model is going to be evaluated on.\n\nAn analysis of the relation between the fine-tuning dataset and the OOD test set would be useful. Right now the relationship is alluded to based on natural distribution shifts, but it's not clear how this might generalize to other types of distribution shifts. Overall this paper is very thorough. The authors set out to investigate the role fine-tuning has on OOD robustness and they successfully identify several key variables to consider. There are many experiments in the main paper as well as in the appendix that validate their claim. This work will be very valuable to the community as it provides some insight into what variables lead to OOD robustness for pre-trained, fine-tuned models.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "ifpIjM6pki3",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "Official Review of Paper3428 by Reviewer kcNT",
                "comment": "In the manuscript entitled, \"The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning\", the authors present an empirical investigation of model exhibiting a property known as 'effective robustness'.  In particular, their focus is on how 'effective robustness' changes during fine tuning and on the characteristics of these models. Apologies to the authors for what may sound like a rather glib judgement of this submission (and for which I acknowledge through the confidence scores below that my opinion is not absolute as I do not work directly in the space of image classification), but the results and conclusions of this paper seem remarkably obvious.  Namely, that models that have been pre-trained on a large collection of different datasets tend to lose their strengths at predicting out of distribution as they are progressively fine-tuned towards predicting a specific type of data.  And that when these models are performing in the 'effective robustness' mode the types of in sample problems they find easy (alt. hard) are different to those that models trained on the dataset at hand find easy (alt. hard).  The perils of over-fitting to a particular training set are well known and strategies to avoid this and improve generalisation are a major component of ongoing work in the machine learning (see e.g. Roger Grosse's comp sci lecture notes: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L09%20Generalization.pdf ).  To change my mind on this point would require additional discussion by the authors to connect this work to general principles of machine learning and establish the novelty of the insights reached from these numerical experiments.\n\nThat said, my many years in research have taught me that sometimes results that seem to me to be 'remarkably obvious' are actually not so for the general audience, and that 'simple' examples demonstrating such principles can actually have a large impact and generate huge citation indices.  I mean this genuinely; not trying to be cynical here.  So for that reason I would respect the decision of other reviewers and the AEs if this paper was in fact recommended for the conference series.  Certainly, having a reference to point to for e.g. the fact that self-driving cars probably shouldn't spend too much time refining their algorithms to over-fit to a commuter's every day journey to work (this being inevitably at the expense of performance when he/she wants to take a drive in the countryside), could actually be very useful. Conclusions seem 'obvious' to this reviewer, but willing to consider other opinions.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "iTwhMw28rWC",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "Official Review of Paper3428 by Reviewer cHRW",
                "comment": "\nThe paper conducts an empirical study into an interesting problem of robustness of deep models on out of distribution data. The paper finds that the pre-trained models exhibit better effective robustness during training which will disappear upon convergence of the same models.\n ### Summary:\n\nThe paper conducts an empirical study into an interesting problem of robustness of deep models on out of distribution data. The paper finds that the pre-trained models exhibit better effective robustness during training which will disappear upon convergence of the same models.\n\n### Pros:\n\nThe paper is well-written, easy to understand and follow along.\n\nMost significant of all is that this paper has an extensive study of various initializations with pretrained models for vision problems.\n\nThe breadth of explorations such as pre-trained models ER during training, data set size, example difficulty, \n\n### Cons:\n\nare there proper bounds for the ER values, what would it really mean to have higher value, lower value etc, can you briefly explain?\n\nER ~ 0 for CIFAR-10 (Figure 3a) at the end of training, exactly the point at which any of the models are having the corresponding best accuracies on IN set. The only difference at that point is, the accuracy of various models is different which is already known and well-studied in literature. Similar for Imagenet, it is visible at low accuracy, and the trends are visible as the accuracy gets better similar to CIFAR. The question is, why should anyone care, if the ER is high in the middle of training at low accuracy? This is not well-justified in the current version of the paper. Also, the reasons for the peaks in ER during training are not justified, why are they intriguing? -- is it because the pre-trained models change significantly to the down-stream tasks, or something else? The random initializations don\u2019t fluctuate that much, why not investigate these observations in detail?\n\nIn Figure 4b, why further fine-tune only the BiT-M-1k model, what happens if you further fine-tune all the models? This experiment is not a fair comparison, not all models see the same amount of data.\n\nAgain, in Figure 4c and the corresponding appendix, why would anyone use a low accuracy classifier when one knows it will perform bad on the hard to classify examples, in that case ER is not even a thing to worry in the first place, accuracy becomes the first concern. Fine, at least the ones that the classifier can classify, there is better robustness but not entirely convincing though. \n\nThis paper relies heavily on Taori et al. (2020), which seem to have a number of unresolved concerns, most important of all is that the paper is a bit short on novelty, however, the empirical study in itself is interesting.\n\nShow the same findings hold for at least one more domain, for example, NLP.\n\nOverall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings.\n\n Overall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings. Also, the paper lacks novelty or detailed analysis of the proposed concepts. I would give it a score of 4.",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "val"
    },
    "HkeuD34KPH": {
        "paper_id": "iclr_2020_HkeuD34KPH",
        "paper_title": "SSE-PT: Sequential Recommendation Via Personalized Transformer",
        "paper_abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.",
        "paper_acceptance": "reject",
        "meta_review": "The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.\n\nThe paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3's concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper. \n\nIn the AC's opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SJenAyg6tH",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #3",
                "comment": "The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. \n\nThe novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. \n\nThe experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. \n\nUpdate: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score. ",
                "rating": 1,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "S1ej5Lf7jB",
                "reply_to": "B1xkMwxJYB",
                "title": "Personalization of NLP models (such as Transformer and BERT) is an important research direction",
                "comment": "Hi Reviewer, thank you very much for your constructive and just-to-the-point feedback. \n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nWe will definitely correct the typos in our final version of the paper and will include more baselines such as Fossil, MARank, and/or BERT4Rec into our final version of the paper. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rylW9BGXoS",
                "reply_to": "Hyg-LKcJcS",
                "title": "Exponential decay idea does work empirically better than Uniform one",
                "comment": "Hi Reviewer, Thank you very much for your insightful feedback and suggestions.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nYes, we should definitely include CIKM'19 BERT4Rec in the final version of the paper, which we were not aware of. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n\nAlso, your idea of sampling start index $v$ based on the recency (e.g., with exponential decay) sounds very intuitive and could be very promising. We did a quick experiment, we find using exponential decay gives slightly better results on movielen1m data when we use max length of 50. We find using combination of our idea and your idea (most of weight on the last start index but rest of times we sample start index based on recency with exponential decay) empirically performs better, giving NDCG@10 of 0.59945 versus 0.59509 and Recall@10 of 0.81109 versus 0.80414. I think our works point to a future direction that worth more explorations, both empirically and theoretically.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJeSNEfXoH",
                "reply_to": "SJenAyg6tH",
                "title": "Clarifying Doubts on Experimental Section",
                "comment": "Hi Reviewer, thank you very much for raising your confusion to us on experiments. We will do a better job in clarifying on how we compared with HGN in experimental section. \n\nTo explain why HGN is not doing as well as SASRec in our reported results: First, it is worth noting that the HGN paper not only used completely different datasets, but also used very distinct evaluation procedures from SASRec. Instead of predicting next engaged item, it has 10% interactions in test set, such that one prediction is correct as long as it falls into the test set, while we (and SASRec) are doing another task of predicting precisely next item, in which there is only 1 correct answer. So the task they consider is an easier task than us. Moreover, If you read the HGN paper carefully, they are mainly focused on accommodating very short sequences. In paper's experiments, they use hyper-parameter $L = 5$, where $L$ is the length of sequence used for training and inference. On contrast, our method SSE-PT and SASRec uses $L = 200$ for Movielens1m and $L=50$ for other datasets. We think that mainly accounts for the difference in original paper's reported performances and our reported performances. It is very possible that for very short sequences, HGN works quite well, better than SASRec as they have shown in their paper. We will add this delicate detail to the final version of the paper to avoid any confusions for future readers. Moreover, we modified original HGN codes to make HGN's evaluation the same as that of SASRec and open sourced at: https://github.com/SSE-PT/SSE-PT/tree/master/HGN_baseline. You have a look at our codes for both our SSE-PT and HGN baseline.\n\nAs to your other comments. \n1. Yes, it is correct that first few rows (A to F and H, I) of results in Table 1 are from SASRec paper, the reason is that we use the exactly same experimental settings on exactly same datasets. So we decided to trust the results reported in SASRec paper for older methods. We include those earlier baselines for completeness but those are not as important as SASRec because SASRec has been shown to outperform those methods. We did re-run SASRec and got slightly better results in Table 1 than the ones originally reported in SASRec paper.\n\n2. Yes, Table 3 we use different metrics than Table 1, because we realize the NDCG@10, Recall@10 does not accurately reflect how bad over-fitting is as NDCG@5 and Recall@5. The percentages of improvement for using a well-suited regularization are much more dramatic once you switch the metrics to top 5 from top 10. This means good regularization are extremely important for top k ranking results, especially when $k$ is small. The results in Table 3 would still hold for top 10 but less dramatic for percentages of gains. On the other hand, because we want to make a fair comparison with SASRec on the same datasets, we chose to use same top-10 metrics in Table 1. This is our reasoning as to why metrics used in Table 1 and 3 are different.  \n\n3. As to the ablation study, the ablation study of personalization is done in Table 10 in Appendix and we have had a dedicated section 4.3 for different ablation studies done for each component of the model.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and \nis proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1xkMwxJYB",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #2",
                "comment": "In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user $i$, i.e., $u_i$, shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques.\n\nExtensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods.\n\nOverall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. \n\nMy major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance.\n\nSome suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec.\n\nSome minors:\nTypo: in the paragraph below Eq.(3): user $l$ -> user $i$\nTypo: FPMF, PFMC in different places\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hyg-LKcJcS",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #1",
                "comment": "The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE  regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches.\n\nDetailed comments:\n\n1)\tThe technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental.\n\n2)    In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them.\n\n3)\tIn SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability?\n\n4\uff09 Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. \n\n\nTo summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a \u201cweak accept\u201d to this paper given the reasons listed above.\n\n\n[1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "XL9DWRG7mJn": {
        "paper_id": "nips_2021_XL9DWRG7mJn",
        "paper_title": "Rethinking gradient sparsification as total error minimization",
        "paper_abstract": "Atal Sahu, Aritra Dutta, Ahmed M. Abdelmoniem, Trambak Banerjee, Marco Canini, Panos Kalnis",
        "paper_acceptance": "accept",
        "meta_review": "This paper reformulates an existing problem (how to sparsify gradients in distributed training) and proposes to minimize a new objective (the total compression error subject to communication constraint as opposed to per-iteration compression error). This change of viewpoint leads to a new algorithm (hard threshold algorithm with variable sparsity). The authors show the effectiveness of the proposed algorithm through theoretical bounds and experiments. All reviewers agree that this is a valuable contribution. Comments from previous submission to ICML are adequately addressed.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "O7Pa-FIoBJ8",
                "writer": "author",
                "reply_to": "sb4i9bO5c6F",
                "title": "Thank you for your positive reassessment",
                "comment": " We thank the reviewer for the positive feedback, and for pointing out a meaningful direction for future research. We will clearly elaborate our use of the word optimal, as mentioned in the response to reviewer o9PJ.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sb4i9bO5c6F",
                "writer": "official_reviewer",
                "reply_to": "8BKgSsPRwr8",
                "title": "Comments after rebuttal",
                "comment": " Many thanks to the authors' detailed replies and other reviewers' insights into this paper. I do appreciate the great value of this work, but I am still worried about the claim of optimality, even though I understand that this optimality is restricted to the model the authors proposed. My understanding is that from the upper bounds in Theorem 1, no matter what optimization trajectories, the upper bound only depends on the total compression errors, and the hard-threshod is optimal for any fixed optimization trajectories as long as the right threshold is chosen, and thus the optimality (please let me know if I misunderstood this). However, Theorem 1 only provides an upper bound, if we are minimizing an upper bound, claiming optimal seems to me kind of strong. I understand this upper bound motivates the hard-threshold compression methods, and the paper presents very solid theoretical and experimental results for this method. However, in terms of communication efficiency, i.e. bits over guaranteed optimization errors, there is no clear theoretical improvements shown, so maybe this can be some future works. I agree with other reviewers' votes and raise my score to 7.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Inez-R5X9DB",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "Official Review of Paper10919 by Reviewer TZ2h",
                "comment": "This paper analyzes the hard-threshold sparsifier in distributed SGD with convergence analysis and extensive experiments. Main contributions include: 1. Provides upper bounds of the optimization errors for the hard-threshold sparsified distributed SGD, which improves from top-k compressor with linear speedup and compressor operator parameter dependence. 2. Conducts extensive experiments to demonstrate the benefits of hard-threshold sparsifier, achieving much better performance given the same average compression density (# of used coordinates / # of coordinates of DNN weights).   1. I am trying to understand why the proposed communication-complexity model is new, it seems to me it is an adaptive compression operator allowing different # of coordinates to be sent for each iteration. And the section 4.4 and Lemma 3 seems incorrect to me, since the optimization over B is sequential, the choice of first block will change the second block of A, so I didn\u2019t quite follow why the proposed sparisfier is optimal for this communication-complexity model. \n2. On the confusion of total error minimization. In the paragraph starting from line 44, it seems to the authors are trying to present a new perspective that is not minimizing per iteration compression error but total compression errors, but the last sentence seems to me conveying the message of considering fixed total communication budget, which is exactly what I think other papers have discussed, how to tradeoff optimization error with total communication budgets. So there may be some confusion in this paragraph.\n3. The authors provide upper bounds of optimization errors for the proposed spasifier, in strongly convex, convex, and nonconvex settings respectively. Those upper bounds improve from the results using top-k sparsifier in terms of linear speedup and compressor parameter dependence. \n4. The provided upper bounds, however, seem to me not clearly they are more communication-efficient than the top-k sparsifier, since there is no characterization of the total number of coordinates being used in theory. Ideally, if we set the threshold be very small, most coordinates will be used. In experiments, the authors provide solid results showing that the hard-threshold sparsifier does use less coordinates in total than the top-k. \n5. The total error minimization perspective seems to me more like an observation of the consequence using a hard-threshold sparsifier, it may need further arguments to show it is the reason for better performance. \n6. The paper is in general well written and very clear, the extensive experiments are helpful for the understanding of the practical benefits of the hard-threshold sparsifier.  Yes.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "hO-JUkgT_HK",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "Official Review of Paper10919 by Reviewer VFA3",
                "comment": "The paper studies the role of compression operators on the convergence of Distributed SGD with Error Feedback. In particular, via simple observations, the authors conclude that a hard-threshold sparsifier with a carefully tuned threshold parameter minimizes the total error appearing in the analysis because of the presence of compression. Moreover, they show empirically the connection between poor behavior of EF-SGD with Top-k compression and the severe error accumulation.\n\nMotivated by these observations, the authors derive new convergence guarantees for EF-SGD with absolute compressors. This class of compressors covers hard-threshold sparsifier. The derived bounds show that the compression does not affect the slowest terms in the bound. Moreover, the authors derived the first complexity result in the non-convex case for EF-SGD with $\\delta$-cnotraction operators without bounded gradient assumption and $n > 1$ (though, under bounded data dissimilarity). Finally, the paper contains a good empirical study of the performance of EF-SGD with the hard-threshold sparsifier. The authors also provide an insight on how to tune the threshold parameter in order to outperform EF-SGD with the Top-k operator.  ## Strengths\n\n1. **Simple but important observations about total error minimization.** The paper provides a closer look at the convergence of EF-SGD and identifies what quantity should be minimized in order to get better results. That is, via the sequence of simple observations, authors show that a hard-thresholding sparsifier (with fine-tuned threshold parameter) is the optimal choice in terms of the total error minimization. Moreover, the authors properly explain all the details and support their theoretical observations with empirical findings (e.g., see Figures 1 and 3).\n\n2. **Clarity and proofs.** The paper is clearly written. The proofs are easy to follow and contain only a couple of typos.\n\n3. **New results for EF-SGD.** The authors derived new results for the convergence of EF-SGD with absolute compression for strongly convex, convex, and non-convex objectives. The slowest terms in the derived bounds have a linear speedup and are not affected by compression-dependent parameters. This is a good property since the derived bounds match the ones for SGD without compression if the target accuracy is small enough / the number of communication rounds is large enough. Moreover, the authors derived the first convergence result for EF-SGD for $\\delta$-contraction operators without assuming boundedness of the gradients, but under Assumption 4, that bounds dissimilarity between local loss functions. Although the proofs substantially rely on the known techniques, the obtained results are quite good.\n\n4. **Numerical experiments** show a connection between the behavior of EF-SGD with Top-k and hard-thresholding sparsifier and \"error buildup\". Therefore, these numerical results justify the insights provided in Section 4.\n\n## Weaknesses\n\n1. **No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** The derived bounds in the non-convex case substantially rely on Assumption 4 that bounds the dissimilarity between local loss functions. Although this is a significant limitation, previous works on EF-SGD rely on even stronger assumptions. Therefore, this is a minor drawback.\n\n## Questions and Comments\n\n1. **Rates for EF-SGD with $\\delta$-contraction operator.** The rates shown in Remarks 5 and 7 can be significantly improved via the results from [19]. Although, in [19], Assumption 3 is not considered it can be easily cast in the general framework from [19] via the following derivation: $\\frac{M}{n}\\sum_{i=1}^n\\|\\nabla f_i(x^k)\\|^2 + \\sigma^2 \\leq 2LM(f(x^k) - f(x^*)) + \\frac{M}{n}\\sum_{i=1}^n \\|\\nabla f_i(x^*)\\|^2 + \\sigma^2$. Using the results from [19] one can actually show the linear speedup even for $\\delta$-contraction operators in the $\\mathcal{O}(1/T)$ and $\\mathcal{O}(1/\\sqrt{T})$ decaying terms for $\\mu > 0$ and $\\mu = 0$ respectively. Therefore, the conclusion from lines 270-272 is not correct.\n\n2. **equation after line 654:** the enumerator in the second term should be $\\mu L^2(1+M/n)^2R_0 + L\\kappa^2 \\ln(T)$.\n\n3. **line 247, $\\nu = \\gamma_t \\kappa$:** It is better not to use $\\kappa$ in the definition of $\\nu$ because $\\kappa$ usually denotes the condition number of the problem in the optimization literature.\n\n4. **lines 283-284:** This is done for $\\delta$-contraction operators in [19] and in Qian, X., Dong, H., Richt\u00e1rik, P., & Zhang, T. Error Compensated Loopless SVRG for Distributed Optimization, Qian, X., Richt\u00e1rik, P., & Zhang, T. (2020). Error compensated distributed SGD can be accelerated. arXiv preprint arXiv:2010.00091.\n\n5. **Lemmas 6 and 7.** First of all, one should add that $\\gamma = \\min\\left(\\frac{1}{d}, \\sqrt{\\frac{r_0}{cT}}\\right)$. Next, Lemma 7 can be tightened when $c$ is small, see Lemma D.3 from [19].\n\n6. **Lemma 9.** When $c = 0$ this result is incorrect since the logarithmic factor becomes infinity. See Lemma D.2 from [19] for the correct version.\n\n7. **Lemma 11.** It is better to cite the assumptions of the lemma in the statement (or at the beginning of the subsection).\n\n8. **lines 623-627:** The discussion in these lines should be rewritten after applying the corrections suggested in comment 1. Moreover, one should also say that $\\lambda$ can be large.\n\n## Comment after rebuttal\nI thank the authors for their response. I have read other reviews as well. Overall, my evaluation of the work remains the same. Therefore, I recommend the paper for acceptance and hope that the authors will apply all necessary corrections mentioned in the reviews. The authors adequately addressed the limitations and potential negative societal impact of their work.",
                "rating": 7,
                "confidence": 5
            },
            {
                "review_id": "izpo_kBPLuT",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "Official Review of Paper10919 by Reviewer o9PJ",
                "comment": "The paper considers gradient sparsification for learning in distributed setup, and advocates using a hard-threshold sparsifier combined with error-feedback mechanism. The paper shows that such algorithm is _optimal_ in a certain sense, and give several convergence guarantees for the error-feedback algorithms using absolute compressors and relative compressors. The empirical performance of HT and top-k compressors are also compared.  Post-rebuttal: Thank you for the reply. My concerns have been addressed well. Raising the score to 7.\n\n---\n\n__TL;DR.__ I think this paper presents several meaningful theoretical results as a contribution. However, I think some of the paper's claims are being quite oversold.\n\n__Strengths.__ Some of the theoretical results are definitely very cool to have. I believe that the convergence results in Section 5 (Theorems 2--5) is a nice contribution, and would be of interest to the distributed learning society, especially to those who study error-feedback mechanisms. Also, the proof technique going through the perturbed iteration analysis (via Lemma 10) is quite neat. Finally, the manuscript seems to discuss the related work relatively well.\n\n__Claims on optimality.__ I am very worried about the paper's claims about the optimality. The word \"optimal\" is a very bold, and should be used with a great care (in my opinion), as they could be quite misleading without delivering the assumptions and conditions it relies on. For instance, the abstract states that top-k is \"communication-optimal given a per-iteration $k$-element budget.\" But, what exactly does the word \"communication-optimal\" here mean? It is very easy to understand the statement as saying that such algorithm gives the hypothesis with a smallest loss---either in expected or high-probability sense. If I understood correctly, I think the paper is pointing to lemma 2, where authors state that Top-k gives the sparsified version of the error-feedback (or actually any signal) that has the smallest squared distortion from the original gradient signal. This also relies on the assumption that the choice of sparsification does not affect the subsequent gradient signals. This discrepancy gets more significant for the claims on the optimality of hard-threshold methods, where this \"independency assumption\" is critical for the proof. I believe that these ill-specified claims on the (possibly vacuous notions of) optimality should either be toned down to a certain degree to help readers better understand what the paper is contributing. Also, I think presenting the optimality claims in a form of lemmata without proofs is unnatural, no matter how straightforward the proofs are. I recommend either stating the claims in plain words without a formalization, or provide the result-specific assumptions clearly in the lemma statement---so that it is self-contained---and give at least a formal proof.\n\n__Stating the limitations.___ If I understood correctly, such hard-threshold sparsifiers may need a very high communication throughput at some epochs (mostly earlier). However, there are many setups such high peak communication rate is undesirable, due to the limited capacity of the communication channel (but when the delay is crucial). In such cases, having a constant communication rate could be beneficial. I think authors should discuss such scenarios to appropriately deliver the cases where the considered hard-threshold methods are desirable, and the cases they are not (sorry if I missed these parts).\n\n__Clarity: Why error-feedback?__ I am not entirely sure what is the big motivation behind considering the error-feedback mechanisms for this paper. Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ...\" but I couldn't really locate the part that necessitates considering error-feedback. Could you please further clarify?\n\n__Clarity: $\\gamma_t$.__ The quantity $\\gamma_t$ appears at line 133, but I do not think this quantity is defined or introduced properly in the text.\n\n__Clarity: Assumption 2.__ I do not think Assumption 2 is explicitly assumed in any theorem appearing in the main text. But if I am correct, it is implicitly used for every theorems that use $R_0 := \\lVert x_T - x^\\star \\rVert$ in its bounds.\n\n__Question: $v$-dependency__ It was unexpected to me that the convergence guarantees for the absolute compressors $C_v ( \\cdot ) $ does not depend explicitly on the compression factor $v$, especially considering the fact that the guarantees for the $\\delta$-contraction operators contain a term that is inversely proportional to $\\delta$ (see Theorem 2 & Remark 5, for instance). Any further discussion explaining why such discrepancy happens (especially for the case $\\delta \\to 0, v \\to \\infty$) would be very nice to have; does this suggest the existence of a tighter bound for $\\delta$-contraction operators under additional assumptions the size of $p_{i,t}$?\n\n__Suggestion: Discussing $P_T$.__ While the quantity $P_T$ appears in many optimization literature, the meaning of the quantity may not be very straightforward for the readers who are relatively new to the field (like myself), especially because it gives the performance bound for $\\bar{x}_T$ generated by some weights $w_t$. Giving more ideas about the quantity may help the readers a lot, including whether one can choose $w_t$ arbitrarily or not, whether it implies that we should use an averaging scheme... I think authors should additionally mention when can such variable-communication-load methods may not useful or usable.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "8BKgSsPRwr8",
                "writer": "author",
                "reply_to": "Inez-R5X9DB",
                "title": "Author Response to Reviewer TZ2h",
                "comment": " We sincerely thank the reviewer for the positive assessment of our paper and for constructive feedback. Below we address the points mentioned by the reviewer:\n\n\n**1.Communication-complexity model and total error minimization.** We thank the reviewer for pointing this out. We accept that there is a slight misunderstanding about the simplification in our communication-complexity model. As mentioned in line 192, we assume a simplified model---Instead of the error-corrected update, $\\gamma g_t +e_t$, we consider a sequence of *fixed* vectors $(a_t)_{t \\in T}$  and formalized the optimization problems (5) and (6) in Section 4.4. Hence as the reviewer asserted, the optimization in Section 4.4 is not sequential. We will clarify this. \n\nWe note that the existing communication-optimal strategies [18, 38, 13, 4] minimize the compression factor (see Footnote 2) under a budget for each vector; please see lines 221-227. In contrast, the novelty of our communication-complexity model is that it better captures the total error. We owe this insight to Theorem 1 that accounts for the effect of sparsification in the entire training process. Please refer to the statement of Theorem 1, which presents the convergence of distributed error-feedback SGD with compressed communication (here, sparsification). The third term on the right hand side of the inequality captures the effect of compression between the error-corrected update, $\\gamma g_t +e_t$, and its compressed form, $C(\\gamma g_t +e_t)$ over all iterations, $t=0,1,\\cdots, T-1$. This is a well-accepted theoretical result and appears abundantly in the literature; please see [29,46]. Motivated by this result, we consider the total error perspective as a communication complexity model----where simplified total error is minimized under a total communication budget. Hard-threshold sparsifier comes out as the communication-optimal compressor under this communication complexity model. Therefore, the total-error minimization is not the consequence of using the hard-threshold sparsifier; it is the reason behind the hard-threshold as a communication-optimal sparsifier. Moreover, we substantiate this with insights from our experiments in Figures 1, 3, 5, and 6. \n\nNext, we respectfully note that we use the overall communication budget to denote the total communication throughout the training.\n\n\n**2. Convergence of hard-threshold.** We thank the reviewer for this question. The reviewer is correct---Our convergence results do not show that the hard-threshold is more communication-efficient than Top-$k$. Because we do not characterize the average data transmission for a threshold. However, we have demonstrated that hard-threshold is communication-optimal in our communication-complexity model. Our communication-complexity model is motivated by the EF-SGD non-convex convergence result, and it provides us insight into why hard-threshold has better convergence than Top-$k$ in practice.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wpfn0XbVep",
                "writer": "author",
                "reply_to": "izpo_kBPLuT",
                "title": "Author Response to Reviewer o9PJ",
                "comment": " We are grateful to the reviewer for the constructive feedback and for providing a positive assessment of our paper. The reviewer has raised some valid questions and provided many mindful suggestions. The following are our responses to the reviewer\u2019s comments: \n\n**Claims on optimality:** We thank the reviewer for this comment. Our use of the word *optimal* is motivated by recent literature in compressed distributed optimization [ 39, 18, 13, 4] that focus on the compression-error (see Footnote 2). Any compressed optimization convergence analysis captures the effect of compression via the compression-error, and this effect is always inverse---the lower the compression error, the better the optimization upper bound is. Please refer to one-step descent Lemmas 12 and 15. To derive a convergence rate from these lemmas, we need to use the worst-case compression error/factor . For instance, to derive a convergence rate for $\\delta$-contraction operators in Remark 5 and Remark 7, one uses the worst-case compression factor in equation (7) and lines 211-212. Due to this, [39, 18, 13, 4] directly optimize for this worst-case compression factor and call their compressors as *optimal*. \n\nHowever, for gradient sparsification with a fixed $k$ element communication per iteration, this worst-case bound is not insightful. For example, to sparsify a $d$-dimensional vector, both Random-$k$, and Top-$k$ have the same compression factor, $k/d$, although Top-$k$ performs significantly better in practice than Random-$k$; please see further discussion in [8]. But we know that for a given signal, Top-$k$ attains the lowest compression-error among all sparsifiers with $k$ element communication budget. And therefore, we state Top-$k$ as the communication-optimal sparsifier under a fixed $k$-element communication budget. Precisely, this is the message behind Lemma 2 as the reviewer has correctly identified. We will elaborate on this in detail in the final version.\n\nNext, we will mitigate the confusion regarding Lemma 3. We agree that we should provide proof for Lemma 3, and will do so in the Appendix of the revised paper. Please note that we have been careful to stress throughout the text that hard-threshold is communication optimal *in our communication complexity model*, and have stated that the total-error cannot be directly minimized (Lines 175-177). We coined the term *total-error* because this term captures the compression error in the entire training process. We formalize our communication complexity model in (5) after simplifying the total error; please see Lines 191-193. \n\n**Stating the limitations.** We thank the reviewer for pointing this out and we agree with the comment. Indeed there are scenarios where a predetermined compression ratio for an iteration is desirable. Examples include dynamic network environments such as a public cloud or a shared cluster with colocated jobs [Abdelmoniem et al., 2021]. In such a setting, one may want to adjust the compression knobs according to the current network bandwidth so that training finishes within a time budget, and therefore hard-threshold is not a good candidate for this setting. \n\nIn a standard distributed cluster setting with a dedicated network, if communication is a bottleneck for Top-$k$, i.e., there is not a complete overlap between communication and computation, so a hard-threshold with the same total communication volume will have non-overlapped communication in the iterations with high data transmission, but may also have completely overlapped communication in iterations with low data transmission. Thus, hard-threshold can have less non-overlapped communication time than Top-$k$ in this case. This happens in large-scale CRT models such as DeepLight which has 90% of non-overlapping communication. Considering the opposite, if there is complete overlap between computation and communication for Top-$k$, then a hard-threshold with the same total communication volume may have non-overlapped communication time in some iterations with high data transmission. Here, we ignored two important aspects of hard-threshold: (i) Hard-threshold has better statistical efficiency than Top-$k$, thus one may require smaller iterations to a target accuracy. (ii) Hard-threshold has negligible compression overhead in comparison to Top-$k$ (lines 73-79). \n\n[Abdelmoniem et al., 2021] DC2: Delay-aware compression control for distributed machine learning. IEEE INFOCOMM 2021.\n\n**Why error-feedback?** This is a critical question and we thank the reviewer for asking this. Please allow us to discuss the development of the error-feedback theory in this context. Error-feedback was first empirically introduced by Seide et al. [40] in 2014, to alleviate the convergence of 1-bit low-precision SGD in training language models. But, for the next 5 years, the community was unaware about why error-feedback is an important technique. In 2018, Stich et al. [45] were the first to theoretically establish the convergence of SGD using $\\delta$-contraction operators with error-feedback, which was extended to the distributed setting by Zheng et al. in [55]. More interestingly, in subsequent work, Karimireddy et al. [29] theoretically showed that error-feedback can remedy the convergence issues of aggressive quantizers, such as 1-bit/Sign SGD, as well as biased $\\delta$-sparsifiers, such as Top-$k$, Random-$k$, etc. We also refer to [46] for a detailed discussion on the error-feedback framework. In a nutshell, without error-feedback, most sparsifiers (which also belong to the class of $\\delta$-contraction operators) diverge [29,46]. Moreover, the best compression ratios for gradient sparsification are achieved when we use error-feedback. Please refer to Table 1 in the comprehensive survey [54], where all implemented sparsifiers use error-feedback. This makes error-feedback an \"indispensable and essential\" technique for gradient sparsification. And this was the \"big motivation\" of using error-feedback in our work as it is focussed on gradient sparsification. However, we also thank the reviewer for pointing out \"Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ....\".\" We will rewrite this line to justify why we use the error-feedback. \n\n**Clarity:$\\gamma_t$**. Thank you for this important observation. We will clearly mention that $\\gamma_t>0$ is the stepsize sequence. \n*Assumption 2.* We thank the reviewer for catching this typo and rectifying us. We will mention Assumption 2 in the theorems where it is supposed to appear. \n\n**$\\upsilon$-dependency.** We apologize for the confusion. Absolute compressors\u2019 convergence is in terms of $\\kappa$; please see lines 244-250. We will make this clearer by directly compressing the error-compensated gradients in Algorithm 1 instead of the error-compensated updates as done presently. \n\n**Discussing $P_T$.** The quantity $P_T$ in our paper denotes the expected suboptimality gap, $E[f(\\bar{x}_T)]-f^\\star$ at averaged iterate, $\\bar{x}_T$, where $f^*$ is the global minimum, defined in Assumption 2. We understand and appreciate the reviewer's concern in clarifying the meaning behind this quantity. Here $w_t$ is a carefully chosen set of weights such that we achieve the convergence result. We will highlight its meaning with references at the beginning of Section 5.1 where it appears for the first time. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oiEJMsDqEUb",
                "writer": "author",
                "reply_to": "-lB5hoBJkV",
                "title": "Author Response to Reviewer eex4",
                "comment": " We thank the reviewer for the effort in reviewing our paper. We also thank the reviewer for providing a positive assessment of our work and for appreciating our communication-complexity model which is indeed a key contribution of the paper. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aChnufbJY_s",
                "writer": "author",
                "reply_to": "hO-JUkgT_HK",
                "title": "Author Response to Reviewer VFA3",
                "comment": " We thank the reviewer for the positive review of our paper. We are glad that the reviewer considers total error minimization as an important contribution to our paper. Indeed, this is the heart of our paper. Below we discuss the questions and the comments of the reviewer.\n\n**No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** We sincerely thank the reviewer for pointing out this interesting aspect. We will address the arbitrary heterogeneous case in our future work.\n\n## Questions/Comments\n\n**1. Rates for EF-SGD with $\\delta$-contraction operator.** We sincerely thank the reviewer for bringing the tighter rates in [19] to our notice. We will rewrite this discussion accordingly.\n\n**2. Typo after line 654.** We thank the reviewer for indicating this typo. We will correct this. \n\n**3. Notation $\\kappa$.** We thank the reviewer for this comment; and yes, you are right. We will use a better notation in the final version of the paper.\n\n**4. Related works.** We thank the reviewer for pointing out these works. We will indeed mention them and include more discussions in a proper context in the final version of the paper. \n\n**5. Lemmas 6 and 7.** We thank the reviewer for giving us this insight. We will use the tighter result from [19].\n\n**6. Lemma 9.** We thank the reviewer for spotting this. We will correct it.\n\n**7. Assumption in Lemma 11.** We thank the reviewer for this insightful comment. Indeed we will state the assumptions in the main statement of Lemma 11. \n\n**8. Lines 623-627.** As mentioned in comment 5, we will now compare absolute compressors and $\\delta$-contraction operators using [19]. We will also state that $\\lambda$ can be arbitrarily large.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-lB5hoBJkV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "Official Review of Paper10919 by Reviewer eex4",
                "comment": "In this paper, the authors demonstrate that, in the context of distributed optimization problems with $n$ workers, the hard-threshold sparsifier is the optimal sparsifier for a proposed communication complexity model which where the goal is to minimize the total error for a sequence of responses. This allows the authors to compare the sum of compression errors for various algorithms and to demonstrate that while for the per-iteration $k$-element budget the Top-$k$ sparsifier is optimal, when it comes to total error the hard threshold sparsifier is better. \n\nThe authors also compare the convergence rates of the top-$k$ sparsifier vs the hard threshold sparsifer for image classification, language modeling, and recommendation tasks.   I find this paper interesting and vote to accept it. I think it is an interesting result but unfortunately I am not an expert in the field and so am not sure about it's significance in the context of the field. I do like the communication complexity model.  This is a theoretical paper and has limited societal impact. ",
                "rating": 7,
                "confidence": 3
            }
        ],
        "label": "train"
    },
    "r1exVhActQ": {
        "paper_id": "iclr_2019_r1exVhActQ",
        "paper_title": "DEEP-TRIM: REVISITING L1 REGULARIZATION FOR CONNECTION PRUNING OF DEEP NETWORK",
        "paper_abstract": "State-of-the-art deep neural networks (DNNs) typically have tens of millions of parameters, which might not fit into the upper levels of the memory hierarchy, thus increasing the inference time and energy consumption significantly, and prohibiting their use on edge devices such as mobile phones. The compression of DNN models has therefore become an active area of research recently, with \\emph{connection pruning} emerging as one of the most successful strategies. A very natural approach is to prune connections of DNNs via \u21131 regularization, but recent empirical investigations have suggested that this does not work as well in the context of DNN compression. In this work, we revisit this simple strategy and analyze it rigorously, to show that: (a) any \\emph{stationary point} of an \u21131-regularized layerwise-pruning objective has its number of non-zero elements bounded by the number of penalized prediction logits, regardless of the strength of the regularization; (b) successful pruning highly relies on an accurate optimization solver, and there is a trade-off between compression speed and distortion of prediction accuracy, controlled by the strength of regularization. Our theoretical results thus suggest that \u21131 pruning could be successful provided we use an accurate optimization solver. We corroborate this in our experiments, where we show that simple \u21131 regularization with an Adamax-L1(cumulative) solver gives pruning ratio competitive to the state-of-the-art.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non-zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed.",
        "meta_review_title": "A study on sparse properties of L1-regularization in deep neural networks, yet experimental supports seem week.",
        "reviews": [
            {
                "review_id": "BJlZt9ptRX",
                "reply_to": "BylbaEivn7",
                "title": "Reply to AnonReviewer2",
                "comment": "We thank the reviewer for the feedback and comments.\n\n(1) \"whether the theory for (5)  is rigorously justified by the experiments\":\n\nWhile our theorem is designed for the layerwise objective (5), in practice for simplicity we find that directly optimize (8) yields promising results is more simple. We will show experimental results for both (5) and (8) in future revisions. Note that by optimizing (8), we achieve satisfactory results satisfying our bounds from analyzing (5) in all experiments in this work.\n\n(2) Regarding the bound tightness:\n\nWe perform experiments on Cifar 10 with Vgglike-networks with different \\lambda values by compressing the last 2 FC layer. \nWe would like to point out that the bound for NNZ per-layer in this setting is 50000 * K_s, which depends on the number of supports in the stationary point.\n\nIf a max-margin loss is used, K_s can be close to 1, which would give us an NNZ bound around 50000, which is not far from the empirical compressed NNZ (~ 10000).\n\nepsilon     |   1e-4     |    1e-5   |   1e-6    |   1e-7    |  1e-8   |   1e-9  | 1e-10  |      0     |\nnnz_fc1    |   9052    |    9947   |   10046 |   10053 | 10054  | 10054 | 10054 | 262144|\nnnz_fc2    |   4549    |    4567   |   4570   |   4570   |  4570   |  4570  |  4570  |   5120  |\ntrain_acc  |  0.9970  |  0.9974  |  0.9979 |  0.9970 | 0.9969 | 0.9972| 0.9969| 0.9970 |\n\n(3) regarding minor points:\n\nWe will fix the mistakes and typos in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkxH-cpFCQ",
                "reply_to": "Byevdiz3nm",
                "title": "Reply to AnonReviewer1",
                "comment": "We thank the reviewer for the nice feedback and concerns.\n\n(1) the assumption of \u201cgeneral position\u201d:\n\nThe columns of V do not need to be independent to be in general position. It is sufficient if V is drawn from any continuous probability distribution. In other words, the assumption holds as long as we add a very small continuously-distributed perturbation to V. Note general position is a much weaker condition than the RIP condition used widely in sparse recovery.\n\n\n(2) Theorem 1 claims the sparse inequality holds for any \\lambda:\n\nTo validate that the sparse inequality holds for any \\lambda, we perform experiments on Cifar 10 with Vgglike-networks with different \\lambda values by compressing the last 2 FC layer. \nThe result is shown below:\n\nepsilon     |   1e-4     |    1e-5   |   1e-6    |   1e-7    |  1e-8   |   1e-9  | 1e-10  |      0     |\nnnz_fc1    |   9052    |    9947   |   10046 |   10053 | 10054  | 10054 | 10054 | 262144|\nnnz_fc2    |   4549    |    4567   |   4570   |   4570   |  4570   |  4570  |  4570  |   5120  |\ntrain_acc  |  0.9970  |  0.9974  |  0.9979 |  0.9970 | 0.9969 | 0.9972| 0.9969| 0.9970 |\ntest_acc   |  0.9271  |  0.9270  |  0.9266 |  0.9267 | 0.9264 | 0.9262| 0.9265| 0.9268 |\n\nWe note that we perform SGD with L1 regularizer to train the network as a pretraining step. Empirically, we find that after the L1 norm is penalized, even a very small epsilon can lead to very sparse solutions. (However, when epsilon is too small, the converging time may grow a lot.) For epsilon >= 1e-9, the nnz_fc1 becomes <= 10054 for the first training epoch. However, for epsilon = 1e-10, nnz_fc1 drops to 10054 after the second epoch.\n\n(2) regarding minor points:\n\nWe will fix the mistakes and typos in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1lBnKatR7",
                "reply_to": "SkeePQCJTQ",
                "title": "Reply to AnonReviewer4",
                "comment": "We thank the reviewer for the feedback.\n\n1) About \"Ignoring the latest improvement in (C. Louizos et al., 2017) and (J. Achterhold et al.)\":\n\nWhile we thank the reviewer for providing us more related works,  it worths noticing that pruning ratios in (C. Louizos et al., 2017), (J. Achterhold et al.) are not as strong as our compared baseline \"Variational Dropout\". For example, for LeNet on Mnist, the former have ~0.65%, while the latter (and our result) are less than 0.4%, and for VGG on CIFAR-10, the former have ~5.5%, while the latter (and our result) are less than 2%. That is, both our method and VD has better results compared to the two related works.\n\nNote many results provided in (C. Louizos et al., 2017), (J. Achterhold et al.) are for simultaneous pruning and quantization, while our submission focuses more on investigating the pruning effect of the simple L1 regularizer. In this work, we focus on the weight pruning ratio without quantization.\n\n(2) About  comment \"Repeating the old story from other papers\":\n\nOur story focuses more on the analysis of \"problem\" instead of the \"algorithm\". In other words, we argue that different problems have different compression rate, depending on their number of supporting labels, when a simple L1-regularized pruning objective is used.  The algorithm we proposed is just a tool for helping our iterates getting closer to the stationary points.\n\n(3) About comment \"quite limited novelty\":\n\nFirstly, our novelty lies more on the analysis of the pruning objective than on the algorithm. Second, it is a wrong impression that we are proposing ADAM over SGD.  Our proposition for the algorithm is the \"L1 cumulative\" technique as a general extension module to modify any stochastic-gradient-based algorithms, such as SGD and ADAM, into a sparsity-inducing solver.\n\n(4) About comment \"lacking solid experiments\":\n\nThe sentence is an editorial mistake. We will strengthen our experiments in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkeePQCJTQ",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "Repeating the old story from other papers, quit limited novelty, lacking solid experiments",
                "comment": "The main concerns come from the following parts:\n\n\n(1) Repeating the old story from other papers:\nA large part of math is from previous works, which seems not enough for the ICLR conference.\nIt is very surprising that the authors totally ignore the latest improvements in neural network compression. Their approach is extremely far away from the state of the art in terms of both methodological excellence and experimental results. The authors should read through at least some of the papers I list below, differentiate their approach from these pioneer works, and properly justify their position within the literature. They also need to show a clear improvement on all these existing pieces of work. \n\n(2) quite limited novelty:\nIn my opinion, the core contribution is replacing SGD with Adam.\nFor network compression, it is common to add L1 Penalty to loss function. The main difference of this paper is change SGD to Adam, which seems not enough. \n\n(3) lacking solid experiments:\nIn section Experiment, the authors claim \"Finally, we show the trade-off for pruning Resnet-50 on the ILSVRC dataset.\", but I cannot find the results. \n\nIs the ResNet-32 too complex for cifar-10? Of course, it can be easily pruned if the model is too much capacity for a simple dataset.  Why not try the Resnet-20 first?\n\n[1] C. Louizos et al., Bayesian Compression for Deep Learning, NIPS, 2017\n[2] J. Achterhold et al., Variational Network Quantization, ICLR, 2018",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Byevdiz3nm",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "an interesting perspective on the L1 regularization of neural network",
                "comment": "This paper discusses the effect of L1 penalization for deep neural network. In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. \n\nThe perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. If the coefficients of the linear equation are distributed in general positions, then the number of variables should not be larger than the number of equations. \n\nWhile I mostly like the paper, I would like to point out some possible issues:\n\nmain concerns: \n\n1. the columns of V may not be independent during the optimization(training) process. In this situation, I am not quite sure if the assumption of \u201cgeneral position\u201d still holds. I understand that in literatures of Lasso and sparse coding it is common to assume \u201cgeneral position\u201d. But in those problems the coefficient matrix is not Jacobian from a learning procedure. \n\n2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. It is against the empirical observation that when lambda is extremely small, effect of the regularizer tends to be almost zero. Can authors also show this effects empirically, i.e., when the regularization coefficients decrease, the nnz does not vary much? (Maybe there is some optimization details or approximations I missed?)\n\nSome minor notation issues:\n1. in theorem 1: dim(W^{(j)})=d should be dim(vec(W^{(j)}))=d\n2. in theorem 1: Even though I understand what you are trying to say, I would suggest we describe the jacobian matrix V in details. Especially it is confusing to stack vec(X^J) (vec(W^j)) in the description.\n3. the notations of subgradient and gradient are used without claim\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BylbaEivn7",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "Nice Theoretical Insights, but Not Sure How Experiments Substantiate the Theory",
                "comment": "The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance. Specific comments follow.\n\n1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). Experimental results of optimizing (5) seem missing. While the reviewer understands that (5) and (8)  are closely related, and the theoretical insights for (5) can potentially translate to the scenario in (8), the reviewer is not sure whether the theory for (5)  is rigorously justified by the experiments.\n\n2. It is also unclear how tight the bound provided by Theorem 1 is.  Is the bound vacuous? Relevant statistics in the experiments might need to be reported to elucidate this point.\n\n3. It is also unclear how the trade-off in point (b) of the abstract is justified in the experiments.\n\nMinor Points:\npage 2, the definition of $X^{(j)}$, the index of $l$ and $j$ seem to be typos.\npage 2, definition 1, the definition of the bracket need to be specified. \npage 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability.\npage 4, Corollary 1, should it be $nnz(\\hat{W})\\le JN k_{\\mathcal{S}}$?\npage 7, Table 2, FLOPS should be FLOP? \npage 8, is FLOP related to the time/speed needed for compression? If so, it should be specified. If not, compression runtime should also be reported.\n\n\n\n\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "val"
    },
    "a0yodLze7gs": {
        "paper_id": "iclr_2021_a0yodLze7gs",
        "paper_title": "Disentangling Action Sequences: Discovering Correlated Samples",
        "paper_abstract": "Disentanglement is a highly desirable property of representation due to its similarity with human\u2019s understanding and reasoning. This improves interpretability, enables the performance of down-stream tasks, and enables controllable generative models.However, this domain is challenged by the abstract notion and incomplete theories to support unsupervised disentanglement learning. We demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences. We further introduce the concept of disentangling action sequences which facilitates the description of the behaviours of the existing disentangling approaches. An analogy for this process is to discover the commonality between the things and categorizing them. \n      \n      Furthermore, we analyze the inductive biases on the data and find that the latent information thresholds are correlated with the significance of the actions. For the supervised and unsupervised settings, we respectively introduce two methods to measure the thresholds. We further propose a novel framework, fractional variational autoencoder (FVAE), to disentangle the action sequences with different significance step-by-step. Experimental results on dSprites and 3D Chairs show that FVAE improves the stability of disentanglement.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The initial round of reviews showed a consensus among the reviewers that the presentation of the paper was poor, the novelty was unclear, claims were not properly justified, and the experimental evaluation and discussion were quite insufficient. The authors provided a rebuttal and an updated version of the paper. Although the updated paper demonstrated that the proposed approach indeed provides some benefits, it appears that the authors were not successful to address the numerous but constructive reviewers' comments.\n\nThe paper is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading. ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "0Lnoczyks-X",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Interesting model idea w.r.t. annealing capacity of latent representation, but relation to competing SOTA approaches requires more clarity",
                "comment": "### Summary:\nIn this submission, a common modelling assumption for unsupervised disentanglement is challenged: that the disentangled representation follows the independence structure of the underlying (data generating) factors. Instead, the paper proposes to consider *action sequences* which describe how datapoints are interrelated. The paper provides evidence that the capacity of the latent representation (controlled by Lagrange parameter beta in beta-VAE related models) is related to the significance of particular action sequence for disentanglement. To leverage this insight, the fractional VAE (FVAE) is proposed, consisting of several sub-encoders and different training stages. The disentangling properties of the FVAE is demonstrated on the dSprites and 3D chairs datasets, with the FVAE performing favourably to the beta-VAE w.r.t. the Mutual Information Gap (MIG) disentanglement metric on dSprites.\n\n### Strengths:\n- Novelty / relevance: The submission addresses the important topics of inductive biases and disentangling factors in learning disentangled representations and suggest the interesting and novel concept of action sequences which seems to be related to the general idea of uncovering symmetries with deep latent variable models. In particular, the annealing approach with respect to the KL-divergence Lagrange parameter beta in the FVAE setting to separate \u201csignificant modes\u201d might pose a relevant insight useful in other related approaches and to a more broader audience. \n\n### Weaknesses:\n- Technical quality / significance: The submission mentions the similarities to approaches like AnnealedVAE by Burgess et al. and qualitatively discusses differences and relates some results to this competing approach, but an empirical evaluation of the proposed approach to the competing method is missing. This is quite important, as the technical details of annealing the capacity of the latent representation seem very much alike. Also comparing the disentangling scores to more state-of-the-art approaches like FactorVAE (Kim and Minh) would be important. The evaluation is solely done with respect to the beta-VAE which might not be the most relevant competitor here. For instance, figure 3 in Burgess et al. reports a similar finding as provided in figure 7a in the submission, i.e. controlling the information capacity disentangles first positional / translational factors, then scale and then orientation / shape. Therefore, it is difficult to assess the validity of the claims of the proposed approach and whether a significantly different contribution than in Burgess et al. is made.\n- Figure 5a suggests for dSprites that position/translation, scale and shape are the relevant actions in that order. However, the result in figure 7a suggest, that first translation, then scale and lastly rotation are gradually disentangled which seems to contradict the first result in figure 5a. Shouldn\u2019t these be the same?\n- Figure 6a and 6b are not explained or discussed and their interpretation is not clear. A reader might be familiar with similar plots e.g. in the paper by Higgins et al., but still the key insight should be stated somewhat more clearly in the paper.\n- Clarity: At times it is difficult to follow the presentation of the content in the paper and in some cases I find it hard to follow the statements and conclusions. For instance:\n- Toy example in section 3.1, especially last paragraph: I believe the interpretation of the results in figure 1 requires a little bit more explanation. As I understand it, the ground-truth factors here are the positions X, Y of the rectangles. The dataset provides the variables (i) orientation of the rectangle, (ii) coordinates in either Cartesian or polar coordinate system. I do not quite follow the statement that *\u201c[\u2026] learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d* In case (A1, A3) I would say the latent representation is the same up to permutation of coordinate axes / rotations, which is inherent to VAE / PCA approaches. I.e. the meaning of the axes would be still the same (up to these transformations). Therefore, I am also not quite sure about the statement: *\u201cAs we have shown in Sec. 3.1, the orientation of the rectangle can affect the direction of the disentangled representation\u201d* (p. 5). The \u201cdirection\u201d of the representation is less relevant, as the interpretation of the axes is still the same. However, I might miss the point which is tried to be made here. Could the authors comment on that?\n- Definition of action sequence, section 3.2: The paper tries to motivate \u201caction sequences\u201d but in my opinion the notion remains somewhat unclear in an abstract setting. A \u201cmeaningful action sequence\u201d is defined as *\u201ca sequence / ordered permutation of elements from a subset of the dataset, which reveals the relationship among the elements\u201d*, with elements being images here. In a simple example as scaling or translating objects, this notion and the distinction to \u201cground-truth factors\u201d might be clearer. However, in more complex / less structured examples, say images of faces, the difference between \u201caction sequence\u201d and \u201cground-truth factors\u201d is not very clear to me. The paper suggests for a more formal definition to consider Higgins et al. but, in order to be self-contained and clear, a more explicit and formal definition of this notion is required in the paper, in my opinion. Could the authors maybe provide a more formal definition? \n\n### Additional Feedback:\n- Page 6 and figure 3: *\u201cPlease note that the maximum for both are reached when theta=90 and L is at its maximum.\u201d* Figure 3 suggests that the maximum (yellow region) is reached for large L and theta close to 0 or about 180. It seems that there is a discrepancy between the description and the figure.\n- Figure 5, page 7: In 7b the legend specifies integers, but it is not clear, what these integers encode. And is it maybe *\u201cKL divergence vs beta\u201d* (-> *\u201dy against x\u201d*) in the caption?\n\n- Abstract (p. 1): I would suggest rephrasing the following sentence:  *\u201cWe demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences.\u201d*\nMaybe get rid of the first *\u201cand\u201d* as well as making clear what *\u201cfactors\u201d* (maybe rather *\u201cground-truth / separating factors\u201d*?) are meant. On the first read, this sentence was quite confusing to me.\n- Introduction (p. 1): Second sentence, *\u201cthinking\u201d* -> *\u201cthink\u201d*.\n- Introduction (p. 1): Third sentence, *\u201c[\u2026] single glance this is because [\u2026]\u201d* -> *\u201c[\u2026] single glance. This is [\u2026]\u201d*.\n- Introduction (p. 1): Notion paragraph first word, *\u201cthe\u201d* -> *\u201cThe\u201d*.\n- Introduction (p. 1): Notion paragraph, *\u201c[\u2026] a question arise here is [\u2026]\u201d* -> *\u201c[\u2026] a question which arises here is: [\u2026]\u201d*.\n- Figure 1, caption: *\u201cleaned\u201d* -> *\u201clearned\u201d*.\n- Section 3.3, incomplete sentence after equation 5 or unnecessary *\u201c,\u201d*.\n- Section 4, page 6: *\u201c[\u2026] leading to the disentangling process decays [\u2026]\u201d* -> *\u201d[\u2026] decaying [\u2026]\u201d*\n- Section 4, page 6: *\u201c[\u2026] targeted action into the leaned codes.\u201c* -> *\u201c[\u2026] learned [\u2026]\u201d*\n- Figure 7, page 8: Full stop *\u201c.\u201d* missing in the last sentence of the caption.\n\n### Recommendation:\nIn general, the paper deals with relevant issues in learning disentangled representations and provides interesting tools to address some of these aspects. In particular, the annealing procedure in the FVAE is potentially a relevant contribution. However, the relation to similar approaches is not evaluate adequately, in my opinion, which makes it difficult to assess the justification of some claims. Also, a careful revision of the submission seems advisable which might clarify some of the aspects raised above. In the current form, I believe that the paper is not ready for publication and I would rather see this submission rejected. Nevertheless, I am willing to reconsider my rating if the authors are able to address some of the concerns and questions raised above.\n\n### Post-Rebuttal:\nI want to thank the authors for their responses and clarifications. I think the revision already improved the quality of the submission quite a bit. However, I still believe that there are some aspects which need a better presentation and clearer discussion. \n\nFor example, a more direct discussion and (empirical) comparison to other approaches like AnnealedVAE is necessary, as also other reviewers pointed out, to justify the points made (qualitatively) in the paper. The added results in figure 6c already provide results in that direction.  \n\nI appreciate the clarifications in the notions of action and action sequence. Although I agree that the notions are comprehensible in the toy example and dSprites setting, I still think that the point I raised in my initial review applies. In order to provide a well-defined notion a more formal definition is required. To me it is still unclear what an action sequence in the case of e.g. images of faces should be.\n\nI genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion. Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. However, I would like to encourage the authors to do a major revision taking the issues raised by the reviewers into consideration and to submit again.\n\n\n### References: \n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained\nvariational framework\u201d, ICLR 2017.\n- Kim and Mnih, \u201cDisentangling by factorising\u201d, ICML 2018.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "02b3uKKfUTh",
                "reply_to": "mwf_pCynhc9",
                "title": "Response to Reviewer1",
                "comment": "Thanks for your detailed feedback and the insightful reviews!\n\n> What's the KL divergence here? \n\nThat's right. The KL divergence is the regularization term of the VAE objective.\n\n> KL divergence is consistent with that of entropy.\n\nBasically, the relationship between the KL term and entropy has not yet been verified. We admit that mainly is a hypothesis, though we conduct two experiments in Fig. 3 and 11. However, the current works, such as AnnealedVAE and PCA Directions indicate similar results that different actions have different thresholds. Indeed, such a conclusion can hardly induce; hence, abundant experiments are required. However, the available datasets are insufficient to verify it.\n\n> What's definition of the label here?\n\nThe dSprites is an artificial dataset and contains the factors information. The labels are ground-truth factors. Although our method is an unsupervised approach, the calculation of action's entropy needs label information.\n\n> The authors arranged three stages for dSprites. \n\nThe direct answer is prior to this dataset. We already know it consists of five factors, and two are similar (posX, posY); the shape is not a good action. Besides, we also provide an annealing test in Sec 4.6 to no prior information case. Increasing the number of stages has little influence on disentanglement, but it does waste the computational resource.\n\n> training objective function\n\nThe objective function is beta-VAE. The only difference is the training process.\n\n> How are the curves in Fig. 5 derived?\n\nEach line denotes the dimensional KL diverge over \u03b2 increasing. For the supervised case, we can name the dimension by its most informative factor. For the unsupervised case, we show their index of dimension.\n\n> Thus, the following three-stage training process is questionable.\n\nFor now, FVAE needs the participation of humans. It may not be a bad idea because unsupervised disentanglement learning without inductive biases is impossible. The main purpose of this work is not to propose a competent disentangling method. We focus on the interpretation of why VAEs can disentangle. In this work, we try to provide an insight into combining the data and the representation, and the thresholds could help disentanglement.\n\nReference:\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xZGTs_fBrux",
                "reply_to": "uCwzsSi3Am5",
                "title": "Response to Reviewer4",
                "comment": "Thanks for your detailed feedback and the insightful reviews! \n\n**Ambiguous definitions**\n\n1. Action : *the continuous set of images over a certain direction.*\n2. Action sequence: the discrete action or a sequence of sampled images from the action.\n3. Generating action sequence: the action sequences traversing the ground-truth factors.\n4. Learned action sequence: the action sequences traversing the latent variables.\n5. Entropy of action:\n\n$$H(S') = - \\frac{1}{N}\\sum_{x_i \\in S'} \\mathrm{log} \n        (\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp^{-\\frac{(x_i-\\bar{X})^2}{2\\sigma^2} })$$\nwhere $S'$ is the set of an action, $x$ is the sampled images form this action, $\\bar{X}$ is the mean of the action.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mxmoeU86x3q",
                "reply_to": "t4Js5vRgHqo",
                "title": "Response to reviewer 3.",
                "comment": "Thanks for your detailed feedback and the insightful reviews!\n\n**The difference between Annealed VAE**\nThe interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely.  \n\n> Would this help disentangle position at first then orientation of rectangles?\n\nWe are sorry for the confusing description that A1-3 have the same two-dimension factors denoting images' position. The orientation of images is a fixed property of the dataset. \"The most significant improvement\" should have the largest variation, which can also understand from the PCA theory. In fact, the current theories (information bottleneck, PCA Directions) support the results of A1-3. The action sequence moving along the rectangle's short side has the largest variation and improves the log-likelihood the most significantly. Hence, we say disentangling action sequences is a proper description of disentanglement.\n\n> a formal definition of the inductive bias is still unavailable.\n\nThough Burgess and Rol\u0131nek indicate the inductive bias, they don't propose a calculation for that. \n\n> existing models disentangle the ground-truth factors by accident.\n\nRol\u0131nek used a similar expression, \"Variational Autoencoders Pursue PCA Directions (by Accident).\" That means the success of the current approaches (beta-VAE, TC-VAE, AnnealedVAE, DIP-VAE, FactorVAE) mainly contributes to the well-designed dataset. For instance, they fail to disentangle in the cases of A1-3. If the disentangled representation depends on the data, these approaches don't guarantee the disentanglement when facing an unknown dataset.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LvWAinpHjnj",
                "reply_to": "0Lnoczyks-X",
                "title": "Response to Reviewer 2",
                "comment": "Thanks for your detailed feedback and the insightful reviews! We feel sorry about failing to show the comprehensive results of our work. We hope our responses correctly answer your concerns.\n\n**The different contributions** \n1. The interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely. \n2. As far as we know, we are the first to define the inductive biases on the data and associate it with disentanglement. Though Burgess and Rol\u0131nek indicate something similar, we give a former and explicit definition. \n3. We improve disentanglement by solving re-entanglement.\n\n**Explanation of toy examples** We want to show some evidence of inductive biases on the data in this part. The learned action sequences should match the generating action sequence precisely for the popular view of disentanglement learning. However, the experimental results of A2 and A3 reveal that the current disentangling approach learns a significant action sequence or the principal component on the data.  We will update this figure for easy understanding. A1 and A3 have the same generating action, but the model learns two different action sequences. In contrast, A1 and A2 have different generating actions, but the model learns similar action sequences.\n\n**Contradiction** The orders should be the same if they are all actions.  However, there are only three types of shape, eclipse, square, and heart.  We don't feel surprised by this result because *shape* is not an action like others having internal frames. An action should consist of a series of continuous images. \n\n**Notion** Action: The continuous set of images over a certain direction. \n\nThis notion denotes the real action in reality, i.e., a ball falls. However, the action is infeasible for the machine, and we have to sample the discrete action as *an action sequence*. Therefore, a subset of the dataset varying one factor is an action sequence. We call the action sequences generated by the ground-truth factors *generating actions* or just *actions*, and the reconstructed sequences by the decoder *learned action sequences* or just *action sequences*.\n\n\nReference:\n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained variational framework\u201d, ICLR 2017.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019.\n- Francesco et al., Challenging common assumptions in the unsupervised learning of disentangled representations. In 36th International Conference on Machine Learning, ICML 2019.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "knDDkRot8Q8",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Response to All Reviewers",
                "comment": "I apologize for my poor writing skills and all the defects of our paper. It seems necessary to reclaim the motivation of this work. This paper's main purpose is to emphasize the importance of the data itself on disentanglement learning. We argue that a proper definition of disentanglement is disentangling explanatory action sequences because an isolating sample and its representation are insufficient for disentanglement or interpretability.  The internal relationships between the samples are the key to understanding and disentanglement.\n\nThe current works mainly try to interpret disentanglement from the model perspective.\n1. AnnealedVAE claims that an information bottleneck enforces the model to find a local minimum for the objective, and \"which are aligned with factors of variation.\" It suggests that increasing information on latent leads disentanglement.\n2. Rol\u0131nek shows the similarity between PCA and VAE about \"the local behavior of promoting both reconstruction and orthogonality.\"\nHowever, we believe that the data plays a primary role, and the model is secondary. Therefore, we examine the effects of the data by four cases in Sec. 3. The special cases show little correlation between the learned representation and the ground-truth. Though PCA-like behaviors can interpret the A1-A3, it needs more explanations to interpret A4. The other difficulty is measuring the principal component quantitatively. Dividing the dataset into action sequences also helps us calculate the entropy of actions or the variation of components.\n\nOur method is similar to AnnealedVAE both in results and the method; nevertheless, the interpretation is the main difference. We argue that the step values of beta instead of gradually modifying are vital to disentanglement. The other reason is the phenomenon of re-entanglement. The disentanglement metric reaches the highest in the middle phase, and it falls on the last phases. Here is the MIG score of one trail (beta-vae):\n\n| Step | discrete_mig        |\n|------|---------------------|\n| 230  | 0.3756360504736129  |\n| 461  | 0.37883395407382375 |\n| 693  | 0.399240366607318*  |\n| 924  | 0.3772557544010944  |\n| 1156 | 0.3333029456260484  |\n| 1387 | 0.3523813802097221  |\n| 1618 | 0.3801707492631984  |\n| 1850 | 0.349180837978194   |\n| 2081 | 0.3663198905598549  |\n| 2313 | 0.34369199263536326 |\n| 2544 | 0.3663410570974127  |\n| 2775 | 0.353156190268057   |\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mwf_pCynhc9",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Algorithm is not clearly explained and more experiments are needed.",
                "comment": "Summary:\n\nThe authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.\n\n#################\n\n\n. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \\beta-VAE?\n\n. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  \n\n. Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?\n\n. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.\n\n. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.\n\n. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \\beta-VAE. \n\n\n",
                "rating": 2,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "t4Js5vRgHqo",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Disentangling action sequences is interesting but more details and experimental results are needed.",
                "comment": "Summary: \nThis paper addresses the problem of disentangling representations using Variational Autoencoders. In particular, the authors introduce the concept of disentangling action sequences and propose the fractional variational autoencoder framework to disentangle them step-by-step. To this end, they analyze the inductive biases on the data and define latent information thresholds which are correlated with the significance of the actions.\n\n##################################################################\n\nStrengths:\n- The paper tackles the important problem of disentangling representations.\n- Overall, the paper is well structured. In particular, the introduction section clearly motivates the problem and summarises existing approaches.\n- The idea of disentangling action sequences is interesting as it allows to analyse the inductive bias on the data.\n\n##################################################################\n\nWeaknesses:\n- Fractional variational autoencoder (FVAE) proposed in this paper is closely related to the work of Burgess et al. (2018). Although authors include a brief discussion comparing both methods, the main novelty of FVAE (i.e. explicitly avoid mixing the factors and defining thresholds to prevent re-entanglement for extremely high capacity) is still not sufficiently emphasised throughout the paper. Moreover, it would be good to include experimental comparisons to Annealed VAE (for instance in Figure 6) to give more insights on the relevance of the proposed approach.\n- Description of the toy dataset family is not easily understood and it would be good to clarify annotations in Figure 1(a). Since Figure 9 of Appendix is clear enough, it might be nice to include it in the main paper to help the reader follow the analysis of the corresponding experiment. In the latter, it is shown that the disentangled representations are not invariant to orientation of rectangles (A1, A3). Here, one can assume that positions x and y and orientation of rectangles contribute differently to reconstruction. Hence, it would be interesting to see the effect of progressively increasing the bottleneck capacity on the obtained representations, as proposed in Burgess et al. (2018). Would this help disentangle position at first then orientation of rectangles?\n- While a quantitative analysis has been provided in Figure 6 using the MIG metric to compare FVAE and Beta-VAE, it is still insufficient to make clear conclusions on the performance of the proposed method. Several metrics (e.g. Mutual Information Gap, Modularity, etc.) and evaluation benchmarks have been integrated in DisLib (Locatello et al. (2019)) allowing easy evaluations of disentangling approaches. I recommend using it for further quantitative analysis. \n- In Section 2, authors present the concept of disentangling representations and describe the work of Locatello et al. (2019) which shows the necessity of inductive bias to unsupervisedly disentangle the underlying factors. After mentioning that \u201ca formal definition of the inductive bias is still unavailable\u201d, this point would be more clear with some examples, for instance the assumption in Burgess et al. (2018) that Beta-VAE aligns latent dimensions with components that make different contributions to reconstruction.\n- In Section 3.1, authors mention that existing models disentangle the ground-truth factors by accident. This would be a little misleading to previous claims on the role of inductive bias on the data (or the model) which allows to achieve disentanglement  Locatello et al. (2019). I suggest more clarification to this point.\n",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "uCwzsSi3Am5",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "This paper gives a new kind of comprehension to disentangled representation learning. In this paper, existing unsupervised disentangled methods are trying to obtain disentangled action sequences instead of independent factors. Through the proposed FVAE model, action sequences with different levels of significance can be obtained step by step, and experiment results show that the weight \\beta has positive correlation with the significance.",
                "comment": "Pros:\n1. This paper gives a new comprehension of existing unsupervised disentangled representation learning method, regarding it as finding commonality of input data and disentangling action sequence information in factors.\n2.  This paper gives a new idea that \\beta has positive relationship with action significance and conduct an experiment to validate it.\n3.  This paper proposes a new variant of VAE called FVAE which learns the disentangled action sequence step by step.\nCons:\n1.  This paper mainly gives descriptions of insights while lacking some formulations to explain the settings and methods better.\n2.  Experimental results are not well organized, some axis lack corresponding labels, like Figure 5. Figures are not clear, like numbers in Figure 1 (a). \n3.  Some definitions are ambiguous, like x in equation 5. \n4.  Some descriptions in the paper are confusing, e.g. \u201cWe argue that the factors are not the key to disentanglement since the learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d This experiment, from my perspective, shows that the learned factors are disentangled in a particular form which is not consistent with the preset ground truth. And different action sequences are also different factors. Section 3.1 might be described in a more considerate way to show what the experiment results really indicate. \n5. There exist some typos in this paper, like \u201cleaned\u201d for \u201clearned\u201d.\n\nOverall review:\nThis paper gives a new comprehension of existing disentangled representation learning by regarding it as finding disentangled action sequences, which is interesting and has some good insights. However, some ideas should be supported by clearer formulations and some conclusions of experiments are not valid. Moreover, the logic of this paper is a little unclear, and experimental figures are incomplete. With some modifications, this paper could be an excellent paper..\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "e2zns8ME8EK",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Review 5",
                "comment": "Manuscript Summary\n====\n\nThis paper constructs a disentanglement problem from a temporally causal view, where data are observed in sequences, and where actions cause those observations to change as the sequence progresses in time. Their stated objective is to recover the specific actions and their parameters (e.g. rotation and translation, and their magnitudes/signs).\n\nThe authors thus construct a \"Fractional VAE\" (FVAE), and then construct sequences from Dsprites and Chairs based on their statement of the problem.\n\nInitial Decision (from this reviewer), Review, and Reasoning\n====\n\nI think this paper should be rejected; were this a journal, I would suggest at least major revision.\n\nOverall the concept of bringing temporal causality (which for some cases *is* valid as the causal diagram/frame) into the disentanglement problem statement is a good idea. However, after that point in the manuscript, I cannot understand what has done. For example, section 2 is a restatement of previous work, and section 3 begins with an explanation of the dataset construction. Section 3.3, section 4, and Figure 4A I think describe the paired asymmetric autoencoder method, but a sparse few paragraphs are given at this point. What they _do_ describe is a set of \"sub-encoders\" with varying compression rates $\\beta$. However, beyond varying the rates, it's not clear how particular \"ground truth\" factors (e.g. $\\theta, L$) can be selected for and locked in to specific latent factors in an unsupervised manner, or even if this should happen.\n\nBy varying $\\beta$ we receive different amounts of information in the representation, but how can we ensure across \"learning phases\" disentanglement? Further, if these KL divergences are set to different $\\beta$, this means we don't have a divergence for the joint representation? (the concatenation of the sub-encoders) So how can we ensure that these are disentangled themselves? While successively learned encodings would optimally not include previously encoded data, why would these encoders learn separate concepts instead of coarse grained representation with all concepts to successively finer representations (or refinements to those coarse grained representations) with each successive sub-encoder.  Or are these separate phases repeated?\n\nPerhaps these questions have answers in the positive, but they should be answered by the manuscript.\n\nI further cannot make a connection between the actions sequences and the training methods/arch. I think I have understood both (...save for the above highlighted problems), but I cannot understand where the sequences come in practically speaking, even modulo the aforementioned issues. How does the FVAE or its training scheme use this information? Does it use this information?\n\nI think the positive experimental results in Figure 6c mean that there is something here. However, I cannot tell given section 4 what is actually being done.\n\nSuggestions\n====\n\nI suggest a clear procedure section with numbered steps. It is of vital importance that the reader understand what has been done. If it is already there, it should be made much more obvious/clear.\n\nI think the connection between sequences of images under actions and the proposed method needs to be made, or, if I missed this connection, should be made clear.\n\nThe initial portion of section 3 concerning dataset construction might also be moved to much later.\n\nThere are philosophically challenging sections which I did not comment on in the review portion. I think these are approximately orthogonal to the method due to the scope of the problem: rotation/translation may be disentangled. Can dog breeds *be* disentangled, even in theory (example from Section 3)? Disentanglement of simple mechanisms/\"actions\" are perfectly acceptable at least in my opinion for the state of the field at this moment. Using more complex examples may not be helpful. Similarly, the discussion in section 1 raises questions that are unrelated to the later method. Since a literature review is undertaken in Section 2, the paper could have started at \"In this paper, we first demonstrate that instead of the ground-truth factors the disentangling approaches [should] learn [disentangled] actions.\", with an update for phrasing.\n\nI would also give the paper another read through for grammar.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test"
    },
    "DGIXvEAJVd": {
        "paper_id": "iclr_2021_DGIXvEAJVd",
        "paper_title": "Learning Chess Blindfolded",
        "paper_abstract": "Transformer language models have made tremendous strides in natural language understanding. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess.  Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that chess notation itself allows for directly probing the world state, without requiring any additional probing-related machinery.  Additionally, we have access to a vast number of chess games coupled with the exact state at every move, allowing us to measure the impact of various ways of including grounding during language model training. Overall, we find that with enough training data, transformer language models can learn to track pieces and predict legal moves when trained solely from move sequences. However, in adverse circumstances (small training sets or prediction following long move histories), providing access to board state information during training can yield consistent improvements.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "I thank the authors for their submission and very active participation in the author response period. World state tracking is an important problem that encompasses existing problems like coreference resolution. I agree with R2 and R3 that proposing a novel environment in which we can investigate to what extend Transformers can tackle world state tracking should be interesting to the community. The majority of the reviewers agree that this paper presents an interesting benchmark [R2,R3,R4] with good thorough experimental work [R1,R2,R4]. However, R1 is confused about the positioning of the work and R4 finds the work narrow. R2, despite positive review, agrees with this assessment. I agree with this assessment as well and, after discussion with the program chairs, came to the decision that this paper is not ready for publication in its current state. I strongly encourage the authors to incorporate R1's and R4's feedback, in particular with respect to positioning this environment in comparison to TextWorld, and resubmit to the next venue.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "BMMGNmlo1ge",
                "reply_to": "QmDeyu3SOIP",
                "title": "Comparison with other frameworks and contributions of the work",
                "comment": "We thank the reviewer for their appreciation of our effort and the increase in score. \nOur claims about the frameworks were based on the intended use case. While it may be possible to use frameworks like TextWorld to just predict observations, this is not what the original work or the follow-up work has done. Moreover, it is not clear to us that TextWorld-like environments allow for probing the model in the same way.\n\nOur work demonstrates that in chess we have a tailor-made domain where models can be trained with the language modeling loss, and evaluated on world state tracking via simple prompts. We are not aware of any other framework that readily allows for this. Moreover, our contributions are not just limited to proposing this new framework. The results of using the proposed framework provide insights into how effective transformers are at tracking the world state, their robustness to input perturbations, and, perhaps most importantly, the remaining challenges facing transformer LMs even in this relatively simple domain.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LQmJrTEqC5a",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Review 1",
                "comment": "Summary: This paper explores the abilities of transformer-based models to do grounded state tracking via chess. They train a GPT-2 on chess games, showing that it can learn the rules of the game by being able to state-track and predict valid next actions.\n\nPros:\n1. A very thorough set of experiments are given to explore how transformers can be used to track states in a game such as chess and results show that with enough data, transformers can predict the locations of pieces across a decent amount of history as well as predict legal actions.\n2. The paper is well-written in terms of general writing clarity and I was able to follow *what* was happening throughout.\n\nCons:\n1. I am a bit lost as to the motivation and positioning behind the paper, i.e. I was unsure as to *why* things were happening as they were. The authors say that they are using transformers to see how transformers can learn grounded language when world states are available but I do not see this work positioned with respect to other work on grounded language nor work on state tracking generally found in model-based RL.\n(i) An example of the former for grounded language learning (given that they cite Bender and Koeller) would be instances such as vision and language navigation (Anderson et al. https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), the Nethack Learning Environment for game grounding (Kuttler et al. https://arxiv.org/abs/2006.13760), or text games (Cote et al. https://arxiv.org/abs/1806.11532 and Hausknecht et al. https://arxiv.org/abs/1909.05398). I am not sure how state tracking in chess implies that transformers can do grounded language learning.\n(i) In terms of just the state tracking parts, there has already been much work in agents that learn the rules of the world as they play through them. This can happen with World Models (Ha and Schmidthuber https://arxiv.org/abs/1803.10122) or in cases like Alpha-Zero (Silver et al. open access version https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd) which learn the rules of Chess from scratch via self play. How does using a transformer compare to these works?\n2. Given the above and the fact that all of the pieces of the methodology of this work are taken from others (the architecture, training, state representation, etc.) - the main contribution is the experimental design and the results themselves. In this case, I would have liked to see more analysis regarding exactly what properties of the transformer they think is responsible for helping the model to learn and also a potential qualitative analysis of what the failure cases are.\n\nOverall, the paper has some interesting ideas, experiments, and results but is not connected to the motivation/is not positioned well with respect to closely related work.\n\nPost author response:\nSee comment below for further score justification.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "QmDeyu3SOIP",
                "reply_to": "dKBUxLgokNR",
                "title": "Response to Rebuttal",
                "comment": "I appreciate the authors efforts to clarify my questions and revise their manuscript.\n\nI am satisfied with the answers given differentiating this work from Alpha Zero as well as the additional experiments performed. I would contend though that the differences with the other environments I have provided in point 1 of my initial review are not sufficient. The three dimensions given with respect to differences with TextWorld (and hold for the other envs too) are not entirely accurate. There is nothing in the framework itself that focuses on reward maximization instead of next state probability - its the same as having a chess simulator where you can either focus on predicting the next state or just have an external reward the indicates whether or not you've won the game. It is possible to generate oracle traces, etc. equivalents to this chess dataset in most of these frameworks.  Overall that is to say, chess can also be framed in exactly those three terms and given that these are frameworks and not agents, you cannot say that these three dimensions hold. \n\nThis being said, in appreciation of the author's efforts for the other clarifications - I will increase my score to a 5.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H6RiRjk3IyA",
                "reply_to": "1Gvj7GQWLUA",
                "title": "Follow up 1",
                "comment": "We appreciate your positive comments. \n\nTo reiterate the wider implications of this work, our results shed light on the following interesting properties of transformers: (a) they are robust to RAP-like changes in input distribution, and (b) they require access to long context (Appendix C.1), and large training sets. Future work can use the first finding to introduce the world state, or more specifically the output of linguistic analyzers such as coreference, via RAP-like tokens during pre-training and fine-tuning of transformers. RAP-like tokens can also be used for debugging/diagnosing the model\u2019s understanding, similar to the starting square prediction tasks. The second finding can be another motivation to search for new architectures that are adept at understanding long text with access to limited history (Rae et al., 2020), and that require small training sets. The framework we have introduced allows for probing and understanding new architectures that address these challenges. \n\nAdditionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embeddings.  This makes us optimistic about the proposed future work direction. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "a_wNUuuNIz5",
                "reply_to": "1F2i8m95k5K",
                "title": "Additional comments on RAP",
                "comment": "We wanted  to add a couple of more findings on RAP:\n* In RAP, piece types are not added during inference. Though in earlier experiments, which we didn't report, the RAP models used piece types during inference and it certainly helped the model (as suggested by the reviewer). The reasons for not using piece types during inference have already been explained in the previous reply. Note that the oracle model has access to all the piece types during training and inference. \n*  Additionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embedding. \n\nHope this helps. Let us know if any further clarifications are needed. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cLH-0kgzCF",
                "reply_to": "_AD5WkeYFtW",
                "title": "Follow up 2",
                "comment": "We thank the reviewer for their increase in score. \n\nRegarding the wider implications of this work, we want to say that our results shed light on the following interesting properties of transformers: (a) they are robust to RAP-like changes in input distribution, and (b) they require access to long context (Appendix C.1), and large training sets. Future work can use the first finding to introduce the world state, or more specifically the output of linguistic analyzers such as coreference, via RAP-like tokens during pre-training and fine-tuning of transformers. RAP-like tokens can also be used for debugging/diagnosing the model\u2019s understanding, similar to the starting square prediction tasks. The second finding can be another motivation to search for new architectures that are adept at understanding long text with access to limited history (Rae et al., 2020), and that require small training sets. The framework we have introduced allows for probing and understanding new architectures that address these challenges. \n\nAdditionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embeddings.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "o_tBTgw7vE3",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Paper Revision Overview",
                "comment": "We thank all the reviewers for their valuable feedback which we have tried to incorporate in the current revision. This comment is intended to give a consolidated view of all the changes made:\n* We have substantially revised the related section work based on Reviewer 1 and 4's feedback.  We have tried to specify more clearly the contributions of our work in the revised introduction section. \n* \"Legal moves\" are now denoted as LgM to avoid overloading of LM (Reviewer 2\u2019s suggestion), and \"Exact moves\" use the ExM acronym rather than EM.  \n* A detailed analysis of illegal moves in Appendix D. \n* Added a random legal move baseline for exact move evaluation as suggested by Reviewer 2.\n* The entire experimental setup is available at [this URL](https://anonymous.4open.science/r/f0c718e1-16af-4f1a-a129-d4ace9ac6820/)  \n* Variations over the base transformer architecture (GPT2-small) are explored in Appendix C. Specifically, we report results with limited attention window + bigger models. We find that the success of the transformer model in our setting relies on access to the whole history, and the model suffers performance drop with limited attention window. This suggests that the model is not able to learn a compressed state representation even though chess is markovian and the board state can be described in less than 1K bits.   \n* We have made a minor change to the \"oracle\" baseline which has improved its performance. In the earlier version, we used a limited attention window since the language model doesn\u2019t have to track the board state as it\u2019s already provided. There were empirical reasons as well, the model with limited attention window converged faster and hence did better than the model with access to the full history in the earlier stages of training. But we later found that the model with limited attention converged to an inferior perplexity than the one with access to the full attention history. We have updated the oracle model\u2019s description and the corresponding numbers. (More details in response to Reviewer 3)\n* We plan to publicly release the trained models via the [Hugging Face modelhub](https://huggingface.co/models)\n\nFinally, we want to reiterate the contribution of our work (quoted from revised Introduction):\n* Propose chess as a testbed for evaluating world state tracking capabilities of language models.\n* Show that by selecting (and tweaking) the appropriate chess notation, we can probe the language model for aspects of the world state using simple prompts (Section 3).\n* Propose a suite of probing tasks to evaluate language models for chess on world state tracking (Section 5.3). These probing tasks go beyond simple exact match, and use a more fine-grained evaluation, and allow for automated error analysis (Appendix D).\n* Show that given enough training data, transformer language models can learn to track piece locations and predict legal moves at high levels of accuracy.\n* Evaluate the effect of grounding by training and evaluating a spectrum of transformer language models with varying degrees of access to the world state. We find that grounding helps in challenging settings of our proposed probing tasks.\n* Provide insights on transformer language models such as their robustness to incorporating the world state in various ways, their dependence on access to long context, etc.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_AD5WkeYFtW",
                "reply_to": "-BVhGmQwBEa",
                "title": "Updated score to 5 in light of code release + experimental clarifications, decreased confidence to 3 since score is mostly a function of the narrowness of the contribution, which is more subjective.",
                "comment": "In light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.\nThe main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work.\n The main reason not to increase the score further is that I believe the contribution still is quite narrow.  \n\nI chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work.\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-EaJH0Qn9dR",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Interesting new benchmark that would benefit from more connections to the literature.",
                "comment": "\t\n### Topic\nThis paper explores learning chess from raw notation as a benchmark for the ability of language models to track world state. Chess is an interesting benchmark, as a set of moves can be unambiguously linked to a world state, there are large amounts of data available and the model can easily be probed for its board tracking abilities. The contributions of this paper are twofold: (i) introducing blindfolded chess as a benchmark for grounded language learning and world state tracking, as well as a suite of probing tasks to evaluate models; (ii) empirical evidence that transformer language models can learn both the rules of the game and to track board state.\n\n### Pros:\n-\tBlindfolded chess is an interesting benchmark for grounded language learning and as a testbed for models to track world state. It is unambiguous, data-rich, has a limited vocabulary and models trained on it can easily be probed. This adds to prior papers on learning chess with transformers that have mainly focused on the performance of such models.\n-\tThe use of SAN + RAP trick is an interesting one to be able to probe the current location of pieces.\n-\tThe oracle model is interesting, as it demonstrates the gap between a model that must track world state and one that has access to it. The multi-view model is also interesting as an example of how additional supervision may help the model.\n-\tThe analysis across different data sizes and game length is interesting.\n\n## Cons:\n-\tThe paper feels rushed at times: for instance, there are no references in text to Appendix D, despite it being an interesting demonstration of the analysis that can be done in this environment. \n-\tThe comparison to related work on world state tracking and grounded language learning could be substantially improved. The authors only very briefly mention previous work: the papers on the TextWorld environment (Cote et al, 2018), seem relevant, although the framework is that of interactive environments and RL. Other papers on grounded language learning also seem relevant, such as Alexander G. Ororbia II et al, 2019. More so than the lack of references, an issue with this paper is that few connections are made to wider research issues in grounded language learning / implicit world state tracking. Instead, the text tends to simply state experimental results. This makes the contribution of this paper very narrow.\n-\tIt is not clear why the multi-view trained models underperform on the low data settings compared to the models without the extra supervision. If there is more explicit supervision of the model, should it not be better at tracking world state?\n-\tIt would be good to make clear if this dataset and probing tasks will be released, and the code made available. This would be a welcome step in standardizing this new domain, as prior approaches have all used different settings/datasets/notations/evaluations.\n-\tDespite this being a GPT-small architecture and the authors using a small subset of the available training data, the accuracies are high for legal move predictions on Train L. Those are the ones that are directly related to tracking the world state and knowing which piece movements are allowed. This limits the use of this task as a future benchmark for world state tracking in the data-rich setting. It might be possible to make the task more challenging by asking the language model to predict the entire board state (deconvolution) or by focusing on hard subsets of the task (e.g: pseudo-legal infractions on long histories).\n-\tMany challenges of natural language are not present in this environment, such as coreference, limiting the applicability of results on this task to e.g bAbl. The use of a composite vocabulary (e.g: \u201ce4\u201d instead of \u201ce\u201d, \u201c4\u201d) also limits the compositional challenges of the dataset. However, it is true that using a vocabulary where \u201ce4\u201d is two tokens would make probing more difficult.\n\nMinor comments:\n-\tIncomplete board state: The board state as represented is actually not complete, as it ignores whether a pawn can be taken en passant ( https://en.wikipedia.org/wiki/En_passant ). En passant is rare enough that this might not matter, but it does mean the Oracle/Multi view models do not perfectly capture the state. Note that this seems to also be an issue in Oshri and Khandwala (2015) from whom this representation is derived.\n-\tThe ending square task does not allow us to probe whether the model captures the full range of possible moves for a piece, or whether it selects a subset of those. E.g: the model might reach 100% accuracy on this task without ever moving a rock/bishop/queen by more than one square (less likely that there are pieces in between).\n-\tIt is not clear what you do with games of length 100-150. These seem included in the training set but excluded from the probes. Are those included in the perplexity results of Table 6.\n-\tTypos:\no\tTable 4 breakdown in appendix B\n\n### Recommendation:\n\nI lean reject for this paper.\nThe core idea is interesting, but the paper fails to make connections to wider issues in world state tracking and grounded language learning, making it overly narrow. Both the missing references and the missing links to wider concepts in this litterature are a symptom of that. With a few caveats, the experiments are sound, but the analysis could be improved to go further than simply stating the results. The overall wording and presentation of the paper must also be improved.\n\nQuestions:\n- Are you planning to release the code and data to facilitate work on this topic?\n\n\n\n## Author response update\n\nIn light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.\nThe main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work.\n The main reason not to increase the score further is that I believe the contribution still is quite narrow.  \n\nI chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work.\n",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "1Gvj7GQWLUA",
                "reply_to": "p6aoZ7TolTe",
                "title": "Thanks for the replies!",
                "comment": "Thank you for the detailed replies and adding more baselines. I will stick to my accept score of 7 since I really enjoyed reading the work and thought it had thorough experiments! I won't raise my score further since I sort of agree with other reviewers (partly voiced in my weakness #4) that the work is a bit narrow and does not have too many insights which can be applied to real-world applications. But I appreciate the fact that you have proposed future work in this direction.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "p6aoZ7TolTe",
                "reply_to": "Q1nqxtbXL8",
                "title": "More Analysis Added",
                "comment": "We thank the reviewer for their positive and helpful comments.\n\n> Does model performance / chess quality improve with larger language models like GPT2-md or GPT2-l?\n\nWe have added results with larger models (upto GPT2-medium) in Appendix C.2. For bigger training sets there are minor gains with larger models.   \n\n> What are the kinds of errors these language models make (in terms of legal moves)? Do these errors disappear when you check top-k tokens?\n\nA detailed analysis of illegal moves for the end square prediction is done in Appendix D. We find two prominent error categories: (a) piece(s) obstructing the predicted move, and (b) the current player\u2019s king is in check or remains in check after the predicted move is executed.  \n\n> What are the kinds of moves the model is good at (when considering argmax predictions)? Is it learning any strategy at all? You can measure this quite well automatically using the chess engine scores which indicate who is winning, and comparing the change in scores when the actual move is played vs the language model's predicted move\n\nChess strategy evaluation would be an interesting analysis. However, right now the language model is trained to predict both the winning and losing moves. We would\u2019ve trained the language model in a different way if the focus was trying to train a better chess player. Specifically, adding a prefix bit to indicate the winning player i.e. black or white (and conditioning on that during gameplay) or suppressing the language model loss corresponding to the losing player. This is outside the scope of our current work but an interesting future direction.  \n\n> Finally can insights from (2) and (3) be transferred to other real-world applications? Is there a correlation between the probing literature on natural language processing tasks and the results you find?\n\nWe plan to apply findings from the chess testbed to natural language applications. The RAP experiments have demonstrated that incorporating parts of the world state as tokens in the original sequence is a very effective strategy (similar in spirit to pseudo self attention). In future work, we want to incorporate coreference chains in text via special [ENTITY_i] tokens, where i is the cluster ID, and finetune pre-trained encoders. Similar to RAP, these tokens provide a slice of the world state and can be used as prompts to test the model\u2019s understanding.\n\n >  It will be cool to check other kinds of visual state fusion strategies like pseudo-self attention.  \n\nPseudo-self attention indeed offers an interesting way of incorporating visual modality.  Due to limited time we won\u2019t be able to test it out in the rebuttal phase but thanks for the suggestion.  \n\n> on the bottom of page 6, did you mean Table 2?\n\nYes, thanks for pointing that out. \n\n> A couple of baselines will be useful in Table 2 and 3. The first could be upperbound EM baselines using engines like Stockfish or AlphaGo (since nearly everyone is worse at Chess than them, I expect the EM score to be lower than 100%). The second could be a random lowerbound to EM, where a random move from the set of LM is chosen. Finally it will be good to see EM performance of GPT-2 considering only the set of LM.\n\nWe have added the random legal move baseline to the table. Selecting the top legal move improved the performance on exact end move prediction by UCI in the Train-L setting by 1% absolute in short histories, and 2.7% absolute in long histories. We will add that and the chess engine baseline in the final version.  \n\n> LM is an overloaded acronym which can cause confusion to the reader (language model vs legal move).\n\nThat's a great suggestion. We now refer to legal move as LgM, and exact move as ExM.\n\nWe have added the suggested references. \nLet us know if you have any additional questions or concerns.\n\n\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-BVhGmQwBEa",
                "reply_to": "-zsQm3NJ2tD",
                "title": "(Response continued) Trivial Emulation vs Actual Understanding",
                "comment": "> The ending square task does not allow us to probe whether the model captures the full range of possible moves for a piece, or whether it selects a subset of those. E.g: the model might reach 100% accuracy on this task without ever moving a rock/bishop/queen by more than one square (less likely that there are pieces in between).\n\nThis is a very valid concern. However, the models do achieve reasonably high accuracy on the exact move task which makes us think that it's unlikely that the model is just making trivial predictions. \nWe also conducted a preliminary analysis to verify this. The analysis focused on the distance between the starting and ending square in \"king-moves\" ([Python-chess implementation](https://python-chess.readthedocs.io/en/latest/core.html#chess.square_distance)). We only included moves made by rook, bishop, and queen in this analysis because both king and knight make moves constant in king-move distance (pawns were already excluded due to their simple dynamics).   We first present statistics for the ground truth:\n* Actual Ending Square (Short); Filtered prompts (rook, bishop, queen) = 658; Average king-move distance between starting square and ending square = 2.0; Percentage of prompts with ending square at king-move 1 distance =  48.9\n* Actual Ending Square (Long); Filtered prompts (rook, bishop, queen) = 697; Average king-move distance between starting square and ending square = 2.2; Percentage of prompts with ending square at king-move 1 distance =  42.9\n\nNext, we present statistics for the UCI model trained on Train-L:\n* Actual Ending Square (Short); Filtered prompts (rook, bishop, queen) = 658; Average king-move distance between starting square and predicted ending square = 2.0; Percentage of prompts with ending square at king-move 1 distance =  50.8\n* Actual Ending Square (Long); Filtered prompts (rook, bishop, queen) = 697; Average king-move distance between starting square and predicted ending square = 2.1; Percentage of prompts with ending square at king-move 1 distance =  43.6\n\nFiltering the predicted moves by whether they were correct or not didn't change the stats by much. \n\nTo summarize:\n* The average distance covered in ground truth moves made by rook/bishop/queen for short game histories is about 2 (upper-bound of 7)\n* In long histories the average distance covered is slightly higher. This is quite possible because as the game proceeds, there are lesser pieces on the board and thus lesser obstructions. \n* For both histories, more than 40% of moves are within 1 king-move\n* The predicted moves are slightly closer on average in comparison to ground-truth moves. \n* More often than not, the predicted ending squares are more than 1 king-move distance away. \n\nThe broader implications of LM merely emulating chess without understanding chess still needs more investigation. \nIn particular, we plan to investigate the quality of top K predictions of the model where K could either be the number of legal moves possible in the position or all the moves above a chosen probability threshold. \n\n> It is not clear what you do with games of length 100-150. These seem included in the training set but excluded from the probes. Are those included in the perplexity results of Table 6.\n\nThe games of length 100-150 are indeed used for perplexity calculation. These games can also be used in the future to develop harder evaluations. \n\nWe hope this clarifies the concerns raised above. Let us know if you have any additional questions or concerns.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-zsQm3NJ2tD",
                "reply_to": "-EaJH0Qn9dR",
                "title": "Added connections to literature",
                "comment": "We thank the reviewer for their detailed and constructive feedback. Below we have tried to address the reviewer\u2019s concerns.\n\n> No references in text to Appendix D, despite it being an interesting demonstration of the analysis that can be done in this environment.\n\nWe have added more details to the illegal move analysis section and also added references to the same in the main text (Appendix D).\n\n> The comparison to related work on world state tracking and grounded language learning could be substantially improved.\n\nWe appreciate the pointers to related work and agree that we could be more expansive in describing connections with and differences from this prior work.  We have accordingly significantly expanded the related work section. With regards to the specific references suggested by the reviewer, we quote the following text from the related work section.\n\nTextWorld-style environments resemble ours in that the true world state is, by construction, available, models trained for a TextWorld environment differ in: \n1. their objective (reward maximization vs. maximizing the probability of the next observation), \n2. in how they are ultimately evaluated (final reward vs. world state tracking), and \n3. in whether we can directly probe the model\u2019s knowledge of the entire state. \n\nThe world models of Ha & Schmidhuber (2018) also maximize the probability of the next observation, but differ along the other two dimensions. Similarly the work by Hermann et al. (2017) and Hill et al. (2017) on developing and using 3D world simulations for learning grounded language has only partial overlap with the objective function, and differ along the other two aspects.\n\nOur work is related to work on grounding in that we are interested in comparing model performance when it does not have access to grounding information to when it does (Bruni et al., 2014; Kiros et al., 2014; Ororbia et al., 2019). However, unlike the work of Ororbia et al. (2019), for instance, the goal is not to improve the performance of language models using access to more of the world state, but to assess how much of this state has been recovered by the model from just learning the language modeling task.\n\n> It is not clear why the multi-view trained models underperform on the low data settings compared to the models without the extra supervision. \n\nRegarding multi-view trained models, our guess is that the board state model may require more data than it is being given in Train-S to train well. Without enough data, a poorly learned representation for board state can adversely interact with the language model. Pretraining the board state model might be a useful strategy. However, in an earlier experiment using a fully-connected network for board state, pretraining the network didn\u2019t help.\n\n> It would be good to make clear if this dataset and probing tasks will be released, and the code made available. \n\nThe entire setup is available at [this URL](https://anonymous.4open.science/r/f0c718e1-16af-4f1a-a129-d4ace9ac6820/). We chose a publicly available chess database to keep the setup reproducible. We also plan to release pretrained models via the huggingface model hub. \n\n> Despite this being a GPT-small architecture .... make the task more challenging ...\n\nThe tasks can be made more challenging in various ways, including the suggestion of the reviewer to predict the whole board state. Some other possible ways are: (a) limiting the attention window of the transformer as chess is almost Markovian (results in Appendix C.1) (b) evaluating on even longer game histories, and (c) focusing on Train-S which we refer to as small but still has more than a million moves.\n\n> Many challenges of natural language are not present in this environment.\n\nWe agree that chess notation is incredibly simple compared to natural language. The goal of our work was to start with a simple setting to determine how well transformer language models are tracking the world state, and how this state tracking capability can be improved with access to the world state during training. We found RAP to be an effective and easy way of adding slices of the world state to the text sequence, which improves performance and can later be used for diagnosing/probing the learned models. In our future work, we plan to explore adding entity-related information, such as coreference chains, via RAP-like tokens during pre-training of natural language models. \n\n> Incomplete board state: The board state as represented is actually not complete, as it ignores whether a pawn can be taken en passant\n\nThe board state representation is indeed incomplete. Information such as the possibility of en passant, counter for threefold repetition, counter for fifty move rule, etc., are missing from the board state. Since this information is needed only in rare cases, we didn\u2019t use it as part of the board state. We have mentioned this in a footnote in the revision.\n\n**Response continued in later comment (limit on characters)**",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dKBUxLgokNR",
                "reply_to": "LQmJrTEqC5a",
                "title": "Clarifications on positioning of the work",
                "comment": "We appreciate the pointers to related work and agree that we could be more expansive in describing connections with and differences from this prior work.  We have accordingly added this discussion to the related work section.  \n\nIn relating our work to this other work we emphasize that a major goal of this paper is to determine how well models trained just on symbolic inputs (with a language modeling objective) are able to track the world state underlying these symbolic inputs. Certain probing papers also have a similar goal, but Chess/UCI is unique in that we can precisely automate probing tasks and can evaluate them at a fine-grained level, and this probing requires no extra machinery that also may give less direct answers. \n\nWhile TextWorld-style environments resemble ours in that the true world state is, by construction, available, models trained for a TextWorld environment differ in: \n1. their objective (reward maximization vs. maximizing the probability of the next observation), \n2. how they are ultimately evaluated (final reward vs. world state tracking), and \n3. whether we can directly probe the model\u2019s knowledge of the entire state. \n\nThe world models of Ha & Schmidhuber (2018) also maximize the probability of the next observation, but differ along the other two dimensions. Similarly the work by Hermann et al. (2017) and Hill et al. (2017) on developing and using 3D world simulations for learning grounded language has only partial overlap with the objective function, and differ along the other two aspects.\n\nOur work is related to work on grounding in that we are interested in comparing model performance when it does not have access to grounding information to when it does (Bruni et al., 2014; Kiros et al., 2014; Ororbia et al., 2019). However, unlike the work of Ororbia et al. (2019), for instance, the goal is not to improve the performance of language models using access to more of the world state, but to assess how much of this state has been recovered by the model from just learning the language modeling task.\n\nWith regards to AlphaZero, our setup, goals, and training objectives are very different. \n* AlphaZero has access to the board state and rules of chess. AlphaZero starts with random play governed by rules of chess, and learns from just self-play (Without the knowledge of rules of chess and access to actual games it\u2019s impossible to learn anything meaningful). On the other hand, our models don\u2019t have access to the board state during inference (except for the theoretical oracle baseline) and don\u2019t know the rules of chess.\n* A major goal of this paper is to determine how well models trained just on symbolic inputs (with a language modeling objective) are able to track the world state underlying these symbolic inputs. This is very different from AlphaZero\u2019s goal to demonstrate the potential of self-play in closed domains.      \n* AlphaZero\u2019s training objective is to predict the next move that maximizes the reward, where reward is winning the game, while our training objective is to maximize the probability of the next observation, regardless of the quality of the play.\n\nWe hope this clarifies the positioning of our work w.r.t. prior literature. \n\n> I would have liked to see more analysis regarding exactly what properties of the transformer they think is responsible for helping the model to learn and also a potential qualitative analysis of what the failure cases are.\n\n* We present additional results with variations in the basic transformer architecture (GPT2-small) in Appendix C. In Appendix C.1, we compare the effect of limiting access to previous tokens. Even though chess is Markovian, we find that there\u2019s a significant performance drop with limited history. This demonstrates the dependence of the transformer LM's performance on access to unrestricted history, and its limited ability to learn a compressed state representation for the relatively simple domain of chess.   \n* Appendix D has a detailed illegal move analysis, and we have added references to the same in the main text. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1F2i8m95k5K",
                "reply_to": "n_C9O0bQdV",
                "title": "Clarification on RAP and Oracle Baseline",
                "comment": "We thank the reviewer for their helpful and positive feedback. Below we answer the clarification questions asked by the reviewer.\n\n> I'm a little confused as to why performance peaks with p=0.05/0.15 in the UCI+RAP models. If my understanding is correct, then RAP with p=1.0 would include piece annotations with every move during training time and at inference. So shouldn't this make it easier to track pieces (as the authors themselves note in 4.1)? The reason for this is given as greater \"mismatch between training and inference\" -- I'm not sure what this means.\n\nIn the RAP setting, the **piece types are only added during training** and not during inference, except in the prompt for the starting square prediction task. To clarify this we have added Table 1 with tokenized sequences. The motivation for including RAP in training is twofold:\n* It allows us to probe at test time, where the model thinks each piece is, by simply appending the piece type of interest to any game history's prefix.  \n* We can use the available world state during training but the model doesn\u2019t require it during inference. \n\n> On a related note, I'm also confused by the performance of the Oracle Baseline in Table 3. In some instances, this is outperformed by trained models with less information. But my understanding was that this oracle would serve as an upper bound on model performance. So how are we outperforming the upper bound?\n\nRegarding the performance of the oracle baseline:\n* We have made a minor change to the \"oracle\" baseline which has improved its performance, though it is still not the top performer in two evaluations with Train-L. In the earlier version, we used a limited attention window since the language model doesn\u2019t have to track the board state as it\u2019s already provided. There were empirical reasons as well:  The model with the limited attention window converged faster and hence did better than the model with access to the full history in the earlier stages of training. But we later found that the model with limited attention converged to an inferior perplexity than the one with access to the full attention history. This might be because access to all the previous tokens can better reveal insights about the players involved, and we do see improvements in the Exact move accuracies. We have updated the oracle model\u2019s description and updated the numbers in the latest version. \n* The \"oracle\" baseline is intended to serve as an **approximate upper bound** where the model has access to more of the world state. It\u2019s still a language model and can still make mistakes. The term \"oracle\" is probably a misnomer in our context given its typical use in the ML literature. We plan to change this in the final version but are keeping it for now to avoid too many changes. A more strict upper bound might use the constraints of chess to restrict prediction to legal moves and focus on just predicting the exact moves (suggested by Reviewer 2). That being said, the \"oracle\" model with access to board state doing slightly worse than some of the baselines is still intriguing. Our hypothesis is that the models are currently being trained and tested on both the long and short histories, almost like a multi-task setup. Different models can weigh these \u201cdifferent\u201d tasks differently. Given the evaluation results, the oracle model may be prioritizing the long history tasks over the short history tasks.   \n\n> Couple of minor issues:\nAt the bottom of Page 6, the text references results in Table 3, which should be Table 2.\nSection C in the Appendix is currently empty.\n\nThanks for pointing these out. We have fixed them in the revised version. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Q1nqxtbXL8",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Interesting testbed and surprising results!",
                "comment": "Summary: This paper is an interesting exploratory study analyzing the ability of language models to track the state of a chessboard. The authors adopt a clever chess notation which allows them to probe the language model's state tracking ability by looking at its next word prediction (akin to probes in [1]). Quite remarkably, language models finetuned on chess data store a very accurate state representation, and predict legal moves over 90% of the times even without a visual representation of the board.\n\n-----------------------------------\n\nStrengths of the Paper:\n\n1. A clever probing method to analyze language model's state tracking abilities. \n\n2. Well-designed experiments and very interesting results in a mostly unstudied area.\n\n3. Wel written paper with several baselines and ablations.\n\n-----------------------------------\n\nWeaknesses of the Paper / Possible additional analysis:\n\nWhile the work in itself is very interesting and clean, I would have loved to see more analysis studying the model. This a very rich testbed where a lot of interesting experiments can be done! For instance,\n\n(1) Does model performance / chess quality improve with larger language models like GPT2-md or GPT2-l?  \n(2) What are the kinds of errors these language models make (in terms of legal moves)? Do these errors disappear when you check top-k tokens?  \n(3) What are the kinds of moves the model is good at (when considering argmax predictions)? Is it learning any strategy at all? You can measure this quite well automatically using the chess engine scores which indicate who is winning, and comparing the change in scores when the actual move is played vs the language model's predicted move  \n(4) Finally can insights from (2) and (3) be transferred to other real-world applications? Is there a correlation between the probing literature on natural language processing tasks and the results you find?  \n(5) It will be cool to check other kinds of visual state fusion strategies like pseudo-self attention [2]\n\n-----------------------------------\n\nOther Feedback:\n\n1. on the bottom of page 6, did you mean Table 2?\n2. A couple of baselines will be useful in Table 2 and 3. The first could be upperbound EM baselines using engines like Stockfish or AlphaGo (since nearly everyone is worse at Chess than them, I expect the EM score to be lower than 100%). The second could be a random lowerbound to EM, where a random move from the set of LM is chosen. Finally it will be good to see EM performance of GPT-2 considering only the set of LM.\n3. LM is an overloaded acronym which can cause confusion to the reader (language model vs legal move).\n\n-----------------------------------\n\nOverall Recommendation:\n\nThis is an exciting and rich testbed with a lot of interesting questions to answer. The authors have conducted well-thought experiments and reported interesting results. I'm leaning accept, but I encourage the authors to keep working on this setup (perhaps using some of the suggestions discussed above) and try to check if any of the insights here can be transferred to better understanding of language models on natural language.\n\n-----------------------------------\n\nReferences:\n\n[1] - https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115  \n[2] - https://arxiv.org/pdf/1908.06938.pdf\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "n_C9O0bQdV",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "An interesting application of transformers to Chess playing",
                "comment": "This paper considers an intriguing problem: can language models, when trained on\npurely textual representations of Chess games, learn the underlying dynamics of\nthe game? The authors argue that this could be a preliminary step towards\ntackling the symbol grounding critique of methods like transformers. When\ntransformers are utilized in natural language settings, it's challenging to\ndetermine whether the models are operating at a pure syntactic level, or whether\nthere is some rudimentary level of \"understanding\", given that the models are\nonly exposed to text. In contrast, in Chess, one can train in a purely textual\nfashion (or with some limited symbol grounding information), and probe how well\nthe result models the state of the underlying Chess position.\n\nThe authors use the GPT2-small based transformer architecture and train from scratch on a dataset of\nhigh-quality Chess games between humans, represented in UCI notation -- a\ntextual listing of the moves made by each player. In some experiments, this\npurely textual input is supplemented with explicit board state information as\nwell, to test the impact of this additional signal. The authors evaluate the\nsystem on two types of inference tasks: a trained model's ability to\nsuccessfully locate a piece on the board, and the model's ability to determine\nwhere to move a chosen piece to next. In each case, there are two\nevaluation metrics: an \"exactness\" metric (i.e., whether the model picked the\nsame piece/move as the human did in the corresponding game) and a \"legality\"\nmetric (i.e., whether the model picked a permissible move). The former metric is\nmore stringent, as it's also measuring the strategic awareness of the model. The\nauthors demonstrate through their experiments that transformers are successful\nat both inference tasks with very high accuracy, particularly when evaluated\nusing the legality metric, using shorter sequences of moves, and larger\ndatasets.\n\nStrengths of the paper:\n  + This is a creative application of transformers to a non-traditional textual\n    inference task. It may inspire others to devise other interesting applications.\n    The results are intriguing and expand our understanding of what may be\n    possible with transformer architectures.\n  + The authors' approach is a fairly straightforward application of off-the-\n    shelf techniques -- and I mean that in a good way. There are no unnecessary\n    complications or ad hoc additions to the system design.\n  + The paper is very clearly written, well-organized, and easy to follow.\n\nAreas for improvement/questions for the authors:\n  - I'm a little confused as to why performance peaks with p=0.05/0.15 in the\n    UCI+RAP models. If my understanding is correct, then RAP with\n    p=1.0 would include piece annotations with *every* move during training\n    time and at inference. So shouldn't this make it easier to track pieces (as\n    the authors themselves note in 4.1)? The reason for this is given as greater\n    \"mismatch between training and inference\" -- I'm not sure what this means.\n  - On a related note, I'm also confused by the performance of the Oracle Baseline\n    in Table 3. In some instances, this is outperformed by trained models with\n    *less* information. But my understanding was that this oracle would serve\n    as an upper bound on model performance. So how are we outperforming the\n    upper bound?\n\nOn balance, I think the strengths of the paper outweigh my concerns, and I\nrecommend ACCEPTANCE.\n\nCouple of minor issues:\n  - At the bottom of Page 6, the text references results in Table 3, which\n    should be Table 2.\n  - Section C in the Appendix is currently empty.\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "B4OTsjq63T5": {
        "paper_id": "nips_2022_B4OTsjq63T5",
        "paper_title": "Bayesian inference via sparse Hamiltonian flows",
        "paper_abstract": "A Bayesian coreset is a small, weighted subset of data that replaces the full dataset during Bayesian inference, with the goal of reducing computational cost.  Although past work has shown empirically that there often exists a coreset with low inferential error, efficiently constructing such a coreset remains a challenge.  Current methods tend to be slow, require a secondary inference step after coreset construction, and do not provide bounds on the data marginal evidence.  In this work, we introduce a new method---sparse Hamiltonian flows---that addresses all three of these challenges.  The method involves first subsampling the data uniformly, and then optimizing a Hamiltonian flow parametrized by coreset weights and including periodic momentum quasi-refreshment steps.  Theoretical results show that the method enables an exponential compression of the dataset in a representative model, and that the quasi-refreshment steps reduce the KL divergence to the target.  Real and synthetic experiments demonstrate that sparse Hamiltonian flows provide accurate posterior approximations with significantly reduced runtime compared with competing dynamical-system-based inference methods.",
        "paper_acceptance": "Accept",
        "meta_review": "All reviewers agree that the paper proposes an interesting approach to Bayesian inference incorporating coresets with Hamiltonian flows. Although some reviewers have some technical concerns at their first reviews, basically those have been resolved by the authors' responses. Thus, although there are some points that should be modified from the current form, I think we can expect the authors modify the paper in the camera-ready by reflecting the discussion. Based on these, I recommend acceptance for this paper.",
        "meta_review_title": "Meta Review of Paper6521 by Area Chair 4R4G",
        "reviews": [
            {
                "review_id": "32kXBIzj_s",
                "writer": "author",
                "reply_to": "7LD0fAbOz8M",
                "title": "Continued Response to pCaB",
                "comment": " Thanks for the follow-up! \n\nYour understanding of one of the main strengths of our work is correct: the use of a coreset enables our method to be computationally efficient, and Prop 3.1 shows how big the coreset must be to enable an accurate reproduction of the full posterior. \n\nGeneric normalizing flows (Sylvester, planar, etc.) actually cannot use coresets. Generic flows are constructed of black-box parametrized transformations---the flow structure itself does not use information from the target. In contrast, our sparse Hamiltonian flow directly incorporates the coreset target information (see line 156, eq. 5). This allows us to train the coreset weights.\n\nGeneric normalizing flows also do not generally provide error guarantees (in the sense of \"longer flow provides a lower KL\"). There is some past work on analyzing how expressive these families are, but these results are usually abstract universal approximation results (see, e.g., Theorem 3.1, \"The Expressive Power of a Class of Normalizing Flow Models\", 2020). \n\nIt is certainly possible for a generic normalizing flow to outperform our method on a given problem; but without guarantees, it's hard to say much in advance! (We have also found in our experience that it is hard to train generic flows reliably in practice; they often get stuck in bad local optima.)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7LD0fAbOz8M",
                "writer": "official_reviewer",
                "reply_to": "UUMrDeKALZau",
                "title": "Thanks for the clarifications",
                "comment": " Thanks for the detailed response to my questions. It has improved my understanding of the paper. I look forward to reading the camera-ready version, containing the edits you describe. I will be changing my score to a 7.\n\nIf you can find the time, I have one remaining clarifying question (I realize that we are close to the deadline):\n\nIn your explanation of the advantages of using a Hamiltonian Flow over other methods, you highlight 1) the ability to converge to a target posterior distribution, and 2) that it enables the use of a core set. I'm probably missing something here, but aren't these two properties valid for any choice of normalizing flow? Do I understand correctly that the main advantage is the fact that you 1) can provide guarantees in the form of prop 3.1, and 2) that it is computationally efficient? So, in principle, it might be the case that an different choice of (expressive) flow might provide better results empirically, possibly at a higher computational cost?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "THs8nNQi6I2",
                "writer": "author",
                "reply_to": "6YB6xfMQR2C",
                "title": "Response to Reviewer ofer",
                "comment": " Thank you again for your suggestion! We will include the comparisons in both energy distance and MMD in the supplement for the camera-ready version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6YB6xfMQR2C",
                "writer": "official_reviewer",
                "reply_to": "rHi0FbR8udI",
                "title": "Thank you for the authors' response",
                "comment": " Many thanks for your response and clarification on the points raised! I just have one more question:\n\n> Indeed the relative covariance error seems slow to converge, but this is not the complete picture. We need to look at the relative covariance error plot together with the relative mean error plot (Fig. 2c) and the KL plot (Fig. 2b). In particular, the relative mean and covariance error plots depict two different aspects of the quality of our target approximation; the KL plot takes both of these into consideration. For this particular problem, our method finds the center of the target before fine tuning the covariance. The monotonic downward trend of the KL divergence shows that our method keeps on improving the target approximation throughout optimization.\n\n> To understand why the relative mean error and KL divergence go up for UHA, it is important to note that UHA operates on the augmented space based on a sequence of distributions that bridge some simple initial distribution and the target distribution. Therefore, it is not guaranteed that all steps of optimization improve the quality of approximation on the marginal space of the latent variables of interest. This explains the increase in the error metrics on the \\theta-marginal space shown in Figs. 2b and 2c. However, we note that from Fig 2a, UHA\u2019s augmented ELBO as the optimization objective that we maximize over shows a monotonic increasing trend.\n\nRegarding this, in addition to looking at the relative mean and cov separately, perhaps it would be clearer if you plotted the energy distance or MMD?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V0M5bGlREVMW",
                "writer": "author",
                "reply_to": "Kj7xJtGm1s",
                "title": "Response to Reviewer hoPP",
                "comment": " Thank you for reviewing our manuscript, and for the positive feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> To my understanding, SHF makes some assumptions that imply limitations. For example, SHF chooses a random subset of data points. In cases where a random subset of data points is not representative of the entire dataset, SHF might be fast but not useful. Is this correct? I expect that more complicated data types will run into problems with random subsets more easily. Thus SHF might not be an appropriate solution for more complex models and data.\n\nYou are totally correct; there is usually some probability that the subsample we draw will be totally unrepresentative of the full dataset, at which point the coreset construction is flawed from the start. However, our result in Proposition 3.1 provides guidance on how large one should choose the coreset to be to avoid this problem from occurring. In particular, as long as the coreset size M is roughly d \\log(N), where N is the dataset size and d is the \u201cdimension of the log-likelihood function space,\u201d the probability of randomly obtaining a bad subsample is quite small, as it decays at roughly a N^(-d/2) rate.\n\nNow, as you say, the data might be quite complex\u2014in the notation of our paper, this is when that dimension \u201cd\u201d is quite large for the model under consideration. For example, if the data are very high-dimensional (and do not lie on a low-dimensional manifold), the value of \u201cd\u201d may be quite large. In these cases, we may need to use a rather large coreset, and the approach may not be so useful. \n\nWe will add a discussion of this limitation (and others) in the final camera-ready version.\n\n> Is the subset of M values chosen uniformly at random once at the beginning of the process or does the subset change over time?\n\nThe subset of M uniformly subsampled data points is selected once at the beginning and fixed. However, we do update the weights associated with these data points as we run the variational optimization. \n\n> from steps 0-9 the ELBO decreases rather than increases. I don\u2019t understand why that is the case. My naive assumption would be that the blue lines should also go up just not as drastically as the red lines.\n\nThanks for pointing this out \u2013 great observation! Actually, in theory, the ELBO should stay constant during the simulation of Hamiltonian dynamics if the simulation is perfect (see equation after line 114). However, since SHF uses the gradient of the coreset posterior rather than the full posterior to simulate the dynamics, some error will be introduced. Another source of error comes from the fact that we can only approximately simulate Hamiltonian dynamics using leapfrog steps. Both sources of error can cause the ELBO to change between quasi-refreshment steps. This error could either result in an increase (steps 30-39) or decrease (steps 0 to 9) in the ELBO.\n\n> The limitations of the method were discussed insufficiently and should be addressed.\n\nWe will add some text discussing the limitations of our method for the camera-ready version. In fact, one of the limitations of our method has already been touched upon in the response to your comments above. Specifically, our method relies on the assumption that the data are compressible.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UUMrDeKALZau",
                "writer": "author",
                "reply_to": "XVx60WSMtuy",
                "title": "Continued Response to pCaB",
                "comment": " > \"Figs. 2c and 2d demonstrate...\" To which extent is this explained by the fact that the posterior is Gaussian in this case?\n\nThe quote is just a small remark regarding the plots for this particular example; we did not intend to suggest drawing conclusions outside of this Gaussian location model setting. But we recognize that this may have been implied, and will carefully re-word the text for the camera-ready. Finally, note that we see from Fig 5 that our method outperforms other competing methods under models with non-Gaussian posteriors.\n\n> Why was there a difference in the baseline methods included in Fig 2/3 and Fig 4?\n\nThis work builds upon two lines of literature: coreset construction methods, and Hamiltonian-based VI methods. Each of these plots shows a comparison with methods from each line of past work. Figs. 2-3 compare the sample quality, density evaluation time and sample generation time across the posterior approximations produced by various Hamiltonian-based VI methods. Fig. 4 then compares the quality of the posteriors approximated using the coreset obtained from SHF against other coreset construction methods.\n\nFor Figure 2-3 specifically, it is worth noting that all previous Hamiltonian VI methods do not work with a Bayesian coreset; hence they are often quite slow in the regime of large-scale data. In contrast, SHF incorporates coreset Hamiltonian dynamics, leading to fast training, density evaluation, and i.i.d. sampling.\n\nFor Figure 4 specifically, it is worth noting that all previous coreset construction methods do not enable i.i.d. sampling: one needs to use a subsequent inference method on the coreset posterior after construction. In contrast, SHF does enable i.i.d. sampling and density evaluation with no additional secondary stage.\n\n> I would have liked to see simpler Bayesian Inference baselines such as Laplace and simple mean-field VI included as baselines as well.\n\nThank you for the suggestion. We have added a comparison between our method and Laplace approximation using the same Bayesian linear regression model (Section 4.2) in the supplement in Appendix F. Fig. 8 shows that our method provides a higher quality posterior approximation than the Laplace approximation. Specifically, the approximated KL is around 100 for our method, and around 500 for the Laplace approximation. In general, since Laplace and mean-field VI both use Gaussian distributions to approximate the posterior, we anticipate that these simpler baselines will suffer when the posterior is non-Gaussian.\n\n\n> The paper could be further improved with some reflection on the limitations of the approach.\n\nWe agree that there was not enough attention devoted to this in the current version, and will include a discussion in the camera-ready version. One main limitation of this methodology is that we assume that the data are \"compressible\" in the sense that log-likelihood functions of a subset can be used to represent the full log-likelihood (see our response to comments by Reviewer hoPP). If the data are truly very high-dimensional (i.e. not on some low-dimensional manifold), this may not be the case. Another limitation is that while our quasi-refreshment is simple and works well in practice, more work is required to develop quasi-refreshment methods with general guarantees.\n\n> \"...interleaving MCMC and gradient descent steps... \" This sentence was not entirely clear to me.\n\nWe agree that this could be stated more clearly. The idea is: at each iteration of the optimization, MCMC samples for the current coreset posterior are used to estimate the coreset weight gradient. Since the coreset size (M) is much smaller than that of the full dataset (N), MCMC is not expensive. However, the quality of the MCMC samples may be poor without tuning. And it is not realistic to tune the MCMC sampler at each iteration. We will reword this sentence to make it clearer.\n\n> \"Let C be the universal constant\" ...appears a bit unmotivated here \n\nThe constant C here provides an upper bound on the number of spherical balls needed to cover a d-dimensional unit sphere. It is a technical detail that is not of significant importance. We will revise the paper to provide some more context on this constant.\n\n> \"since w only has the first M entries nonzero\" Was the convention of placing the nonzero elements first mentioned anywhere? \n\nWe mentioned this in line 141; this assumption is merely for notational convenience. We will find a way to make it more prominent in the text!\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XVx60WSMtuy",
                "writer": "author",
                "reply_to": "_ZYCQCa8iYX",
                "title": "Response to Reviewer pCaB",
                "comment": " Thank you for your efforts in reviewing our manuscript, and for the positive feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> ...the authors should provide a dedicated Related Work section to elaborate on the connections to earlier work.\n\nYou are correct that the combination of Hamiltonian-based flow with quasi-refreshment and Bayesian coresets is unique to our work. We can certainly add a dedicated related work section that draws connections to earlier work for the camera-ready version. \n\n> ...explain what the advantages of using a Hamiltonian flow are compared to other choices of normalizing flows.\n\nThis is a great suggestion! The intuition here is that Hamiltonian flow methods involve a sequence of transformations that resemble the steps of Hamiltonian Monte Carlo (HMC). In HMC, we run Hamiltonian dynamics for a while, and then resample the momentum, and repeat; in Hamiltonian flows, we run Hamiltonian dynamics for a while, and then quasi-refresh the momentum, and then repeat. Since we know the Markov chain generated by HMC converges (in distribution) to the target posterior distribution, we expect the Hamiltonian flow to do something similar (although not perfectly, as the quasi-refreshment is not a perfect substitute for exact momentum resampling!).\n\nOne other very important advantage of Hamiltonian flows is that the ODE naturally enables the use of a coreset, which enables us to build a computationally inexpensive flow. By the previous argument, our coreset flow should approximate the coreset posterior reasonably well. Moreover, the optimal coreset should provide a good approximation of the full posterior (for the Gaussian location model this is given by our Proposition 3.1). Therefore, we have constructed a variational family that is both inexpensive to work with and flexible enough to well-approximate the posterior. Standard normalizing flows typically come with no guarantees, and may or may not be expensive to work with depending on design (though more flexible families tend to be more expensive).\n\nWe will add some text outlining these advantages for the camera-ready version. \n\n> ...why this is a representative (or even relevant) example model. Can anything be said about the compression for the posterior of other models? Can anything be said for finite N?\n\nGreat question! By \u201crepresentative,\u201d we mean in terms of how well our result regarding the optimal coreset quality extends. In fact, our proof technique has already been extended to general exponential families (the only difference lies in the choice of sufficient statistic). Unfortunately, this result is in a forthcoming unpublished manuscript which we can\u2019t provide a pointer to quite yet! But if you are willing to take that leap of faith, we note that exponential families can be used to well-approximate a very wide range of models. So we suspect that our result is indicative of a more general setting (at least the fact that the coreset size should scale with some notion of a \u201cdimension\u201d of the model). \n\nRegarding finite N, our proof actually provides a finite-N analysis. See the proof in Appendix C (line 573). However, the finite-N result is somewhat complicated and messy. We chose to present the asymptotic version of the result in the main text due to the simpler final expression.\n\n\n> \"In this work we assume that \\rho_t ~ N(mu, Lambda^-1)\" Does this mean that we are ultimately assuming that the posterior can be approximated by a diagonal Gaussian?  \n\nNo, we do not assume the posterior is a diagonal Gaussian. Let us clarify a bit with two points.\n\n1. The posterior \\pi is set by the likelihood and prior, and is usually not Gaussian. We then augment the posterior with a Gaussian distribution for the momentum. Note that this is not limiting; we are free to pick any \"nice\" distribution for the momentum component. The Gaussian distribution is what people typically use in practice for HMC, and we follow that standard here.\n\n2. The variable \\rho_t is the momentum variable at time t of the Hamiltonian flow. So when we say we assume \\rho_t ~ N(\\mu, \\Lambda^-1), we are making an assumption on the momentum component at time t of our flow. \n\nThe assumption is true for all t if the posterior \\pi is indeed Gaussian. But we actually don\u2019t need this assumption in practical application. The method still works without it, and applies to a wide range of non-Gaussian posteriors. The reason we included this text is because if the assumption holds, the \u201cstandardization\u201d quasi-refreshment in equation (7) is guaranteed to reduce the KL by Proposition 3.3.  This assumption usually will not hold exactly in practice, but we have seen that the standardization quasi-refreshment still provides a significant reduction in KL (Figure 1).\n\nWe will revise the wording in this section for the camera-ready version.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rHi0FbR8udI",
                "writer": "author",
                "reply_to": "Ev9ZYoRkA3m",
                "title": "Response to Reviewer ofer",
                "comment": " Thank you for reviewing our manuscript, and for your encouraging feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> Perhaps some other choices of \\rho_t and R_\\lambda(x) could be explored so that a potential user could understand the sensitivity of the algorithm to these choices\n\nThank you for your suggestion. Indeed, there are many possible choices of the momentum distribution for Hamiltonian dynamics. We chose the Gaussian momentum in the paper because it is the most commonly used in practice, and it enables a simple quasi-refreshment scheme of standardizing the momentum variables. We would like to point out that a discussion on some other possible quasi-refreshment schemes for Gaussian momentum was included in the supplement in Appendix A. In our experiments, we did not observe major differences in performance across the various schemes discussed, and so opted for the simplest one. \n\nAs our work provides a general framework for incorporating momentum refreshments in Hamiltonian-based flow methods, however, it would likely not be too onerous to try out previously-studied alternative Hamiltonian momentum distributions, e.g. the Laplace distribution [1-2]. Developing quasi-refreshment schemes for other momentum distributions is certainly an interesting direction to explore; we leave this to future work. \n\n> I am under the impression that the quasi-refreshment step could also be incorporated into some of the other baselines considered in the experiments, and not just for SHF. If this is the case, how would the baselines then perform?\n\nYou are absolutely correct that we can incorporate the quasi-refreshment step into HIS. In fact, one way to think of our new method is that we (1) replace the tempering step in HIS with quasi-refreshment, and (2) introduce the use of a coreset. Indeed, the quasi-refreshment step is precisely motivated by Proposition 3.2, which states that tempering alone is insufficient for HIS to obtain adequate target approximations, even when they are Gaussian. For UHA specifically, there is no need to incorporate the quasi-refreshment step; the momentum variables are already perfectly refreshed, as they are resampled from a Gaussian (at the cost of introducing many auxiliary variables).\n\n> In Figure 2, relative cov error seems to take a long time to converge, compared to UHA-Full. Why did the purple UHA-Full suddenly go up after ~2000 iterations? It seems like at that point, the approximation is just as good as SHF.\n\nIndeed the relative covariance error seems slow to converge, but this is not the complete picture. We need to look at the relative covariance error plot together with the relative mean error plot (Fig. 2c) and the KL plot (Fig. 2b). In particular, the relative mean and covariance error plots depict two different aspects of the quality of our target approximation; the KL plot takes both of these into consideration. For this particular problem, our method finds the center of the target before fine tuning the covariance. The monotonic downward trend of the KL divergence shows that our method keeps on improving the target approximation throughout optimization. \n\nTo understand why the relative mean error and KL divergence go up for UHA, it is important to note that UHA operates on the augmented space based on a sequence of distributions that bridge some simple initial distribution and the target distribution. Therefore, it is not guaranteed that all steps of optimization improve the quality of approximation on the marginal space of the latent variables of interest. This explains the increase in the error metrics on the \\theta-marginal space shown in Figs. 2b and 2c. However, we note that from Fig 2a, UHA\u2019s augmented ELBO as the optimization objective that we maximize over shows a monotonic increasing trend.\n\nWe realize that we have not been very clear about these interpretations in the paper, and will add some clarification on this in the experiment section for the camera-ready version. Thank you for pointing this out!\n\n[1] Zhang, Y. et al (2016). Laplacian Hamiltonian Monte Carlo. In: Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2016. Lecture Notes in Computer Science(), vol 9851. \n\n[2] Nishimura, A. et al. Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods, Biometrika 107(2), 2020, Pages 365\u2013380.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Ev9ZYoRkA3m",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "Official Review of Paper6521 by Reviewer ofer",
                "comment": " This paper proposes a Bayesian inference methodology incorporating coresets with Hamiltonian flows. The paper demonstrates theoretically the challenges that both coresets and variational inference via Hamiltonian dynamics face, and proposes a fix for both in their algorithm \"Sparse Hamiltonian Flows\". Their method first selects a coreset and then follows a sparsified Hamiltonian flow with quasi-refreshments, which allows the flow to update the momentum. Important or argumentative claims are backed up with theoretical proofs. Experiments on a variety of regression problems demonstrate the superiority of their algorithm over current state-of-the-art coreset compression and variational-flow-based methods. Strengths:\n- Important claims and new insights on coresets and Hamiltonian flows are backed up with theoretical proofs. Where may be difficult to prove in the general case, such as Proposition 3.1, a representative example is given and the claim is proven on it. \n- The proposed idea is very novel and addresses important drawbacks that current coreset methods suffer from.\n- A thorough and clear review of related and past methods is provided\n- Experiments are well-conducted and a variety of datasets, both synthetic and real, are explored\n\nWeaknesses:\n- Perhaps some other choices of $\\rho_t$ and $R_\\lambda(x)$ could be explored so that a potential user could understand the sensitivity of the algorithm to these choices - I am under the impression that the quasi-refreshment step could also be incorporated into some of the other baselines considered in the experiments, and not just for SHF. If this is the case, how would the baselines then perform?\n- In Figure 2, relative cov error seems to take a long time to converge, compared to UHA-Full. Why did the purple UHA-Full suddenly go up after ~2000 iterations? It seems like at that point, the approximation is just as good as SHF. Yes",
                "rating": 9,
                "confidence": 3
            },
            {
                "review_id": "_ZYCQCa8iYX",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "Official Review of Paper6521 by Reviewer pCaB",
                "comment": " The paper introduces a new method for constructing Bayesian coresets. The authors demonstrate that a single uniform subsampling of data points is in principle sufficient to obtain an exact coreset, and introduce the sparse Hamiltonian flow to efficiently construct and sample from the corresponding coreset posterior approximation. Notable improvements over other coreset methods are reported in several experiments.\n ### Originality:\nTo my knowledge, the presented method is original. The authors provide a footnote citing concurrent work based on similar ideas, but the combination of Hamiltonian flow approximations to the coreset posterior seems unique to his work. One could argue that the authors should provide a dedicated Related Work section to elaborate on the connections to earlier work.\n\n### Quality:\nThe paper is technically sound, and the claims are carefully developed and well supported. The paper could be further improved with some reflection on the limitations of the approach.\n\n### Clarity:\nThe manuscript is well structured and very clearly written, with helpful introductions to the methodological ingredients that it builds upon.\n\n### Significance:\nThe paper constitutes a significant contribution within research on Bayesian coresets, both in terms of methodology and measured in terms of the performance improvements over other methods. I am not certain how large a contribution it will have to the field of Bayesian inference in general. This would have been easier to assess if the authors had broadened the scope of their baselines to other Bayesian inference procedures.\n ### Detailed comments and questions:\n\nLine 110. After having introduced Hamiltonian Dynamics, the author state that here that it is possible to use it as the basis of a normalizing flow. It would be informative if they could also explain what the advantages of using a Hamiltonian flow are compared to other choices of normalizing flows. I assume this might be explained in [21,22], but it would be helpful if it was reiterated here.\n\nLine 142. The model covered in Proposition 3.1 is referred to as a \"representative example model\". The authors should elaborate on why this is a representative (or even relevant) example model. Can anything be said about the compression for the posterior of other models? Can anything be said for finite N? In particular, it would be interesting to know whether anything could be said about the choice of M in the general setting.\n\nLine 197. \"In this work we assume that p_t ~ N(mu, Lambda^-1)\"\nDoes this mean that we are ultimately assuming that the posterior can be approximated by a diagonal Gaussian? In that case, how is this different from the assumption made in a simple parametric VI setting? In the introduction, the authors explicitly mention the \u201csimple parametric families\u201d as a contrast to the current work. It would be helpful if the contrast between the two was explicitly explained for the case when this parametric assumption of p_t is made.\n\nLine 266. \"Figs. 2c and 2d demonstrate that this reduction in KL divergence is primarily due to a lower relative error in the approximate posterior mean provided by SHF.\"\nTo which extent is this explained by the fact that the posterior is Gaussian in this case? It seems like an idealized setting for the proposed method, given the assumption of Gaussianity of p_t.\n\nIn the Experiments section, the choice of baselines used in the different figures was not clear to me. Why there was a difference in the baseline methods included in Fig 2/3 and Fig 4?\n\nI would have liked to see simpler Bayesian Inference baselines such as Laplace and simple mean-field VI included as baselines as well. This would make it easier for someone not intimately familiar with coreset methods to judge how big an impact these methods have over conventional approaches.\n\nIn the Conclusion section, it would have been helpful with a discussion of the limitations of the proposed approach.\n\n\n### Minor details\n\nLine 81. \"While theoretically not expensive, interleaving MCMC and gradient descent steps is hard to implement and tune, and is too slow to be practical. \"\nThis sentence was not entirely clear to me. First, you state that it is theoretically not expensive, and that it is hard to implement and tune, but then you conclude that it is too slow to be practical, which seems to contradict the first part of the sentence. Please clarify whether the limitation here is fundamental (e.g. efficiency wise), or whether it is practical (difficult to implement and tune).\n\nLine 149. \"Let C be the universal constant from [37], Corollary 1.2.\nThis constant appears a bit unmotivated here right before Propoposition 3.1. It might increase readibility if it was either explained in greater detail, or otherwise moved to the end of the sentence in 152.\n\nLine 163. \"since w only has the first M entries nonzero\"\nWas the convention of placing the nonzero elements first mentioned anywhere? Perhaps I missed it.\n The authors do not discuss the limitations of their method.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "Kj7xJtGm1s",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "Official Review of Paper6521 by Reviewer hoPP",
                "comment": " \u201cBayesian Inference via Sparse Hamiltonian Flows\u201d combines three techniques to make Bayesian Inference faster and more accurate. It combines a) subsampling of the data (core sets), b) sparse flows and c) quasi-refreshments.\n\nThe paper provides theoretical evidence for why these subcomponents reduce the runtime or increase performance (see section 3) and empirical evidence in three different settings. The Sparse Hamiltonian Flows (SHF) clearly and strongly outperform the alternatives in most experiments. \n\n*UPDATE*: thanks for addressing all of my concerns. I update my score from 7 to 8. Great work. Keep it up!\n In short, I think the paper is good and should be published with minor revisions. \n\n*Strengths:*\n- The paper is very well-written and clear\n- The suggested combination of methods clearly and strongly improves performance compared to the alternatives\n- The paper provides a theoretical analysis of why and in which manner the performance improves due to SHF. \n\n*Weaknesses:*\n- The experiments are all in fairly simple settings. The results already convince me that the method is very strong and warrants publication but a more complex experiment would increase this conviction (see questions). \n- The paper says little about its limitations (see questions).\n - Limitations: To my understanding, SHF makes some assumptions that imply limitations. For example, SHF chooses a random subset of data points. In cases where a random subset of data points is not representative of the entire dataset, SHF might be fast but not useful. Is this correct? Are there other limitations of SHF that are not explicitly mentioned?\n- Scale (related to limitations): I expect that more complicated data types will run into problems with random subsets more easily. Thus SHF might not be an appropriate solution for more complex models and data. I\u2019d like to see a more explicit interaction with these points, either by actually running an additional experiment or by stating the implicit assumptions and following consequences in more detail. \n- Subsets: I\u2019m not sure I fully understand the way in which the random subset is chosen. Is the subset of M values chosen uniformly at random once at the beginning of the process or does the subset change over time? \n- Figure 1: I see that during the leapfrog steps (e.g. every 10th iteration), the ELBO jumps up towards a better state. This is in line with the theory. However, from steps 0-9 the ELBO decreases rather than increases. I don\u2019t understand why that is the case. My naive assumption would be that the blue lines should also go up just not as drastically as the red lines. \n I think, the limitations of the method were discussed insufficiently and should be addressed as described in questions 1 and 2. \n\nI\u2019ll use the rest of the section for high-level comments.\n- In its current form, the paper convinces me that SHF decreases runtime and increases performance for datasets with low complexity. The authors show this with their theoretical analysis and empirical experiments. Furthermore, the paper is well-written and the presentation is good. All of this combined already warrants publication in my opinion. \n- The assumptions that SHF makes and the implied limitations are underexplored. I expect that SHF will have a hard time with more complex models and data because it assumes that a random selection of data points is representative of the entire dataset. I think a good response or an additional experiment in this direction would convince me to raise my score further. Note, that I think the paper would be improved, even if the method is more limited than expected. Stating limitations helps readers and practitioners because it defines the scope of possible use cases more clearly. \n- I want to help where I can. In case something is unclear, feel free to ask follow-up questions. \n",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "train"
    },
    "1vusesyN7E": {
        "paper_id": "nips_2022_1vusesyN7E",
        "paper_title": "Autoregressive Perturbations for Data Poisoning",
        "paper_abstract": "The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data ``unlearnable'' by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison. ",
        "paper_acceptance": "Accept",
        "meta_review": "The paper proposed a novel auto-regressive perturbation method to make the data unlearning. The method is independent to models and data, making it more easy to be used. Reviewers found the idea is novel and intuitively reasonable. The authors responded to reviewers' detailed questions about the method and experiments. The rebuttal succeeded to remove the confusions and convince us about the empirical significance. We suggest the authors improve the paper according to the review comments in the next version. ",
        "meta_review_title": "Meta Review of Paper12810 by Area Chair MmyR",
        "reviews": [
            {
                "review_id": "Id4392fnArt",
                "writer": "official_reviewer",
                "reply_to": "7Et5iM7eKiP",
                "title": "Thank you for the response",
                "comment": " Thank you for the response. I already increased the score to 5 (borderline accept). \n\nWhat the paper proposes is a defense to make the data unlearnable, so developing novel defenses means developing adaptive attacks? In some related literature, designing adaptive attacks is considered necessary to verify the effectiveness of defenses, and lacking adaptive attacks is a weakness or limitation mentioned in many reviews. So the limitation remains. But I do not think it is a big limitation. So I raise the score to 5 since the contributions overweigh the weakness and limitation.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7Et5iM7eKiP",
                "writer": "author",
                "reply_to": "8bm3gkjHUrR",
                "title": "Response to remaining concern",
                "comment": " > although a simple signal is easily interpolated by a network, it may also be easily eliminated by well-designed denoising techniques\n\nThank you for your additional feedback.  Designing denoisers for autoregressive perturbations, which are generated using AR processes unknown to the victim, requires that the denoiser be agnostic to the exact AR process. Even if AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (because we sample our starting signal from a Gaussian for a 32x32x3 image and an AR process that uses a window size 3x3) (Figure 3, Left). Recovering or removing perturbations is a challenging direction for future work, but developing novel defense techniques is beyond the scope of this paper.  \n\nFurthermore, while adding AR perturbations to training data makes the data easy to fit, estimating the noise from poisoned data may be very challenging, perhaps no less challenging than estimating perturbations added under other indiscriminate poisoning attacks, such as error-max perturbations [1].  We emphasize that how easy the noised training data is to fit is **not** related to the difficulty of recovering the clean data after the noise is applied. Error-max and Error-min perturbations are \u201calmost linearly separable\u201d [2] and yet denoisers which remove perturbations under indiscriminate poisoning attacks to recover model performance remain elusive. The possibility of denoisers as a defense can just as easily be raised against other indiscriminate poisoning methods. We do think this is an interesting direction, and defenses are worth pursuing. If we have addressed your feedback, we hope you will consider increasing your score.\n\n[1] Adversarial Examples Make Strong Poisons, NeurIPS 2021\n\n[2] Availability Attacks Create Shortcuts, KDD 2022",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8bm3gkjHUrR",
                "writer": "official_reviewer",
                "reply_to": "06lSfSHO3eU",
                "title": "Thank you for your comprehensive response",
                "comment": " The response addressed most of my concerns. So I raised my rating. \nOne remaining concern is that, although a simple signal is easily interpolated by a network, it may also be easily eliminated by well-designed denoising techniques, e.g., a denoising method that is specifically designed for the AR noise. Although difficult patterns are hard to learn, they may be more robust against denoising methods.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "06lSfSHO3eU",
                "writer": "author",
                "reply_to": "Rkvj0Te2L0l",
                "title": "Following Up with Reviewer wQLZ",
                "comment": " Thank you again for your thoughtful review. Does our response help address your feedback? We would appreciate the opportunity to engage further if needed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Pv2nO2V8knN",
                "writer": "official_reviewer",
                "reply_to": "LH7WFi7pZK1",
                "title": "Thanks for addressing my concerns.",
                "comment": " Thanks for the detailed clarification. All concerns have been addressed.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yTu-DyBbts",
                "writer": "author",
                "reply_to": "yc0AsyS3sjy",
                "title": "Response to Reviewer wQLZ [Part 2]",
                "comment": " > Section 3.3 is not easy to follow\u2026The theoretical analysis is not sufficient. The relation between Lemma 3.1 and the effectiveness of the proposed method in poisoning attacks is not obvious.\n\nWe thank the reviewer for pointing this out. We have rewritten Section 3.3 for clarity, focusing on our logic and its relationship to Lemma 3.1. Here is the relevant portion of the new text: A signal that is easily interpolated by a network will be quickly identified and used as a \u201cshortcut,\u201d whereas complex and unpredictable patterns may not be learned until after a network has already extracted useful content-based features [8]. We propose a simple hypothesis: if there exists a simple CNN that can classify autoregressive signals perfectly, then these signals will be easy to learn. By showing that AR filters exist, Lemma 3.1 helps us define the simple CNN that classifies AR signals perfectly. Our experiments demonstrate that our method, motivated by our simple hypothesis, is effective.\n\n### Responses to questions\n> Is the proposed method only applicable to computer vision tasks?\n\nWe only develop perturbations for images, but an AR perturbation can be crafted for any continuous signal. We speculate that our method could work for audio classification as well. We show that AR patterns are easily learned by CNNs, and they are applicable to any setting where you would use a CNN.\n\nThank you again for your feedback. We think that your suggestions have improved our paper. We made a significant effort to address your questions, and would appreciate it if you would consider raising your score in light of our response. Please let us know if you have any additional questions we can address.\n\n[8] The pitfalls of simplicity bias in neural networks, NeurIPS 2020",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wV2VDIlYnkc",
                "writer": "author",
                "reply_to": "4s6zQaGSY6T",
                "title": "Response to Reviewer FXVx [Part 2]",
                "comment": " > It is better that if actual code can be provided for reproduction of the results\n\nWe have uploaded our code repository as supplementary material to our submission. It contains documentation and example Jupyter notebooks. \n\n### Responses to questions\n> It is clear how to generate AR noise at the beginning inside the sliding window. How about the subsequent steps?\n\nTaking Figure 2.2 as an example, if the sliding window slides one step to the right, there is actually only one value (the next white grid cell) to be computed, $x_t$. Equation 5 is applied independently within every window. Put differently, for every window, the value $x_{t-8}$ is always at the top left corner of the window, the value $x_{t-6}$ is always the top right corner, etc. and $x_{t}$ is always the bottom right corner. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5OtTI6NCsLA",
                "writer": "author",
                "reply_to": "uTTNgZIVubW",
                "title": "Response to Reviewer Jf4X [Part 2]",
                "comment": " ### Responses to questions\n> Are tables 4 and 5 computed over a number of models? \n\nWe have run the results of Table 4 and 5 over a number of models as suggested by the reviewer. We report these below.\n\nFor Table 5 (Adversarial Training), we observe that our proposed method is statistically more effective for perturbation radius $\\rho_a = 0.125$ and $\\rho_a = 0.25$. For every cell in the table, we report mean CIFAR-10 test accuracy over 2 additional models (for a total of 3 models). When considering larger perturbation radii for adversarial training, it is important to recall that adversarial training monotonically (and steeply) decreases test accuracy as the perturbation radius, $\\rho_a$ becomes large [1,2]. Additionally, recent work has shown that adversarial training is an effective defense for several data poisons [3]. Still, our method can better defend against adversarial training at small radii, and is competitive in the case when the radius is large. The results are shown below:\n\n| Attack \\\\ $\\rho_a$  | 0.125           | 0.25            | 0.5             | 0.75            |\n|:----------:|:---------------:|:---------------:|:---------------:|:---------------:|\n| AR (Ours)  | 33.22 &pm; 0.77 | 57.08 &pm; 0.75 | 81.27 &pm; 2.61 | 79.07 &pm; 3.47 |\n| Random     | 86.31 &pm; 0.42 | 84.17 &pm; 0.20 | 80.11 &pm; 0.06 | 76.26 &pm; 0.07 |\n| Regions-4  | 75.05 &pm; 0.35 | 81.23 &pm; 0.11 | 79.71 &pm; 0.05 | 76.47 &pm; 0.34 |\n| Regions-16 | 47.99 &pm; 0.25 | 71.43 &pm; 0.17 | 80.47 &pm; 0.10 | 76.65 &pm; 0.07 |\n| Error-Max  | 33.30 &pm; 0.14 | 72.27 &pm; 2.18 | 81.15 &pm; 3.58 | 78.73 &pm; 4.20 |\n| Error-Min  | 70.66 &pm; 0.41 | 84.80 &pm; 2.38 | 83.04 &pm; 3.24 | 79.11 &pm; 3.46 |\n\n \nFor Table 4 (Mixing Poisons with Clean Data), we run 3 additional models for a total of 4 models. When mixing in clean data we observe our method always leads to a decrease in test accuracy when poisoned data is added. Put another way, in all but one case, it is better to exclude AR poisoned data than to use it for training. AR poisoning also performs better than all the other poisons when a small amount of clean data (5% or 10% clean data) is mixed in. \n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Random            | 86.40 &pm; 1.24 | 86.99 &pm; 0.19 | 84.98 &pm; 1.85 | 78.08 &pm; 0.94 | 70.69 &pm; 0.87 |\n| Regions-4         | 88.94 &pm; 0.85 | 86.75 &pm; 0.86 | 83.52 &pm; 0.20 | 78.23 &pm; 0.97 | 70.19 &pm; 3.16 |\n| Regions-16        | 88.03 &pm; 0.57 | 86.23 &pm; 0.68 | 83.01 &pm; 0.48 | 76.52 &pm; 0.91 | 67.24 &pm; 1.72 |\n| Error-Max         | 87.83 &pm; 0.74 | 86.83 &pm; 0.48 | 84.70 &pm; 0.61 | 81.63 &pm; 0.63 | 76.48 &pm; 1.72 |\n| Error-Min         | 88.32 &pm; 1.57 | 87.23 &pm; 0.84 | 84.56 &pm; 0.88 | 78.76 &pm; 1.83 | 67.82 &pm; 1.92 |\n\n> What is the fundamental difference between the noises in the related literature and the one produced in the paper?\n\nA set of AR perturbations comprise a provably separable set of image vectors, and we use the manual-specification of CNN parameters to specify the function which separates them. Unlike other methods, AR perturbations are not optimized with a surrogate network and are resistant to strong data augmentations.\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019 \\\n[2] Robustness May Be at Odds with Accuracy, ICLR 2019 \\\n[3] Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training, NeurIPS 2021 \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yc0AsyS3sjy",
                "writer": "author",
                "reply_to": "Rkvj0Te2L0l",
                "title": "Response to Reviewer wQLZ [Part 1]",
                "comment": " We would like to thank the reviewer for their thoughtful feedback, and for considering our method \u201cinteresting.\u201d \n### Responses to concerns\n> Is the proposed method only applicable to $\\ell_2$ norm? \n\nWe initially measured the $\\ell_2$-norm because our poisons are not optimized for a specific $\\ell_p$ constraint. AR perturbations may have single entries which are high and violate a strict $\\ell_{\\infty}$ constraint. Still, our proposed perturbations can be projected onto any $\\ell_p$-norm ball, including $\\ell_{\\infty}$. To demonstrate that AR poisoning *can* work in the $\\ell_{\\infty}$-norm constrained setting, we provide CIFAR-10 test accuracy for a RN-18, where perturbations are of size $\\frac{8}{255}$ in $\\ell_{\\infty}$-norm. \n\n|                       | Standard Aug | +Cutout | +CutMix | +Mixup |\n|-----------------------|--------------|---------|---------|--------|\n| AR (Ours)              |     20.49    |  26.93  |  17.08  |  15.22 |\n\nImportantly, how one fits an AR perturbation $\\delta$ within the constraint that $\\lVert \\delta \\rVert_{\\infty} \\leq \\epsilon = \\frac{8}{255}$ affects performance. Here, we simply compute $\\epsilon \\frac{\\delta}{\\lVert \\delta \\rVert_{\\infty}}$, which may be suboptimal, but we haven\u2019t had the chance to explore different options. Clipping values or taking the scaled sign of $\\delta$ would make the perturbation no longer autoregressive. We agree that the addition of this experiment makes our work more complete, so we have included this new table in the Appendix Table 6. \n\n> The proposed attack requires a high poison rate to be effective? \n\nThe goal of our work is to prevent others from using our poisoned data to increase the performance of their models, or to train their models in the first place, as in [4, 5, 6, 7] which also poison a very high fraction or all of the data.  We have updated Table 4 (with standard deviation over 4 independent runs), and we see that, in all but one case, adding our poisoned data to clean data reduces the test accuracy of models, so we can indeed effectively prevent others from leveraging our data.  This goal, namely preventing the addition of poisoned data from boosting accuracy, is standard in the literature [4, 5, 6, 7].\n\n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Difference         | -2.66               | -2.95               | -1.89                | +1.48              | -7.51             |\n\n> In Table 4, the lowest poison rate is 0.6\n\nTaking the reviewer\u2019s suggestion, we have also performed new experiments for when the poison proportion is under 60% (i.e. when clean proportion exceeds 40%). We find that it is better to train a network without AR poisoned data when the clean proportion is 50%, 60%, and even 70%, as desired. The goal is to render the data useless for generalization, while maintaining the content of the image.\n\n|  Poison/Clean Proportion          | 90%            | 80%            | 70%            | 60%            | 50%            |\n|------------|----------------|----------------|----------------|----------------|----------------|\n| Clean        | 91.89 &pm; 0.51 | 91.77 &pm; 0.15 | 91.18 &pm; 0.16 | 91.10 &pm; 0.32 | 90.86 &pm; 0.28 |\n| AR (Ours)  | 92.37 &pm; 0.16 | 91.79 &pm; 0.14 | 91.05 &pm; 0.32 | 90.46 &pm; 0.32 | 89.28 &pm; 0.52 |\n| Random     | 92.68 &pm; 0.39 | 92.08 &pm; 0.42 | 91.94 &pm; 0.22 | 90.58 &pm; 1.22 | 89.78 &pm; 0.57 |\n| Regions-4  | 92.43 &pm; 0.26 | 92.15 &pm; 0.17 | 91.47 &pm; 0.15 | 91.32 &pm; 0.77 | 90.16 &pm; 1.14 |\n| Regions-16 | 92.04 &pm; 0.27 | 91.76 &pm; 0.39 | 91.46 &pm; 0.22 | 90.08 &pm; 1.03 | 89.75 &pm; 0.74 |\n| Error-Max  | 91.26 &pm; 0.23 | 91.18 &pm; 0.47 | 90.68 &pm; 0.83 | 90.12 &pm; 0.50 | 88.76 &pm; 0.70 |\n| Error-Min  | 91.99 &pm; 0.16 | 91.71 &pm; 0.72 | 91.98 &pm; 0.17 | 91.28 &pm; 0.89 | 89.83 &pm; 0.48 |\n\n[4] Unlearnable Examples: Making Personal Data Unexploitable, ICLR 2021 \\\n[5] Adversarial Examples Make Strong Poisons, NeurIPS 2021 \\\n[6] Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder, NeurIPS 2019 \\\n[7] TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications, IEEE Access 2019 \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LH7WFi7pZK1",
                "writer": "author",
                "reply_to": "CnrzvU3QDMJ",
                "title": "Response to Reviewer GeFk",
                "comment": " We thank the reviewer for their time and for indicating that our method is \u201cefficient,\u201d \u201ctechnically sound,\u201d and \u201cvery practical considering real-world applications.\u201d Below, we respond to concerns and then answer posed questions.\n\n### Responses to weaknesses\n\n> Consider that if the parameters for AR are leaked, can it be used to recover the original image?\n\nNo, the leaked parameters are not sufficient to recover the original image. Consider that we start from a random Gaussian starting signal (Figure 2.1) for every image, which is independent from the AR process and not shared, before applying an AR process. This means that even if AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (for a 32x32x3 image with an AR process that uses a window size 3x3). While AR perturbations from the same AR process may look similar, they are unique (Figure 3, Left). \n\n> but adaptive method (if there are any) can be applied to these samples. Or the model trainer could wait for future advancement for the recovery method\n\nWhile there could be potential detection techniques, as the reviewer suggested, developed for different data poisons, removing these AR poisons is not trivial. We agree this would be a topic for future work.\n\n### Responses to questions\n> In experiments section line210: \"We say that poisoning effectiveness drops from setup A to setup B if the network from poison-trained on setup B has higher test set accuracy than the network poison-trained on setup A. \" I find this is confusing.\n\nThis sentence was mainly used to describe \u201cpoisoning effectiveness.\u201d We agree this sentence was confusing, so we have removed it. Other sentences already define what is \u201ceffective\u201d and how to read numbers in the experimental section.\n\n> For experiments in Table 4, for clean only, is it the same subset of data as in mixing poisons/clean?\n\nThe selected clean subset in Table 4 is different, and i.i.d sampled for clean-only training and for each poison. However, we do not believe this impacts the trends we observe. Below, we provide an updated table, where we report results over 4 independent runs, and observe the same trends.\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Random            | 86.40 &pm; 1.24 | 86.99 &pm; 0.19 | 84.98 &pm; 1.85 | 78.08 &pm; 0.94 | 70.69 &pm; 0.87 |\n| Regions-4         | 88.94 &pm; 0.85 | 86.75 &pm; 0.86 | 83.52 &pm; 0.20 | 78.23 &pm; 0.97 | 70.19 &pm; 3.16 |\n| Regions-16        | 88.03 &pm; 0.57 | 86.23 &pm; 0.68 | 83.01 &pm; 0.48 | 76.52 &pm; 0.91 | 67.24 &pm; 1.72 |\n| Error-Max         | 87.83 &pm; 0.74 | 86.83 &pm; 0.48 | 84.70 &pm; 0.61 | 81.63 &pm; 0.63 | 76.48 &pm; 1.72 |\n| Error-Min         | 88.32 &pm; 1.57 | 87.23 &pm; 0.84 | 84.56 &pm; 0.88 | 78.76 &pm; 1.83 | 67.82 &pm; 1.92 |\n\n\nWhen mixing in clean data we observe our method always leads to a decrease in test accuracy when poisoned data is added. Put another way, in all but one case, it is better to exclude AR poisoned data than to use it for training. AR poisoning also performs better than all the other poisons when a small amount of clean data (5% or 10% clean data) is mixed in.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4s6zQaGSY6T",
                "writer": "author",
                "reply_to": "67TlknM7kNa",
                "title": "Response to Reviewer FXVx [Part 1]",
                "comment": " We thank the reviewer for their thorough evaluation and for mentioning that our paper is \u201cnovel\u2026well-written and easy to follow.\u201d \n### Responses to weaknesses\n> When tested against adversarial training, the performance is not satisfactory\n\nWe have updated Table 5 (Adversarial Training) by performing additional runs, and we observe that our proposed method is statistically more effective for perturbation radius $\\rho_a = 0.125$ and $\\rho_a=0.25$. For every cell in the table, we report mean CIFAR-10 test accuracy over 2 additional models (for a total of 3 models). \n\nWhen considering larger perturbation radii for adversarial training, it is important to recall that adversarial training monotonically decreases test accuracy as the perturbation radius, $\\rho_a$ becomes large [1, 2]. Additionally, recent work has shown that adversarial training is an effective defense for several data poisons [3]. Still, our method can better defend against adversarial training at small radii, and is competitive in the case when the radius is large. Our updated Table 5 is below:\n\n| Attack \\\\ $\\rho_a$  | 0.125           | 0.25            | 0.5             | 0.75            |\n|:----------:|:---------------:|:---------------:|:---------------:|:---------------:|\n| AR (Ours)  | 33.22 &pm; 0.77 | 57.08 &pm; 0.75 | 81.27 &pm; 2.61 | 79.07 &pm; 3.47 |\n| Random     | 86.31 &pm; 0.42 | 84.17 &pm; 0.20 | 80.11 &pm; 0.06 | 76.26 &pm; 0.07 |\n| Regions-4  | 75.05 &pm; 0.35 | 81.23 &pm; 0.11 | 79.71 &pm; 0.05 | 76.47 &pm; 0.34 |\n| Regions-16 | 47.99 &pm; 0.25 | 71.43 &pm; 0.17 | 80.47 &pm; 0.10 | 76.65 &pm; 0.07 |\n| Error-Max  | 33.30 &pm; 0.14 | 72.27 &pm; 2.18 | 81.15 &pm; 3.58 | 78.73 &pm; 4.20 |\n| Error-Min  | 70.66 &pm; 0.41 | 84.80 &pm; 2.38 | 83.04 &pm; 3.24 | 79.11 &pm; 3.46 |\n\n> Assuming that all the data can be poisoned is not realistic\n\nThe goal of our work is to prevent others from using our poisoned data to increase the performance of their models, or to train their models in the first place, as in [4, 5, 6, 7] which also poison a very high fraction or all of the data.  In Table 4 we see that, in all but one case, adding our poisoned data to clean data reduces the test accuracy of models, so that we can indeed effectively prevent others from leveraging our data.  This goal, namely preventing the addition of poisoned data from boosting accuracy, is standard in the literature [4, 5, 6, 7].\n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Difference         | -2.66               | -2.95               | -1.89                | +1.48              | -7.51             |\n\nTaking the reviewer\u2019s suggestion, we have also performed new experiments when the poison proportion is under 60% (i.e. when clean proportion exceeds 40%). We find that it is better to train a network without AR poisoned data when the clean proportion is 50%, 60%, and even 70%, as desired. The goal is to render the data useless for generalization, while maintaining the content of the image.\n\n|  Poison/Clean Proportion          | 90%            | 80%            | 70%            | 60%            | 50%            |\n|------------|----------------|----------------|----------------|----------------|----------------|\n| Clean        | 91.89 &pm; 0.51 | 91.77 &pm; 0.15 | 91.18 &pm; 0.16 | 91.10 &pm; 0.32 | 90.86 &pm; 0.28 |\n| AR (Ours)  | 92.37 &pm; 0.16 | 91.79 &pm; 0.14 | 91.05 &pm; 0.32 | 90.46 &pm; 0.32 | 89.28 &pm; 0.52 |\n| Random     | 92.68 &pm; 0.39 | 92.08 &pm; 0.42 | 91.94 &pm; 0.22 | 90.58 &pm; 1.22 | 89.78 &pm; 0.57 |\n| Regions-4  | 92.43 &pm; 0.26 | 92.15 &pm; 0.17 | 91.47 &pm; 0.15 | 91.32 &pm; 0.77 | 90.16 &pm; 1.14 |\n| Regions-16 | 92.04 &pm; 0.27 | 91.76 &pm; 0.39 | 91.46 &pm; 0.22 | 90.08 &pm; 1.03 | 89.75 &pm; 0.74 |\n| Error-Max  | 91.26 &pm; 0.23 | 91.18 &pm; 0.47 | 90.68 &pm; 0.83 | 90.12 &pm; 0.50 | 88.76 &pm; 0.70 |\n| Error-Min  | 91.99 &pm; 0.16 | 91.71 &pm; 0.72 | 91.98 &pm; 0.17 | 91.28 &pm; 0.89 | 89.83 &pm; 0.48 |\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019 \\\n[2] Robustness May Be at Odds with Accuracy, ICLR 2019 \\\n[3] Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training, NeurIPS 2021 \\\n[4] Unlearnable Examples: Making Personal Data Unexploitable, ICLR 2021 \\\n[5] Adversarial Examples Make Strong Poisons, NeurIPS 2021 \\\n[6] Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder, NeurIPS 2019 \\\n[7] TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications, IEEE Access 2019 \\\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uTTNgZIVubW",
                "writer": "author",
                "reply_to": "9y3pjgOA3cN",
                "title": "Response to Reviewer Jf4X [Part 1]",
                "comment": " We thank the reviewer for their feedback, for asking questions which will improve our work, and for referring to our method as \u201cfascinating.\u201d Below, we respond to concerns and then answer posed questions.\n\n### Responses to weaknesses\n> Unclear how much of the evaluation is an artifact of chosen optimisation hyperparameters\n\nTo test the effect of optimizer and learning rate, we have run additional experiments to confirm the effectiveness of our method. Specifically, we consider 3 optimizers: SGD, SGD+Momentum ($\\beta=0.9$ and Adam ($\\beta_1=0.9$, $\\beta_2=0.999$). We also consider 3 different learning-rate schedules: cosine learning-rate, single-step decay (epoch 50) and multi-step decay (epoch 50, epoch 75) with decay factor of 0.1. Thus, there are a total of 9 optimizer and learning rate combinations. In the following table, for each poison, we report mean CIFAR-10 test accuracy over the 9 combinations of optimizers and learning rates. Our proposed method remains nearly unaffected by choice of hyperparameters, and performs better than all other poisons.\n\n| AR (Ours)      | Random          | Regions-4     | Regions-16     | Error-Max    | Error-Min       |\n|----------------|-----------------|---------------|----------------|--------------|-----------------|\n| 12.23 &pm; 1.22 | 33.39 &pm; 31.24 | 22.89 &pm; 7.6 | 18.73 &pm; 5.52 | 16.6 &pm; 2.8 | 22.96 &pm;  7.16 |\n\n> Unclear how one compares performance of different correlated noises\n\nAll the poisons we consider use the ground truth label as a dependent variable to generate the perturbations. Thus, poisons we consider contain perturbations that may be correlated with the ground truth. While there is an intractable number of possible noise patterns that could be used to perturb data and induce a correlation, we prove that AR perturbations have a particular structure which allows a manually-specified CNN filter to detect it perfectly (Lemma 3.1). Thus, AR perturbations are correlated noise designed for convolutional layers. By making sure that poisons we consider are bounded by the same $\\ell_2$ perceptibility constraint, we are able to compare performance between different kinds of correlated noise.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9y3pjgOA3cN",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "Official Review of Paper12810 by Reviewer Jf4X",
                "comment": " Paper considers a setting of `unlearnable examples' where a given dataset is perturbed in a way to make it hard to learn the true task. In essence, data here gets perturbed with correlated noise such that when learning is attempted, models learn to focus on the noise rather than on the true features useful for generalisation. While prior work focused on generating class-wise poisons, in this work the noise is generated per sample using a Markov process, producing linearly separatable noise. Paper thoroughly evaluates the setting and demonstrates that the approach effectively stops generalisation when the whole dataset is poisoned and struggles in a similar way in presence of adversrail training or dilution with clean data.  \nStrengths:\n+ Interesting setting\n+ Idea of hardness of learning is rather fascinating\n\nWeaknesses:\n+ Unclear how much of the evaluation is an artifact of chosen optimisation hyperparameters\n+ Unclear how one compares performance of different correlated noises Thank you very much for the paper, it is a very interesting read! I only have a handful of questions:\n\n1. Are tables 4 and 5 computed over a number of models? Given how close the numbers are, it would be great to know if the differences are observed on distributional level, not just per model\n2. Given that we can produce arbitrary correlated noise of different flavors, how should one think about it? What is the fundamental difference between the noises in the related literature and the one produced in the paper? This naturally leads to my final question.\n3. Given the argument of easier learnability of different noises, it turns the question to how much observed behaviour is an artifact of the optimisation procedure itself? Did you try running the experiments with different lr/optimiser options? \n\nMinor:\n* Punctuation missing around eqs in some places N/a",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "67TlknM7kNa",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "Official Review of Paper12810 by Reviewer FXVx",
                "comment": " This paper proposes a new data poisoning attack to prevent data scraping. The proposed method adds class conditional autoregressive (AR) noise to training data to prevent people from using the data for training, and the method is data and model independent, which means that the same noise can be used to poison different datasets and models of different architectures.\n\nThe intuition behind the idea is that easy to learn noise is more effective at data poisoning, and AR noise generated in the proposed way is easy for neural network to learn. The authors show that a manually specified 3-layer CNN with AR filter can easily learn class information from the AR noise. Experiments on four benchmark datasets (CIFAR10, STL10, SVHN, CIFAR100) show that the proposed method performs better than other four baselines (Error-min, Error-max, Regions, Random noise). Strengths:\n\n- The proposed method is novel as autoregressive process hasn't been used before to do data poisoning. The method is easy to implement and the same AR coefficients can be used for different datasets and architectures as long as the numbers of classes are the same. Though code is not available, pseudo code (algorithms) and implementation details are provided. It is better that if actual code can be provided for reproduction of the results.\n\n- The paper is well-written and easy to follow. Empirical results on four different datasets show that the method performs better than other baselines, both under normal setting and defense settings.\n\nWeakness:\n\n- Though the proposed method performs better than other baselines compared in the paper, when tested against adversarial training, the performance is not satisfactory. It performs similarly to other baselines under this setting and the poisoning effect is not good, especially when the radii is large.\n\n- As pointed out in the paper, assuming that all the data can be poisoned is not realistic. In section 4.3.3, the poisoning methods are evaluated using a mix of poisoned and clean data. Under this setting, the performance of the proposed method is not good and similar to those of other baselines.\n\n About the process of AR noise generation:\n\n- It is clear how to generate AR noise at the beginning inside the sliding window. How about the subsequent steps? Take the example in Figure 2 as an example, if the sliding window slides one step to the right, there are three values to be generated. Are $x_{t-7}$ up to $x_t$ used to generate the next one ($x_{t+1}$)? Then $x_{t-6}$ up to $x_{t+1}$ are used to generate $x_{t+2}$, and so on.  The author points out that the method does not perform well against adversarial training and experiments show that when evaluated using a mix of poisoned and clean data, the performance is also not good.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "CnrzvU3QDMJ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "Official Review of Paper12810 by Reviewer GeFk",
                "comment": " This paper proposed autoregressive poisoning techniques to protect data from being exploited by unauthorized machine learning models. The proposed method does not rely on optimizations while generic towards different model architectures and different datasets. This paper also provides insight into why the proposed method is effective. \n Strengths:\n- The proposed method is efficient and technically sound. Existing works rely on optimizations which is the bottleneck. The proposed method does not rely on optimizations, and the parameters for AR are easy to find.  \n- The existing works are also shown that do not transfer well between model architectures or datasets. Experiment results show that one set of AR is generic across different architectures or datasets. \n- The efficient and generic can be very practical considering real-world applications.\n- Experiment results also demonstrated AR generated unlearnable examples are more robust towards augmentations.\n\n---\nLimitations:\n- Once the data is released, the defender may not modifies the data anymore, and the model trainer can retroactively apply new models/methods [1]. The adaptive case should be carefully examined. Consider that if the parameters for AR are leaked, can it be used to recover the original image? Or if a portion of the clean images are leaked, using pair of clean and unlearnable versions, is it possible to reverse the AR process? \n- In section 3.3, the assertion that the noises are easy to learn is more effective for poisoning, this could also mean they are easy to detect. Such as calculating sample-specific loss at the end of each training epoch. Although only detecting such samples does not make them \"learnable,\" but adaptive method (if there are any) can be applied to these samples. Or the model trainer could wait for future advancement for the recovery method as mentioned in [1]. \n\n[1] Data Poisoning Won\u2019t Save You From Facial Recognition, ICML 2021 Workshop AML\n\n---\nAfter the author's response, I increased my rating score to 7. My main concerns over possible reverse operation if parameters are leaked have been well addressed. \n - In experiments section line210:  \"We say that poisoning effectiveness drops from setup A to setup B if the network from poison-trained on setup B has higher test set accuracy than the network poison-trained on setup A. \" I find this is confusing.\n- For experiments in Table 4, for clean only, is it the same subset of data as in mixing poisons/clean?  Please address the potential limitations in the Strengths And Weaknesses section. ",
                "rating": 7,
                "confidence": 5
            },
            {
                "review_id": "Rkvj0Te2L0l",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "Official Review of Paper12810 by Reviewer wQLZ",
                "comment": " This paper proposes to use autoregressive processes to generate perturbations for data poisoning. The generated perturbations, despite looking complex, are actually very simple. One advantage of the proposed method is that its generated perturbations are dataset and architecture independent. The paper evaluates the proposed method on multiple datasets and networks, showing the effectiveness of the perturbations when the poison rate is high. The strengths of this paper include\n1. The proposed attack method is interesting.\n2. The proposed method has good transferability.\n\nBut I still have the following concerns:\n\n1. Is the proposed method only applicable to $\\ell_2$ norm? The paper uses the sentence \"We measure AR perturbations\nin $\\ell_2$ because measuring in $\\ell_\\infty$ would underestimate the extent to which these perturbations are less perceptible than purely  $\\ell_\\infty$ random noise\" to explain why it uses $\\ell_2$ norm. But this sentence is hard to follow, and this short explanation is not convincing. The paper should provide more clear and convincing explanation about why it *only* uses $\\ell_2$ norm.\n\n2. The proposed attack requires high poison rate to be effective? In most experiments, the paper uses poison rate 1. In Table, the lowest poison rate is 0.6. The assumption of high poison rate is very strong. In practice, if the data is collected from multiple sources, then the attack is not effective? In the case that the data is collected from one source (the adversary), the entity who trains the model would be more cautious about the quality of data due to the high risk when the data only comes from one source.\n\n3. Section 3.3 is not easy to follow, and the logic is not very clear. I think Section 3.3 is one of the most important parts in the paper since it explains why the proposed method works. After reading Section 3.3, I am still very confused. The relation between Lemma 3.1 and the effectiveness of the proposed method in *poisoning attacks* is not obvious. \n 1. Is the proposed method only applicable to $\\ell_2$ norm?\n2. The proposed attack requires high poison rate to be effective?\n3. Is the proposed method only applicable to computer vision tasks? 1. The paper only studies $\\ell_2$ norm.\n2. The poison rate is high. The lowest poison rate studied in the paper is 0.6.\n3. The theoretical analysis is not sufficient. The relation between Lemma 3.1 and the effectiveness of the proposed method in poisoning attacks is not obvious.\n",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "6UtOXn1LwNE": {
        "paper_id": "nips_2022_6UtOXn1LwNE",
        "paper_title": "Models of human preference for learning reward functions",
        "paper_abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments. These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling preferences instead as arising from a different statistic: each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences. We also prove that the previous partial return model lacks this identifiability property without preference noise that reveals rewards' relative proportions, and we empirically show that our proposed regret preference model outperforms it with finite training data in otherwise the same setting. Additionally, our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research.",
        "paper_acceptance": "Reject",
        "meta_review": "The submitted paper was reviewed by 4 knowledgable reviewers and the reviewers and authors enganged in intense discussions. The authors clarified many details in these discussion but could not convince the reviewers in all regards (there are still open concerns regardings the proofs and the update proofs came in rather late so that there was insufficient time for the reviewers to further interact; there concerns regarding experiments although I discounted most of those regarding to scalability as I agree with the authors in that regard to some extent; etc.). Moreover, looking at the discussions and the authors' responses, the paper would benefit from making several points more clear/improving their presentation, likely by including parts which came up in the discussions in the paper. Considering all this, I think this paper should go through another round of reviews before it should be accepted and I am recommending rejection of the paper. Please note that it was not easy to come to this decision - there are some important insights and experiments in the paper which should be made available to the community asap. Thus I would honestly encourage the authors to improve their paper considering the reviewers' comments and take-aways from the discussion and submit a revised version of the paper at one of the upcoming conferences. I am already looking forward to seeing an improved version of the paper being published.",
        "meta_review_title": "Meta Review of Paper10488 by Area Chair 6fyf",
        "reviews": [
            {
                "review_id": "Q3R-cxcQoke",
                "writer": "author",
                "reply_to": "BJ2Tzrn6jJc",
                "title": "Considering our contributions",
                "comment": " Regarding that we only use simple grid worlds (though up to 100 random versions of them), we can only make subjective arguments about the necessity of scalability in addition to our various contributions, which we understand may not persuade you, especially if your mind is already made. \n\nIn our responses here, we have argued that the existence of deep IRL work\u2014noting the inner-loop bottleneck\u2014provides optimism that similar approaches can be applied to learning reward functions with regret-based preference models; without such optimism, the importance of demonstrating scalability in this paper would increase. \n\nAlso, consider that **there are 100s of papers on IRL and there will only be _this one paper_ on reward learning from regret-based preferences, if you and our other reviewers decide it is worthy of publication. The first paper on IRL, in 2000 by Ng and Russell, also only focused on simple problems: two grid worlds and two versions of mountain car. We nonetheless share your strong interest in seeing regret-based reward learning scale to complex, real-world problems**. \n\nBut we worry that it would be a disservice to progress in this popular form of learning reward functions to make showing scalability a hard constraint for publishing this _first_ paper, which we believe **provides urgently needed insight about the partial return preference model that continues to pervade new publications on learning reward functions from pairwise segment preferences, likely _including most that are currently in progress_ by other researchers.** If one instead considers whether the contributions we did make\u2014a human study with empirical results, new preference model, theoretical results, synthetic preferences results, a new learning algorithm that approximates regret, and a clear existence proof that the previously overlooked preference model matters\u2014are together sufficient for publication in NeurIPS, our contributions become a strength of this submission.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iIZ-k4os-kp",
                "writer": "author",
                "reply_to": "BJ2Tzrn6jJc",
                "title": "Correcting our error",
                "comment": " **We ask you to reconsider your score, given that you wrote that our original response _resolved several of your concerns_ and also depending on how well our responses below and above (in \"Considering our contributions\") satisfy your concerns about our lack of focus on scalability and our theoretical results.**\n\n**For @xps1 and @SeBN:** Thanks for further clarification about your concerns regarding the comparability of Thms 3.1 and 3.2. In our first response to your reviews, we focused in part on how the choice of optimization (max likelihood or otherwise) doesn't matter in the Thm 3.2 proofs, since noiselessly providing preferences based on partial return creates a many-to-one mapping. \n\n**Your point above about the proof of Thm 3.1 not applying to noiseless preferences is well taken, and we agree with you. Thank you for identifying this error.** We originally removed the temperature parameter from the softmax formula because it is redundant with the scaling of the reward and it simplified our notation to not include it. However, as you suggest, for the proof of Thm 3.1 to apply to the noiseless preferences setting of Thm 3.2, a revision is needed. \n\nFortunately, **the required change is minor**. **We can add back the temperature as a optimizable parameter into Eqs 2 and 5 and allow a special case for temperature = 0, where the result is a hard max, i.e., if temperature = 0, the preference is given deterministically to the segment with the higher partial return in Eq 2 or regret in Eq 5.**\n\nWith this tweak, when the temperature is 0, the proof of Thm 3.1 covers both the stochastic and noiseless settings, and the preference generator is realizable in the noiseless setting for Thm 3.1 and in the two proofs of Thm 3.2 (though these two proofs don't rely on the learning algorithm to provide their negative results). In other words, **Thm 3.1 does include the noiseless settings of Thm 3.2 as a special case, but both theorems' proofs required this \"tweak\" to have a learning algorithm that can realize the reward function that created the noiseless preferences.**\n\nThe need for this added special case in the theoretical setting is now clear. However, in practical settings, the originally submitted algorithm (with no temperature) will simply scale reward until the improvement in loss after each epoch of learning is extremely small, which can be used as a stopping criterion, at which point the reward function results in nearly deterministic preferences. Such an approach is effective with noiseless preference labels in our experiments, as seen in Section 6.2 (with further detail in App F.2.1) and in the randomly generated _stochastic_ MDPs in App F.2.5.\n\nThe revisions and clarifications above will be included in the final copy. In particular, we will revise to emphasize that _in noiseless preference label settings_, both theorems provide insight about whether the preferences contain the information required to recover the set of optimal policies (via directly recovering a reward function). **In other words, the theorems are really about each preference model _as a preference generator_, and the learning algorithm used in Thm 3.1 is merely meant to show _what information exists_ in each type of preferences.** For the negative result of Thm 3.2, its two proofs clearly show that some MDPs exist in which preferences based on partial return have the many-to-one mapping we referred to and therefore is not identifiable under _any_ learning algorithm. For the positive result of Thm 3.1, we merely need to show that there is one learning algorithm that permits identifiability in any MDP, which that proof does with the above tweak to allow a special hard max case.\n\n(Note that what we wrote above was summarized and rephrased in our response to @xps1 with the same subject, \"Correcting our error\".)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "02hkRIywWB",
                "writer": "author",
                "reply_to": "3KddY4BdXaE",
                "title": "Correcting our error",
                "comment": " > I\u2019m not sure I understand this. My concern with this section was not related to the theoretical significance of noiseless settings (by the way, I do think it\u2019s important). I don\u2019t think SeBN suggested this either, so can you clarify what you mean by this?\n\n**We misinterpreted your statement** \"... not convinced that Theorem 3.2 is all that significant\" to be about the noiseless setting, but, assuming we now understand you better, your issue was more specifically with the comparability of 3.1 and 3.2 and that a reward function providing noiseless partial-return-based preferences is not realizable under the algorithm we use in Sec 2 and Definition 1 (and is the common algorithmic component of [9-16]). **We appreciate your added clarification and that your issues do not regard the importance of the setting of Thm 3.2 but rather some technical aspects, since that means we agree that the theoretical problem is impactful and can focus on the solution.**\n\n> In any case, I don\u2019t think either of our points have been addressed here. I'm skeptical of the claimed difference between the regret model and the return model that is presented in Section 3 because it does not appear to be a fair comparison. As SeBN put it quite well: 3.1 and 3.2 do not satisfy the same premise. The noiseless setting falls outside the problem setting established in Eq 2 and 5 where a logistic model is used - it\u2019s misspecified for this model class for finite rewards. Meanwhile, Theorem 3.1 is using this realizability to derive the positive result. This is used in the first line of the proof. I think the comparison would be fair if either the negative result of Theorem 3.2 were strengthened to the case where preference function is realizable but it still fails to be identifiable, or the positive result of Theorem 3.1 were shown to extend to a non-trivial class of problems where there is misspecification.\n\n**We agree with your points above and thank you and @SeBN for pushing back to reveal this error.** In our response to you and @SeBN that is in @SeBN's thread, we share **a small revision to solve the realizability issue** for noiseless preferences. The issue you two identified actually affects both Thm 3.1 (in the noiseless subset of its settings) and Thm 3.2, so the fix for realizability should be applied to both. We also want to emphasize that in our final copy, we will clarify that **the theorems focus on what information such infinite datasets hold when either preference model acts as the preference generator, where any usage of a learning algorithm in the proof Thm 3.1 only serves to show that the information exists in the dataset to recover an equivalent reward function**.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sZTn5_QoKRC",
                "writer": "author",
                "reply_to": "SxGr_BLFMl1",
                "title": "Resolving remaining clarification questions",
                "comment": " > Thank you. Figure 1 seems to be a safe RL example. That is why I\u2019m a little confused about the motivation.\n\nThank you for explaining your confusion. Although Figure 1 may indeed remind readers of research on safe RL (and safety in autonomous driving, which employs a different definition of the word \"safe\"), here Figure 1 is used as a simple example of how human preferences intuitively do not seem to given based on the partial return of a trajectory (or else the left trajectory would be preferred). We hope that this exchange has helped clarify that this work is unrelated to safe RL; it solely focuses on more accurate inference of the reward functions that drive human preferences. Any RL algorithm can then be used to learn a task from such a human-aligned reward function. If an RL practitioner decided to add safety constraints to such a learned reward function (which however is certainly not the focus of this paper), they could then use various safe RL algorithms to learn from the reward function\u2014created through regret preference models\u2014such as CPO.\n\n> Sorry for the unclear part. In the experiments, there are baselines named as Partial return (noiseless) and Partial return (stochastic). Are the two baselines from [9-16] or created by the paper? There are a lot of partial return methods. Why not compare with the methods from those papers?\n\nUnderstood. Thanks for communicating back and forth with us to get to this point of clarification! **The partial return (stochastic) preference model is taken exactly from [9-16] and is common to all of them. And the \"partial return (stochastic)\" reward learning algorithm\u2014created from joining both the preference model (Eq 2) and maximizing likelihood (Eq 2)\u2014is the common part of each learning algorithm from [9-16].** L96-97 of the currently downloadable version (lines unchanged from the original submission) were meant to state this relationship. We will be more explicit in the final version to prevent confusion like yours. Comparing preference models, which was not the focus of _their_ papers, is our focus. Instead, they focused on largely orthogonal algorithmic elements, like what segment pairs to present for elicitation [13] or how to maximize likelihood (Eq 1) with a deep neural network representation of the reward function [9]. So this baseline appears to be precisely what you are asking for. \n\nThe partial return (noiseless) preference model is _only_ used to generate preferences, not for the reward learning algorithm, as we explain at the start of Section 6.2 (L327\u2013331 in the currently downloadable version). This noiseless preference generator is not typically used by [9-16]. You can see in our comments to all reviewers an explanation of the importance of these noiseless versions of each preference model, despite that they are not typically employed in related work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJ2Tzrn6jJc",
                "writer": "official_reviewer",
                "reply_to": "2N6eDn4tzA",
                "title": "Thank you for the response",
                "comment": " I would like to thank the author for the response. It indeed resolves several concerns of mine.\n\nUnfortunately, my concern regarding the main theoretical contribution (Theorem 3.1 and 3.2) of the work remains.\nI am not convinced by the argument of the author that \"3.1 shows that regret is identifiable under the conditions of 3.2 as the setting of 3.2 is a special case of 3.1\". For example, as reviewer xps1 points out in his response, why does the first line of the proof of 3.1 hold if we change to the setting of 3.2 (can minimizing the cross-entropy even work in the setting of 3.2)? Maybe totally re-written proofs of the 2 theorems with a consistent setting are more persuasive. Furthermore, if the noiseless setting is not practical, the common setting should be the one from the logistic observation model in Equation 2 (the setting in 3.1).\n\nBesides, in my view, saying that this work does not focus on scalability to justify the experiment involves only a toy grid world environment is not convincing. While different in the inputs, the solution in this work and IRL have a similar computation bottleneck of an inner loop of policy optimization. However, works on IRL (e.g., the popular Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization by Chelsea Finn 2016) have impressive experimental results. This IRL work has been also cited in many works in the last 5 years.\n\nTherefore, I decide the keep my score.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SxGr_BLFMl1",
                "writer": "official_reviewer",
                "reply_to": "EczQRsbt7Tg",
                "title": "Feedback again",
                "comment": " >Regarding safe RL, this paper focuses on \n\nThank you. Figure 1 seems to be a safe RL example. That is why I\u2019m a little confused about the motivation. \n\n> about partial return and [9-16]\n\nSorry for the unclear part. In the experiments, there are baselines named as Partial return (noiseless) and Partial return (stochastic). Are the two baselines from [9-16] or created by the paper? There are a lot of partial return methods. Why not compare with the methods from those papers? \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3KddY4BdXaE",
                "writer": "official_reviewer",
                "reply_to": "eXpYT0v_GV",
                "title": "Thanks for the response",
                "comment": " Thanks for your response. I appreciate the details and clarifications.\n\n> Noiseless preferences are not theoretically interesting or should be connected to past work. (@xps1, @SeBN)\n\nI\u2019m not sure I understand this. My concern with this section was not related to the theoretical significance of noiseless settings (by the way, I do think it\u2019s important). I don\u2019t think SeBN suggested this either, so can you clarify what you mean by this?\n\n\nIn any case, I don\u2019t think either of our points have been addressed here. I'm skeptical of the claimed difference between the regret model and the return model that is presented in Section 3 because it does not appear to be a fair comparison. As SeBN put it quite well: 3.1 and 3.2 do not satisfy the same premise. The noiseless setting falls outside the problem setting established in Eq 2 and 5 where a logistic model is used - it\u2019s misspecified for this model class for finite rewards. Meanwhile, Theorem 3.1 is using this realizability to derive the positive result. This is used in the first line of the proof. I think the comparison would be fair if either the negative result of Theorem 3.2 were strengthened to the case where preference function is realizable but it still fails to be identifiable, or the positive result of Theorem 3.1 were shown to extend to a non-trivial class of problems where there is misspecification.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EczQRsbt7Tg",
                "writer": "author",
                "reply_to": "HdiHOg3ZLK",
                "title": "To improve our clarity further, some explanations and questions",
                "comment": " We appreciate you following up on these questions and topics. We're happy to discuss further and hope to fully understand your confusion and preempt similar confusion for future readers through revisions for clarity.\n\nRegarding **safe RL**, this paper focuses on the problem of how to _learn a reward function_ that is aligned with the interests of human stakeholders, specifically focused on the well-established technique of learning from preferences over pairs of trajectory segments. To our knowledge, safe RL such as CPO, on the other hand, generally assumes that a reward function and a set of constraints _are already given_ and focuses on RL algorithms that do policy improvement while making various guarantees to avoid violating the safety constraints. In short, reward learning and safe RL are different problems that do not seem particularly related. We do see how a different usage of the word \"safety\" could encompass the alignment of reward functions\u2014since poorly aligned objective functions generally are a significant societal threat\u2014despite the safe RL literature not focusing on such alignment.\n\nWe do not understand your question about partial return and [9-16]. Could you elaborate on what you mean and why more than one implementation would be desirable?\n\nRegarding **Figure 1**, we will add some more explanation below to see if any of it addresses your confusion. We implicitly assume a reward function that\u2014in these two segments\u2014only deviates from 0 to reflect damage to the vehicle or harm to the passengers. If it would help to make that information more explicit, please let us know and we will add it. The left segment involves no damage to the vehicle and therefore has 0 reward, even though it leads the car to an end state in which the car will almost certainly have a horrible collision within seconds. The right segment involves minor damage but otherwise does roughly the reverse of the left segment: it takes the car from a start state in which collision will likely happen soon to a safe end state. (Additionally, are you confused about how the figure is communicating the start states and end states, which are simply the beginning and end of the arrows / where the two images of the car are? If so, we can also write that information more clearly.) So by looking at partial return alone (i.e., the sum of reward along the segment but _not_ the start or end state values), the left segment is better despite being suboptimal, but human intuition prefers the right (optimal) segment, and the right segment has a lower deterministic regret (Eq. 3) because deterministic regret does include the start and end state values.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HdiHOg3ZLK",
                "writer": "official_reviewer",
                "reply_to": "KS33hv1S1j",
                "title": "Thanks and some clarification",
                "comment": " I thank the authors for their responses. However, I notice that some of my comments were not completely addressed. \n\nDo you mean partial turn indicates [9-16] in the experiments? Why just one implementation? \nDo you need to compare with some safety rl methods, like CPO?\n\nFigure 1 does not seem to help improve understanding of the idea. It would be better to provide more explanation about Figure 1 or change an example. Why not add some related work about safety RL? \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2N6eDn4tzA",
                "writer": "author",
                "reply_to": "5Eg4LFVc94B",
                "title": "Response to reviewer, part 2",
                "comment": " **Please compare to inverse reinforcement learning (IRL) methods.**\n\nThe inputs to IRL and learning reward functions from pairwise preferences (which appears to be the second most popular method of reward learning after IRL) are different. IRL requires demonstrations, not preferences over segment pairs. However, because a  a regret-based preference model always prefers optimal segments over suboptimal segments, at least one connection can be made. If one assumes that a demonstrated trajectory segment is noiselessly optimal (as in the foundational 2004 Abbeel and Ng IRL / apprenticeship learning paper), then such a demonstration is equivalent to expressing preference or indifference for the demonstrated segment over all other segments (or, equivalently, that no other segment is preferred over the demonstrated segment). However, IRL has its own identifiability issues in noiseless settings (see [http://proceedings.mlr.press/v139/kim21c.html](http://proceedings.mlr.press/v139/kim21c.html)) that, viewed from the lens of preferences, come in part from the \"indifference\" part of the above statement: since there can be multiple optimal actions from a single state, it's not generally correct to assume that a demonstration of one such action shows a preference over all others, and therefore it remains unclear in IRL what _other _actions are optimal. Note that since partial-return-based preferences can prefer suboptimal segments over optimal segments, the common assumption in IRL that demonstrations are optimal does not map as cleanly to partial-return-based preferences.\n\nAs mentioned in our response to all reviewers under \"Practically using regret is challenging\", our regret preference model also relates to IRL in that many IRL algorithms require solving an MDP in the inner loop, like would be required for a perfect measure of regret while learning a reward function.\n\nAn empirical comparison of IRL and learning reward functions from pairwise preferences would be valuable but is out of our scope\u2014we are focused on an established technique that is not IRL, and its close relationship to IRL does not mandate that any research project test IRL algorithms too\u2014and would be its own full research project. Further, how to do an apples-to-apples comparison is non-obvious, since the inputs are different.\n\nWe have added a discussion of the relationship to IRL in App B.5.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5Eg4LFVc94B",
                "writer": "author",
                "reply_to": "DtNlCuPRJy",
                "title": "Response to reviewer, part 1",
                "comment": " Thank you for your thoughtful review. We appreciate the opportunity to clarify certain technical details, the relationship of this work to inverse reinforcement learning, why noiseless preferences are informative, and why scalability is not our focus.\n\n**Would defining reward as r(s,a) or r(s) affect the results in the paper?**\n\nIt would not. Our definition of reward is a generalization of reward defined over state or state-action pairs, which can be trivially mapped to reward over tuples of state, action, and next state. E.g., r(s,a) is mapped to an r(s,a,s') function simply ignoring the next state, s'.\n\n**\"the proposed regret model is limited to the case of deterministic policies\"**\n\nThis is a misunderstanding. The regret model applies to stochastic policies as well. Our definition of a policy in L68-69 is in stochastic terms. Further, note that the algorithm relies on optimal advantage values; any state-action pair that has support (i.e., non-zero probability) in an optimal policy has an advantage of 0. That's true even if there is more than one action that is optimal from a state. And a policy that stochastically mixes multiple optimal actions from each state will also always create state-action pairs with an advantage of 0 (otherwise it would be sub-optimal). Additionally, please see our response to Reviewer ieTk under \"What if there is a set of optimal policies\u2026\", which is relevant because any MDP with two optimal deterministic policies necessarily has infinite optimal stochastic policies (via mixing them as mentioned above).\n\n**\"if I am not missing anything, Theorem 3.1 and 3.2 are not comparable... in Theorem 3.1, if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})>0.5$, in Theorem 3.2, if $\\Sigma_{\\tilde{r}}(\\sigma_{1}) > \\Sigma_{\\tilde{r}}({\\sigma_{2}})$, $P_{\\Sigma_r}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})=1$.\"**\n\nPlease see our response to all reviewers under \"Noiseless preferences are not theoretically interesting or should be connected to past work\". Building on that general response, note that they are very comparable in the sense that the setting that matches that of 3.2 is a special case of 3.1. In other words,  \"if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})>0.5$\" includes all cases where \"if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})=1$\". Therefore, 3.1 shows that regret is identifiable under the conditions of 3.2, if you swap partial return for regret and flip the > sign, as well as under other conditions.\n\n**\"it raises the question of whether P\u03a3r is indeed non-identifiable if Theorem 3.2 has the same premise as Theorem 3.1.\"**\n\nWe agree and were careful to only claim that partial return is only non-identifiable in this noiseless setting. See L11 (\"without preference noise...\"), L47 (\"without preference noise...\"), L167 (\"without the distribution over preferences providing information\"). We suspect any scale-invariant regression model is identifiable with Boltzmann noise, so both models would be identifiable in that respect.\n\n**\"the work needs to illustrate whether the proposed method is scalable to more practical problems, e.g., by including experiments with neural networks and larger state/action spaces\"**\n\nWe understand that some researchers value showing deep RL applicability of a new approach more than others. However, we don't claim scalability either as a contribution or a characteristic of regret-based preference models and openly discuss how it might be addressed in the latter portion of Appendix F.1. We consider scalability to be one of many important dimensions of engineering and experimentation, but it did not make the cut in this paper for reasons we detail in our general message to all reviewers, under \"This paper does not demonstrate scalability to more complex environments.\"\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eXpYT0v_GV",
                "writer": "author",
                "reply_to": "p-z5ilzAs5",
                "title": "Response to reviewer, part 2",
                "comment": " **\"The description of the algorithm is very confusing\"** // **\"What are the input rewards and policies? Why can they be empty sometimes? Why would line 4 already estimate those? What procedure is being done to estimate these? How many feature functions are necessary?\"**\n\nWe trust that the above explanations help somewhat with your confusion regarding Algorithm 1. Further, without familiarity with doing general policy iteration (GPI) with successor features (SFs) (e.g., via reading Barreto et al. [24]), the description of Algorithm 1 will necessarily seem very complex, but it's a straightforward application of this concept to approximate Q and state values for arbitrary reward functions. Successor features are a potent tool with applications across RL.\n\nRegarding your specific questions, SF functions are learned _per policy_. So an input set of policies gives us a set of SF functions. We can also use a set of reward functions to create or augment this set of policies, by adding an approximately optimal policy for each reward function. To end up with a set of policies to create SF functions with, we can therefore use either or both of an input set of policies and reward functions. Crucially, these reward functions are _not_ the learned reward function that is the algorithm's goal. They merely are a step in a process that allows us to approximate regret of a segment for an arbitrary reward function. What reward functions and policies should be inserted is an important open question for SF-based methods in general, but our sense is that the performance of GPI under SF-based methods is improved with a greater diversity of SF functions (via a diverse set of policies) and by having some policies that perform decently (but not necessarily perfectly) on reward functions for which state and Q values are being estimated via GPI. \n\nTo determine what number of feature components should be used is essentially asking what features are needed to linearly model a reward function. If the reward function is linear and its components/features are known, then only those features should be used. At the other extreme, if the state and action space are discrete, one could know nothing about the reward function and yet linearly model all possible reward functions by creating a feature for each (s,a,s') that is 1 for (s,a,s') and 0 otherwise. If either are continuous and the reward function's linear features are unknown or the reward is nonlinear, then clear guidance requires the follow-up work on scalability that we discuss elsewhere in these comments as out of scope and also near the end of App F.1.\n\n**\"Theorem 3.1 (and Definition 3.1) is dubious. Implicitly there is an assumption that the dataset covers every \u03c31 and \u03c32 pair and infinite data for each one.\" // \"what assumption \u2026 over the covariates\"**\n\nYou are right that we made this assumption, but it was explicit. We explicitly stated this assumption in L137-138: \"Further assume that in this dataset, all possible n-length segment pairs appear infinitely many times.\" This type of assumption is standard in identifiability proofs, where one wants to know if the parameters of a data-generating model can be recovered under the highly favorable conditions of infinite, exhaustive data. Additionally, no assumption is being made over the covariates; as long as all segment pairs appear infinitely in the data, the proof holds.\n\n**\"P(\u22c5|r) is actually not in the class of models from Section 2.2\u2026 The problem is therefore misspecified, and it\u2019s not surprising that there is an identifiability issue\u2026 an unfair comparison.\"**\n\nAs you correctly point out, Thm 3.1 shows that the class of models from Sec 2.2 achieves identifiability with regret-based preferences. However, Thm 3.2, as a negative result, does need to rely specifically on the class of models from Sec. 2.2. Rather, Thm 3.2 shows that there are MDP\\r tasks in which _there is no class of models that _can recover an equivalent reward function from partial-return-based preferences if the preference generator noiselessly prefers according to partial return. Specifically, we show that the mapping from two reward functions with different sets of optimal policies to partial-return based preferences is a many-to-one-mapping, and therefore the information simply does not exist to invert that mapping and recover a reward function with the same set of optimal policies. We state this more clearly in the uploaded diff.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KS33hv1S1j",
                "writer": "author",
                "reply_to": "2uZBzxzAyHj",
                "title": "Response to reviewer",
                "comment": " Thank you for your suggestions and in particular for your encouragement to open our code and data.\n\n**The code should be open sourced.**\n\nWhen we submitted, we wrote in the NeurIPS submission checklist (L478\u2013481) that we were seeking approval for open sourcing the code and data, which includes the learning code, the Mechanical Turk UI for gathering human preference data, and our human preferences dataset. Good news: it was approved and all will be shared! We could not find any previous paper that shared a human dataset (and many never test their algorithm for \"human\" preferences with real humans), so we are excited to be providing the first such dataset for others to reproduce our work and do novel research with the dataset.\n\n**\"compare with other baselines, such as methods using [partial return]\"**\n\nThe commonality of [9-16] is that they use partial return and (usually) likelihood maximization, which is _precisely_ the baseline that we use throughout our paper. Future work could compare using different preference models with each of these papers' novelties, such as choosing segment pairs for max info gain, but such work would be extending the baseline we already focus on to new settings, which would be informative but outside our scope. We do want to draw your attention though to the 4 additional baselines we introduce in Appendix B, each receiving more limited evaluation than our partial-return baseline. Since submission, we did however add in App B.3 and F.3 a second baseline, an \"expected return\" preference model, which is halfway between the partial return and deterministic regret models, considering each segment's partial return and end state value (but not start state value).\n\n**\"What is the segment\u2019s desirability?\"**\n\nDesirability is not a technical word in this paper. We only use it in a colloquial sense to mean \"characteristics that would make something (e.g., segment or a reward) more preferable than something else\". We will clarify in the final version.\n\n**\"environments are also limited\" / \"more results\"**\n\nPlease read why we focus on simple environments for scientific reasons, under \"This paper does not demonstrate scalability to more complex environments\" in our general comment to our reviewers. We also note that though our experiments were limited to grid worlds, our synthetic preferences experiments include learning and testing reward functions 100 different randomly generated grid world MDPs. Regarding the amount of results, we have theoretical results; 6 sets of results with synthetic data: the main one in Section 6.2 and 5 more in Appendix F.2; and a large human subjects experiment tested\u2014which required months of UI design and iterative tuning\u2014with correlational and likelihood results in Section 5 and reward-learning results with 3 preference models (Sec 6, but see App F.3 for all 3 models). As described under \"Updates\" in our comment to all reviewers, a new baseline has been added since submission, in the uploaded diff's App B.3 and F.3.\n\n**\"more details about experiments\" / \"How to define a start state of a segment\u2026 [and] end state of a segment?\"**\n\nFor space considerations we put most experimental detail in our appendix. We have added more detail about how segment pairs were chosen for human labeling (App D.3), which implicitly includes how start and end states are chosen. We will also open source our code, which will provide a secondary form of experimental detail.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "p-z5ilzAs5",
                "writer": "author",
                "reply_to": "BpAFeMpOkZI",
                "title": "Response to reviewer, part 1",
                "comment": " Thank you for your suggestions on improving presentation, including for theoretical clarification. We heartily welcome suggestions for improvement and have addressed many of yours in the uploaded diff in the supplementary material.\n\n**\"In Section 4, it mentions that data was collected via two different methodologies. In the end, was data from only the second [collection methodology] used to present the results in the end or was it a mix?\"**\n\nBoth stages' data was used unless otherwise stated. We have added this missing information in Sec 4.3.\n\n**\"... more details about how the actual segments that were presented to the labelers were generated?\"**\n\nYes! We agree such detail is needed and have added it (see App D.3 in the uploaded diff).\n\n**The section organization is confusing. I did not see an algorithm until Sec 6.**\n\nWith your feedback in mind, we have revised the paper to be much clearer regarding a single point that should help with your confusion. Specifically, Section 2 defines complete algorithms through Equations 1 and 2 for partial return and Equations 1 and 5 for regret. This algorithm assumes that regret can be exactly measured, and the Sec 3 theory makes this same assumption. Sec 4\u20136 comprise our experimental analyses (not counting what is in the appendix), with Sec 4 about data gathering, Sec 5 about results _without_ reward function learning, and Sec 6 about results from reward function learning. Alg 1 is presented in Sec 6 because it focuses on creating approximations of regret for its practical application in our experiments, and otherwise it is the same algorithm as in Section 2, which is packed into line 10 of the Alg 1. The uploaded diff shows our revisions to be clearer: Sec 2.3 has a new paragraph entitled \"Algorithms in this paper\" that explicitly states that two algorithms have been defined (with clarification on what they are), points forward to the algorithms that will be defined, and connects the 3 main algorithms with which results they are used to obtain; and the first sentence of Sec 6.1 now clearly states that it is only introducing an approximation of regret to be used in the algorithm from Sec 2.\n\n**For the partial return algorithm, was the same framework used but the model just swapped out? How does this compare with past algorithms that use partial return?**\n\nAlg 1 is not used for learning reward functions with the partial return preference model, since no approximation of regret is needed for learning via partial return. All reward learning with a partial return preference model uses the algorithm from Sec 2. Our algorithm using partial return matches that of numerous past works, including the most cited one on this topic, by Christiano et al.\n\n**\"... discussion of related work is quite sparse\" / \"difficult to place this work in the literature as a result\"**\n\nInstead of an explicit related works section, we discuss related work throughout the paper, where each work is related to the immediate content of the paper. Putting all of these discussions with citations together adds to a substantial discussion. As far as placing the work in the literature, this work is a direct response to the unexamined and taken-on-faith partial-return assumptions of existing literature ([9\u201316]. We welcome any suggestions on how we might make that clearer than we do in L25-33, and any suggestions for additional work we should discuss.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "E8AG-IJiMFb",
                "writer": "author",
                "reply_to": "XFxlZNrtnhn",
                "title": "Response to reviewer",
                "comment": " We thank you for your kind words and for communicating your clear understanding of the paper.\n\n**\"What if there is a set of optimal policies\u2026\"?**\n\nNeither our core algorithm (Section 2) nor our approximation algorithm (Alg 1) assume a single policy. Note that regret is defined over optimal advantages, and all by definition optimal policies have the same Q values and state values (e.g., see Sutton and Barto)\u2014and therefore the same advantages\u2014from any state-action pair, so our definitions are not affected by having more than one \u03c0*.\n\n**\"biggest weakness \u2026 calculating the regret requires one to know the optimal policy under the true reward r (or at least the value and q-value function of that optimal policy under any reward \\tilde{r})\"** \n\nThis is an easy misunderstanding to make. The algorithm requires knowing the optimal state and Q values\u2014or some approximation of them\u2014for the current candidate reward function, which is _not _dependent on the optimal policy for the true reward function. This solve-an-MDP-in-the-inner-loop problem is common to many IRL algorithms, including in the foundational 2000 IRL paper by Ng and Russell, where numerous approximations have been developed to handle the problem tractably. Our Alg 1 addresses this issue in certain settings, and the latter portion of App F.1 discusses how to do so in deep RL settings, as Brown et al. have done already for learning reward from preferences.\n\n**L63 typo**\n\nGood catch and thank you! Fixed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mBAuO0Nxf0",
                "writer": "author",
                "reply_to": "q69d1pOaQP",
                "title": "Response to all reviewers, part 2",
                "comment": " **-Common concerns among our reviewers-**\n\n**Practically using regret is challenging.** (@ieTk, @xps1, @SeBN)\n\nWe agree yet do not find this assertion problematic. We have provided evidence\u2014theoretically and with experimentation\u2014that regret is more effective when precisely measured or effectively approximated. The challenge of efficiently creating such approximations presents one clear path for future research and is not a good argument for staying within the local maximum of partial-return-based preference models.\n\nIn fact, like our regret model, inverse reinforcement learning (IRL) was founded on an algorithm that requires solving an MDP in an inner loop of learning a reward function. (For example, see the foundational 2000 Ng and Russell paper on IRL and also Algorithm 1 in the recent IRL survey at [https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf).) This challenge hasn't stopped IRL from being an impactful problem, and handling this inner-loop computational demand is the focus of much IRL research.\n\nAppendix A.3 has been added to address this topic.\n\n**This paper does not demonstrate scalability to more complex environments.** (@ieTk, @SeBN)\n\nWe agree. The focus of this paper is not deep RL or complex tasks, so such scalability\u2014while highly important\u2014was not in scope. As we mention above, this paper seeks a _scientific_ analysis of human preferences and preference models, which conflicts with scaling up to tasks commonly used for deep RL. Specifically, testing in more complex tasks would require coarser approximations of regret, which, as stated earlier, can make results harder to interpret. For example, if a regret-based algorithm fails, we wouldn't know whether regret is to blame or whether the approximation of regret is to blame.\n\nFurther, we believe that algorithms for human-created data should be tested with _human_-created data, only using synthetic data to better understand the algorithms. Gathering human datasets is highly expensive in terms of design and engineering (see [https://bit.ly/humanprefs](https://bit.ly/humanprefs) for our UI that trains subjects and elicits their preferences). Testing these models in other tasks with real human data would have required multiple costly human subjects studies. When the cost of a human subjects study is properly appreciated in contrast to that of a purely computational experiment, we trust that our reviewers will agree that our combined 3 proofs, 3 evaluations of our human preferences dataset (Secs 4 and 6.3, extended in App F.3), and 6 synthetic-data evaluations (Sec 6.2 and App F.2) are more than sufficient for a single paper. Our contribution of a released human dataset will make testing in multiple environments easier for future research efforts in this area.\n\nIn future work on the _application_ of regret preference models, we or other researchers can face the _engineering_ task of scalability. Given that IRL has made tremendous progress in this direction and the cited Brown et al. [30] paper has scaled an algorithm with similar needs to those of Alg 1, we are highly optimistic that the methods to scale can be developed and probably already exist (e.g., in [30] and in the later part of our App F.1, under \"Instantiating Algorithm 1 for reward functions that may be non-linear\").\n\n**Noiseless preferences are not theoretically interesting or should be connected to past work.** (@xps1, @SeBN)\n\nWe explain below (and at the end of Appendix C in the uploaded diff in the supplementary material) why noiseless preferences are important.\n\n\n\n1. An intuitive argument: Noise is often motivated as modeling human error. Having an algorithm rely on noise\u2014structured in a very specific, Boltzmann-rational way\u2014is an undesirable crutch. Noiseless preferences are also feasible, if rare.\n2. In many related settings, noiseless data is assumed for theory or derivations. For instance, the foundational IRL paper by Abbeel and Ng on apprenticeship learning treats demonstrations as noiselessly optimal. Also, \"Reward identification in inverse reinforcement learning\", an ICML 2021 paper, focuses on identifiability with noiseless, perfect demonstrations.\n3. Having structured noise provides information that can help both preference models, but these proofs show that there are cases where the signal behind the noise\u2014either regret or partial return\u2014isn't enough in the partial return case to identify an equivalent reward function. So, in a rough sense, regret is better at using both the signal and the noise, which might explain its superior sample efficiency in our experiments, across both human labels and synthetic labels. Relatedly, the noiseless setting can help us understand each preference model's sample efficiency in a low-noise setting.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "q69d1pOaQP",
                "writer": "author",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Response to all reviewers, part 1",
                "comment": " We thank our reviewers for their time and thoughtful consideration. We are encouraged that our reviewers wrote that our submission focuses on an interesting problem (@dwHc), provides a \"key insight\" that human preferences are based on more than just instantaneous reward of segments (@ieTk), contains a \"sound and reasonable\" derivation of the segment's regret as the advantage function (@SeBN), and shows that our novel regret-based preference model significantly outperforms the pre-existing partial-return-based model in experiments (@xps1). In particular, we are grateful that your reviews have led to what we consider to be vast improvement in our submission, viewable via the new version we have uploaded and in **the diff we included in supplementary materials**.\n\n**[Recap] What is our goal?** A _scientific_ study of the effect of using different preference models when learning reward functions from human preferences, which had previously been unexamined.\n\n**[Recap] Why this goal?** To correct a perceived mistake in recent research\u2014assuming human preferences are unaffected by state values of trajectory segments\u2014and to create a strong foundation for further research based on insights from theory and simple domains with minimal approximation error.\n\n**[Recap] How do we achieve our goal?** We reveal flaws in the commonly assumed partial-return preference model; propose an improved preference model (based on regret); compare them theoretically and with real human data and synthetic data; and show that the choice of preference model can be impactful.\n\n**[Recap] What is not the goal?** To demonstrate scalability to complex problems, which is important but beyond scope. Such a focus would force coarser approximations of regret, which would obscure the scientific analysis this paper focuses upon. We elaborate in the scalability part of \"Common concerns\" below.\n\n**-Updates-**\n\n**Open source and data.** (@dwHc) As foreshadowed in our NeurIPS checklist, we are thrilled to share that we have approval to **open the learning and experimentation code, the software for the UI and backend to gather human data on Mechanical Turk, and our human preferences dataset.** We could not find any previous paper that had shared a dataset of preferences from real humans and are particularly excited that we can make the first such dataset available.\n\nWe have also added **another baseline preference model**, based on expected return of a segment, and tested it on the human preference dataset (in App B.3 and App F.3 of the uploaded diff in the supplementary material). The regret preference model matches or exceeds the performance of this model as well.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XFxlZNrtnhn",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Official Review of Paper10488 by Reviewer ieTk",
                "comment": " The paper deals with the problem of modeling human preferences in a setting where they are presented with two segments and choose to compare the two. Importantly, the authors propose that people compare the segments in terms of their values (measured by a notion of regret) instead of the total reward achieved by each segment. A regret notion---the sum of the negative advantage function value of the segment---is used to summarize the value of a segment. The advantage function is defined under an optimal policy pi^* (optimal wrt the true reward r) and the reward of interest \\tilde{r}. Strength: The paper provides a key insight that preferences are not just a function of the (instantaneous) reward of the segment of interest. Given that humans may care about the goal of a task, their preference towards different segments will necessarily be influenced by their long-term value, i.e., how these segments perform in terms of achieving the goal. The identification results presented in Section 3 nicely summarized the proposed insight. \n\n\nWeakness: The biggest weakness to me is how one can utilize this insight in practice. In particular, calculating the regret requires one to know the optimal policy under the true reward r (or at least the value and q-value function of that optimal policy under any reward \\tilde{r}). In general, even if one provides an optimal policy \\pi^*, the planning problem (and possibly learning problem when the transition matrix is unknown) will make the estimation of the regret very hard. As a related note, in the general framework, the regret definition under a particular optimal policy and L150 (\"And regret(\\sigma|r) > 0 if and only if \\sigma is suboptimal\") in the proof seem to only hold if one assumes there is one single optimal policy \\pi^*. What if there is a set of optimal policies under the true reward r? How should one adjust the regret notion for that?\n Could the authors elaborate more on the setting when there are multiple optimal policies, how one should define regret in such cases, and how the results in Section 3 look like when one chooses different optimal policies to define ther regret?\n\n\n-----\n\nL63: typo in the mathematical expression Yes, the authors have discussed the limitations and societal impact of their work in Appendix A. (When writing the above reviews, I have not read Appendix A of their paper yet.)",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "2uZBzxzAyHj",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Official Review of Paper10488 by Reviewer dwHc",
                "comment": " The paper studies the issues of reward functions. It considers alignment on the pairs of trajectory segments from human-generated preferences. The paper finds that it is flawed that human preferences are assumed to be informed by partial return. Previous works consider the sum of rewards over a segment. The paper provides an alternative preference model based on the regret of each segment.\n The paper addresses an interesting problem about human preferences. It shows that the partial return preference model can prefer suboptimal actions with lucky outcomes.\nThe paper also provides a method based on the regret of each segment, which is equivalent to the negated sum of an optimal policy\u2019s advantage of each transition in the segment.\nThe paper also provides theoretical comparisons.\n\nThe main weakness is that the experimental results seem not to be enough. The proposed method does not provide comparison with previous methods from [9-16].\nThe environments are also limited. \nIt would be better to provide more results.\nAnother weakness is that the code is not opened. It would be difficult to reproduce the results by others.\n What is the segment\u2019s desirability? \nFigure 1 is a little confusing.  In Figure 1, the right segment should have a higher sum of reward according to humans\u2019 preference.\nHow to define a start state of a segment? And how to define an end state of a segment?\n\nRegret is computed based on a segment. The segment is also partial. \nWhat is the advantage of using regret? \nThe motivation of using desirability is not clear.\n It would be better to provide more results and more details about experiments.\nThe paper needs to compare with other baselines, such as methods using (2).",
                "rating": 4,
                "confidence": 2
            },
            {
                "review_id": "BpAFeMpOkZI",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Official Review of Paper10488 by Reviewer xps1",
                "comment": " This paper studies the problem of learning from human preferences. A new preference model is proposed which compares the advantage on the optimal policy between two trajectory segments. Some theoretical results are given that show the method is consistent under some assumptions and that a popular alternative is potentially not consistent. An algorithm is proposed to make use of this model. A new dataset is collected for a simulated delivery problem where workers provided preference labels for segments given to them. The proposed algorithm and model were compared with a partial return model on both a synthetic dataset and the collected dataset. Strengths\n- The proposed preference model seems new to my knowledge and makes sense algorithmically.\n- The theory could be interesting although I have some reservations about correctness and the significance of one of the results.\n- The algorithm seems to perform significantly better than the partial return baseline in empirical evaluations.\n\nWeaknesses\n- The presentation could be improved. Generally speaking, there are many seemingly out of place paragraphs and sections that do not seem to advance the content of the paper. For example, Section 3 is about theoretical comparisons of the learning objectives but then Section 4 abruptly is about an experimental model, but we do not even know what the algorithm looks like or how we will use the models from Section 3 at this point. This is only explained in Section 6.\n- The description of the algorithm is very confusing and way too informal to the point where I am still unclear on what it is actually being done.  \n    - What are the input rewards and policies? Why can they be empty sometimes?\n    - Why would line 4 already estimate those? What procedure is being done to estimate these?\n    - How many feature functions are necessary?\n    - For the partial return algorithm, was the same framework used but the model just swapped out? How does this compare with past algorithms that use partial return? This seems important for the experimental comparison.\n- The discussion of related work is quite sparse and it is difficult to place this work in the literature as a result.\n- Theorem 3.1 (and Definition 3.1) is dubious. Implicitly there is an assumption that the dataset covers every $\\sigma_1$ and $\\sigma_2$ pair and infinite data for each one. Not just that it contains infinite data for some distribution over certain segments, which is what Definition 3.1 is ambiguous about. This only becomes apparent in the proof where this unstated fact is used. \n- I am also not convinced that Theorem 3.2 is all that significant. The setting is different from the preliminaries of the paper: to assume noiseless labels is to say that $P(\\cdot | r)$ is actually not in the class of models from Section 2.2 since the softmax cannot realize this model for finite rewards. The problem is therefore misspecified, and it\u2019s not surprising that there is an identifiability issue, considering that the KL divergence cannot even be driven to zero if the true reward function is plugged in! Note that Theorem 3.1 crucially made use of the fact that the model is realizable to prove the positive result, so I feel that this is an unfair comparison.\n- The computational burden of solving potentially many MDPs to optimize the proposed preference model seems difficult to overcome.\n\n - Can the authors clarify what assumption is being made over the covariates in the dataset $D_{\\geq}$ in Definition 3.1?\n- In Section 4, it mentions that data was collected via two different methodologies. In the end, was data from only the second used to present the results in the end or was it a mix?\n- Can the authors provide more details about how the actual segments that were presented to the labelers were generated? In the appendix it just says that certain trajectories were favored, but it\u2019s unclear to me what that means. Were these generated by demonstrations or an algorithm? Were they generated specifically to have good coverage over the state space?\n See above discussion.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "DtNlCuPRJy",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Official Review of Paper10488 by Reviewer SeBN",
                "comment": " The paper aims to learn the reward function from preferences between pairs of trajectory segments by introducing a new notion of the segment's regret that is based on the advantage function. Notable, they argue that the existing approach using the partial returns in the literature is flawed due to the lack of an identifiability property: the ability to recover the reward function underlying the preference dataset. As a result, the importance of using the segment's regret in learning the reward function from preference datasets is highlighted.\n The main contribution of the paper is to introduce a new preference model based on the advantage function. The derivation of the segment's regret as the advantage function is sound and reasonable.\n\nHowever, there are several weaknesses in the paper.\n\n1, if I am not mistaken, the proposed regret model is limited to the case of deterministic policies. How is it extended to stochastic policies?\n\n2, compared with existing approaches that use partial returns, computing the advantage function in the segment's regret is computationally expensive (as mentioned in Appendix A1). While the paper shows an approximation to reduce the computation in Section 6.1, it only works for the linear reward function. Hence, a more detailed discussion on the time complexity of the proposed regret vs. that of the partial returns is necessary, and the work needs to illustrate whether the proposed method is scalable to more practical problems, e.g., by including experiments with neural networks and larger state/action spaces.\n\n3, if I am not missing anything, Theorem 3.1 and 3.2 are not comparable. While both partial returns and the segment's regret use the logistic function (Equation 2 and Equation 5), there is a difference in the two theorems: in Theorem 3.1, if $regret(\\sigma1|\\tilde{r}) < regret(\\sigma_2|\\tilde{r})$, $P_{regret}(\\sigma_1 > \\sigma_2|\\tilde{r}) > 0.5$, in Theorem 3.2, if $\\Sigma_{\\sigma_1} \\tilde{r} > \\Sigma_{\\sigma_2} \\tilde{r}$, $P_{\\Sigma_r}(\\sigma_1 > \\sigma_2|\\tilde{r}) = 1$.\nWhich existing works assume/imply the latter assumption?\nFurthermore, it raises the question of whether $P_{\\Sigma_r}$ is indeed nonidentifiable if Theorem 3.2 has the same premise as Theorem 3.1.\n\n4, there is no comparison with IRL methods (which often rely on the value function or the Q function instead of partial returns): intuitively how they are different, and empirically how they are different (e.g., by experimental results).\n\n5, existing works (e.g., [9,10]), the reward function is define as a function of the state and the action or the state only (i.e., r(s,a), r(s)). Do these reward formulations affect the result in the paper?\n\n6, there is only a toy grid world experiment (a very small state and action space, and linear reward functions) which is quite limited.\n\n\n Please address the weakenesses mentioned above. The scalability of the proposed method to a more realistic environment is not demonstrated.",
                "rating": 4,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "7BlQMwp_44p": {
        "paper_id": "nips_2021_7BlQMwp_44p",
        "paper_title": "ReLU Regression with Massart Noise",
        "paper_abstract": "Ilias Diakonikolas, Jong Ho Park, Christos Tzamos",
        "paper_acceptance": "accept",
        "meta_review": "This paper considers the problem of ReLU regression under the Massart noise model that has recently been studied extensively for classification problems. The main result of the paper is an algorithm that does exact parameter learning under certain distributional assumptions. \n\nAll the reviewers appreciated the results of the paper. While it builds on prior techniques in the area, the technical novelty in the work is high enough. Certain technical questions raised by the reviewers were subsequently resolved by the authors' response. Overall this is a solid theory paper and I recommend acceptance.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "PMXJ7iV3EES",
                "writer": "official_reviewer",
                "reply_to": "ZoZ60fe1L4O",
                "title": "Post-rebuttal",
                "comment": " Thanks for the feedback. I keep my initial score and recommend this paper for acceptance. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K2JFn5SmY_x",
                "writer": "official_reviewer",
                "reply_to": "nIVVQxj8G1P",
                "title": "Final comments to authors",
                "comment": " Thanks for your response,\n\nMost of my concerns are obviated. \nHowever, I keep my score since (at least IMO) the writing of the paper can still be greatly improved and the contents could become more accessible by the general audience. Moreover, some of the transitions between topics are not smooth enough.\n\nOther than that, this work seems to be a valuable technical contribution to the NeurIPS community.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "h9s61mRAQzS",
                "writer": "official_reviewer",
                "reply_to": "ktGH2uNh1O8",
                "title": "reviewer comments",
                "comment": " Thank you for the response. It addressed all my concerns.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ktGH2uNh1O8",
                "writer": "author",
                "reply_to": "1UewisEe8v7",
                "title": "Author response for Reviewer 8NsY",
                "comment": " We thank the reviewer for the helpful suggestions and comments. Below we respond to the main concerns/questions from the reviewer. \n\n**Sample Complexity:** As the reviewer has mentioned, the sample complexity of our algorithm for linear regression with Massart noise is quantitatively higher, compared to the noiseless case (corresponding to $\\eta=0$). While it is an interesting question to improve on the sample complexity of our algorithm, we emphasize that our work gives the *first polynomial sample and time* algorithm with strong recovery guarantees for ReLUs under minimal distributional assumptions. \n\n**Bit complexity Dependence in Runtime:** As the reviewer has noted, the runtime of our algorithm depends on the bit complexity $b$ of the points. This is a common theme in *any* algorithm that uses linear programming as a subroutine. For example, even for the basic problem of learning a linear separator *without noise*, all known algorithms require such a bit complexity dependence. In fact, such a dependence cannot be removed without a strongly polynomial time algorithm for arbitrary linear programs -- a major open problem in computer science. In our specific setting, we solve a linear program (or use the ellipsoid method) to perform exact recovery, which incurs a runtime polynomial in $\\log(1/\\epsilon)$, not $1/\\epsilon$. Given our learning task, we cannot remove the dependence on $b$ without a strongly polynomial algorithm for LPs. However, it is important to note that the sample complexity of our algorithms does not depend on the bit complexity, as a consequence of using radial isotropy. In fact, spectral outlier removal procedures incur a b dependence in the sample complexity, as demonstrated in our PAC learning results (Appendix D). Therefore, we highlight that the bit complexity dependence on the runtime is not unusual and rather that the lack of $b$ in the sample complexity is an achievement.\n\n**Distributional Assumptions and Examples:** The distributional assumption we require, i.e., that $\\Pr[w \\cdot x \\geq 0] \\geq \\lambda$, means that there exists at least some non-trivial probability mass on any homogeneous halfspace. For example, the parameter $\\lambda$ would be $1/2$ for any distribution symmetric around the origin. This is an extremely mild condition that is satisfied by a wide range of distributions. For example, it is satisfied by any mean-zero distribution with non-degenerate covariance matrix. In contrast, virtually all prior results on linear or ReLU regression require at least some non-trivial concentration (tail bounds), anti-concentration, and anti-anti-concentration properties.\n\n**Technical Novelty of the use of Radial-isotropic Transformations:** The central question of our paper is whether there exists realistic label noise models in which efficient learning is possible *without strong distributional assumptions*. We know that stronger adversarial noise models, such as the strong contamination model, have computational hardness results as described in the introduction (Lines 27-34). Moreover, for the Massart noise model, there has been recent work (see references 9, 12 in our paper) that learn halfspaces in a distribution-independent fashion. Analogously, we provide ReLU regression results with only mild assumptions that do not require any strong tail bounds or concentration bounds.\n\nIndeed, as the reviewer points out, if the data is uniformly distributed on the unit sphere, then we do not need to perform radial-isotropic transformations since the data is already well-behaved. However, this is not the case for most distributions, even after normalization. Thus, the use of radial-isotropic transformations allows us to generalize our regression method to apply beyond such well-behaved distributions.\n\n**Computational Cost of Radial-Isotropic Transformation:** Computing a radial-isotropic transformation $A$ can be done in polynomial time as specified in Lemma 2.1. Because we do not need an exact transformation but only an approximate one where $\\gamma = 1/2$, we can use previously established algorithmic results in computing this approximate transform. We describe how to compute $A$ using the algorithmic results of Artstein-Avidan et al. and prove this lemma in Appendix A.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8eFXmROg3nT",
                "writer": "author",
                "reply_to": "VohWkYfl8yd",
                "title": "Author response for Reviewer Dpt6",
                "comment": " We thank the reviewer for the appreciation of our paper and the helpful suggestions.\n\nFor Definition 1.1, the adversary may change the labels of the (potentially) corrupted samples to arbitrary values. We will also update the paper with clear definitions and discussions with regard to the bit complexity of the parameters and samples.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nIVVQxj8G1P",
                "writer": "author",
                "reply_to": "6KEmTiPc2J",
                "title": "Author response for Reviewer vpXo",
                "comment": " We thank the reviewer for the helpful suggestions and comments. Below we address the specific questions from the reviewer. \n\n**Robustness under Model Misspecification:** It is indeed an interesting open question and a promising future direction to extend our work to cases where the true function is not actually a ReLU and allow for some form of model misspecification or additive noise as the reviewer suggests. Our theoretical results focus on the realizable model for learning ReLUs (and linear functions) with Massart noise. This is an important and well-studied setting even for the noiseless case (corresponding to $\\eta = 0$), and has been the focus of a number of prior works, including recent papers appearing in top venues (see references [23, 31, 45, 49] and Lines 66-73).\n\n**Description and Definition of Adversary:** We define the Massart adversary in Definition 1.1 and use this definition throughout the paper. In fact, this definition is equivalent to the informal description given in Lines 117-119. This is because the adversary of Lines 117-119 does not have to change all the labels of the randomly selected $\\eta$-fraction. By keeping some of the labels clean, this adversary is equivalent to that of Definition 1.1.\n\n**Intuition Behind the Massart Model:** As the reviewer has mentioned, the Massart noise model is stronger than purely random noise and weaker than the strong contamination model, which has computational hardness results even for well-behaved distributions, as described in Lines 32-34. It aims to capture cases where the samples that are (potentially) corrupted are not correlated, i.e., a uniformly random subset of examples is corrupted. The adversary may corrupt the values of these samples arbitrarily in correlated ways, but may not choose which points to corrupt in advance. This model allows us to escape the computational hardness results that apply when the adversary is all-too-powerful, and obtain efficient and robust regression algorithms under minimal assumptions about how the values are corrupted.\n\n**PAC Learning ReLUs in Line 107:** PAC learning ReLUs is the task of learning a hypothesis $h$ such that $\\Pr_{x \\sim \\mathcal{D}_x}[h(x) \\neq \\mathrm{ReLU}(w^* \\cdot x)] \\leq \\epsilon$ with probability at least $1-\\delta$ *without* any distributional assumptions on $\\mathcal{D}_x$. So it suffices to learn a hypothesis that outputs similar labels as $w^*$ without the hypothesis having to be exactly $w^*$. For instance, we can PAC learn linear functions even in the case that it is information-theoretically impossible to recover $w^*$ exactly, as shown in Appendix D. In the case of Theorem 1.3, we make a mild assumption that $\\Pr(w\\cdot x \\geq 0) \\geq \\lambda$ to *exactly* recover $w^*$, while PAC learning would not make any assumptions and output a close hypothesis $h$.\n\n**Comment regarding Line 144:** Yes, we did not include the transpose since $A$ is symmetric, but we agree that this may be confusing. We will fix the typo and keep our notation consistent. \n\n**Definition of $\\mathcal{S}^{d-1}$:** Yes, the reviewer is correct. We will add this definition for clarity.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZoZ60fe1L4O",
                "writer": "author",
                "reply_to": "0odb8IOz-v",
                "title": "Author response for Reviewer 463t",
                "comment": " We thank the reviewer for the appreciation of our paper and the helpful suggestions. \n\nRegarding the reviewer\u2019s first comment, we note that our results in the main body of the submission are self-contained and do not depend on Hopkins et al.; only our extended results in the appendix require the facts proved in Hopkins et al. However, we agree that their work provides relevant insight and techniques. We will discuss their work when discussing radial isotropy in the Technical Overview section. \nRegarding the second comment, we note that ReLU regression is significantly more challenging than linear regression. A lot of recent literature aims to address the intricacies of ReLU regression stemming from the non-convexity of $\\ell_1$ and $\\ell_2$ regression. We have already highlighted these difficulties in Section 1.2 (Lines 156-174).\n\nAt a high-level, if we knew a priori which points lie on the positive side of the ReLU, ReLU regression would indeed be a straightforward application of linear regression --  since we can learn the parameter vector $w$ with these points. In the presence of noise, however, it is unclear which region corresponds to the positive part and we must learn it simultaneously. This is one of the main challenges that we overcome. Our algorithm builds on the ideas developed in our linear regression warmup, to iteratively improve the guesses about the positive ReLU region yielding a separation oracle. We will incorporate this additional intuition in the revised version of the paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RxciILAjUVl",
                "writer": "author",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Joint Response",
                "comment": " We thank the reviewers for their time and effort in providing feedback. We are encouraged by the positive feedback, and that the reviewers appreciated the paper for the following: (i) significant and/or interesting results (463t, vpXo, Dpt6), (ii) technical contribution (463t, Dpt6), and (iii) organization/clarity (vpXo, Dpt6, 8NsY). We address the individual questions and comments by the reviewers separately.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6KEmTiPc2J",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Official Review of Paper7733 by Reviewer vpXo",
                "comment": "The aim of this work is to propose a computationally efficient algorithm for linear and ReLU regression in the presence of a bounded adversarial Massart noise model. It has been tried to make the assumptions on the underlying data distribution as mild as possible, not to mention that (as claimed by the author(s)) some assumptions are necessary from an information-theoretic point of view.\n\nThe core idea is to consider the unknown corrupted samples as outliers which do not follow the simple linear or ReLU rules between their features and corresponding labels. This should be added to the fact that (based on the paper's assumption) the fraction of corrupted samples is less than 1/2, so the clean data points have the majority. This way, author(s) have proposed a linear transformation on the samples in order the make them \"Radially Isotropic\", so (with high probability) the ratio of outliers to clean samples remain as low as possible in *every direction* of the space. Then, some existing robust estimators such as sum of $\\ell_0$ or $\\ell_1$ models have been shown to remove the effect of outliers and acquire the true parameters. I haven't completely checked the proofs, however, the overall idea makes sense to me and is, in fact, quite interesting. In any case, I am not completely familiar with this line of research so I wait to see other reviews in order to assess the novelty of techniques that have been utilized in this work.\n\nWith respect to weaknesses, paper lacks proper discussion at some points which I have explained in the \"Main review\" section. Also, author(s) have assumed a completely clean and noise-free model except for the adversarial part which still includes more than half of the samples. This assumption is a little worrisome in practice, where some minimum levels of, for example, additive noise are *always* present. How robust or sensitive is the proposed method and its theoretical guarantees when the presumed ideal noise-free environment is minimally perturbed?\n\nOverall a well-motivated and fairly well-written paper with some interesting theoretical achievements (as far as I am aware). My vote at this stage is weak-accept.\n  \nMy main concerns are as follows:\n\nAuthor(s) seem to have a good grasp of existing literature in this area, but it hasn't been completely reflected in the Introduction section. Some discussions and comparisons are still vague and need clarification. I believe the classification of prior advancements in terms of: achieved guarantees, distributional assumptions and etc. can be improved by some reorganization, which also helps the reader to position the current work w.r.t. others, more effectively.\n\nThe adversary described in Definition 1.1 does not match with the one that has been explained in Lines (117:119). In Def 1.1, adversary is allowed to alter (at will) each point $\\left(x_i,f\\left(x_i\\right)\\right)$, with probability $\\eta\\left(x_i\\right)\\leq\\eta$. No information regarding $\\eta\\left(\\cdot\\right)$ has been given except that it is bounded by the constant $\\eta$. That means the function $\\eta\\left(\\cdot\\right)$ can also be chosen by an adversary. That is different from the explanation given in Lines (117:119): \"we consider a more restricted adversary that is presented with a uniformly random $\\eta$ fraction of the points, which can be corrupted arbitrarily at will\". So a natural question would be which of them is finally considered in this work?\n\nAlso, more discussion w.r.t. the adversarial perturbation model considered in this work would be helpful. It is not as strong as the \"contamination model\", but obviously hurt more than a purely random noise. What is the intuition behind this particular noise model?\n\nIn Section 1.2, the transition from ReLU and linear regression in the presence of Massart noise to $\\ell_0$ or $\\ell_1$-regression is not smooth. More explanations are needed here, otherwise the core idea behind the results become somehow ambiguous and hard to understand.\n\n\n------------------------------------------------\nMinor comments:\n\n-(Line 107): What author(s) mean by \"It remains an interesting open problem whether similar PAC learning guarantees can be obtained for the case of ReLU regression.\"? Aren't the results from Theorem 1.3 associated with the most general case? Or author(s) are referring to a more general adversarial noise model rather than Massart noise here? \n\n-(Line 144): I guess it should be $w=A^Tw'$, right? It might not be important since $A$ is later assumed to be symmetric. However, it may confuse the readers.\n\n-(Definition 1.4): What is $\\mathcal{S}^{d-1}$? Is it the surface of $d$-dimensional unit sphere? No problem here.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "0odb8IOz-v",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Official Review of Paper7733 by Reviewer 463t",
                "comment": "The authors study exact parameter recovery in ReLU regression under a generalization of the semi-random Massart noise model. Assuming certain distributional anti-concentration, the authors propose and analyze an algorithm that runs in polynomial time and recovers the exact parameters with high probability, given a sample of size polynomial in the input dimension and the (inverse of) the Massart parameter. Along the way, they also provide similar guarantees for the simpler case of linear regression.  The paper is a joy to read. The study is important and relevant to the NeurIPS community. To the best of my knowledge, the results are novel and significant. I checked the proofs only at a high level, but the claims seem sound to me.\n\nA couple of comments/questions:\n1. The algorithm design as well as the proofs -- both in linear/ReLU regression -- leverage insights and techniques from the work of Hopkins et al. 2020. While this paper is cited in the appendix, there is no reference to it in the main text. I urge the authors to discuss this reference in the main text.\n2. Given suitable anti-concentration assumptions, what are the main challenges in extending the results from linear regression to ReLU regression? The guarantees in Theorems 1.2 and 1.3 are very similar, and under the anti-concentration assumption for ReLU regression, proof of 1.3 seems like a straightforward extension of 1.2.\n Yes",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "VohWkYfl8yd",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Official Review of Paper7733 by Reviewer Dpt6",
                "comment": "The paper considers the problem of linear and ReLu regression in the Massart noise model, where an adversary is allowed to change the label to an arbitrarily value with some probability at most $\\eta <1/2$. The work develops an efficient algorithm that achieves exact parameter recovery in this model under mild assumptions on the underlying distribution.   **Quality and Clarity**: \n\n-  One of the strong points if the paper is that it is very well-written and well organized. The ideas are presented clearly with intuitive explanations, and the proofs are rigorous. \n\n- The authors are careful in evaluating the strengths/weaknesses of their work. The work seems to be built upon an established literature and the review of related works are informative. \n\n- Minor comments: \n  - Definition 1.1: \"and after inspecting which samples can be corrupted, it may change the label to an arbitrary value\" --> I interpreted this as \"change each label to an arbitrary value\" or \"change the labels to arbitrary values\". Please clarify.\n  - bit complexity of the parameters and samples: this is mentioned a few time but are not defined\n  - the sentence starting at Line 140 is very difficult to read\n\n**Originality**: \n\n-  The paper consider a slightly weakened model (in comparison to the the contamination model), but leads to more generality in the underlying data distribution.\n\n- The result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]), and the methods are designed specifically to address the limitation of those works.\n\n- The generalization of the approach from linear to ReLU regressions are non-trivial in both technical analysis and algorithmic designs. \n\n\n**Significance**:\n\n- As stated in the Originality section, the result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]) and seems to extend existing works in a demonstrable way.\n\n- On the other hand, I'm unfamiliar with some pieces of related work and don't have a strong conviction in how it will impact the field, and would leave the judgement up to other reviewers. \n n/a",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "1UewisEe8v7",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Official Review of Paper7733 by Reviewer 8NsY",
                "comment": "This paper studies regression problems in presence of Massart (bounded) noise, where for each sample x_i, the adversary is allowed to change its label/response f(x_i) to an arbitrary value with probability less or equal to eta. Specifically, the authors focus on two problems: linear regression and ReLU regression. For both cases, they make a ``mild yet necessary assumption on the data, that the distribution is not concentrated entirely on any linear subspace; and provide algorithms which exactly recover the true function in polynomial time with constant probability.   I feel the paper is well-written but I have a few concerns on the theoretical guarantee.\n\n- In Theorem 1.2, it turns out that the sample complexity for robust linear regression is suboptimal. In particular, even when the noise rate eta = 0, it is given by O(d^3) which is worse than that of noiseless regression. It also seems unpleasant to have bit complexity dependence in running time.\n\n- In Theorem 1.3, I cannot follow the assumption that $P( w \\cdot x \\geq 0 ) \\geq \\lambda$ for all w.\n\n- What is the computational cost to obtain a radial-isotropic transformation matrix A? This turns out to be a crucial algorithmic component but I did not find a concrete discussion. Note that since you require unit data norm and identity covariance simultaneously, finding such transformation is nontrivial.\n\n- The distributional assumption is informally stated throughout. Can you provide a fews examples that satisify the condition, say Gaussian distributions or uniform distributions?\n\n- I may miss some important messages, but if the data distribution were uniform, then would it be necessary to perform radial-isotropic transformation? My understanding is not being necessary, in which case the technical novelty appears vacuous.  yes",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "train"
    },
    "aKZeBGUJXlH": {
        "paper_id": "iclr_2022_aKZeBGUJXlH",
        "paper_title": "Gradient Broadcast Adaptation: Defending against the backdoor attack in pre-trained models",
        "paper_abstract": "Pre-trained language models (e.g, BERT, GPT-3) have revolutionized the NLP research and fine-tuning becomes the indispensable step of downstream adaptation. However, the covert attack is the emerging threat to the pre-train-then-fine tuning learning paradigm. The backdoor attack is a typical challenge, which the victim model fails on the trigger-activated samples while behaves normally on others. These backdoors could survive the cascading fine-tuning stage, which continually posing the application of pre-trained models. In this paper, we proposed a Gradient Broadcast Adaptation (GBA) method, prevent the model from controlled producing outputs in a trigger-anchor-free manner. We design the prompt-based tuning, flexibly accessing the rare tokens while providing a fair measure of distance in word embedding space. The gradient broadcast alleviates lazy updating of potential triggers and purges the underlying abnormal weights. The GBA defense method is evaluated over five text-classification tasks against three state-of-the-art backdoor attacks. We find our method can cover nearly 100% embedded backdoor with negligible performance loss on clean data.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper introduces a defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models. It proposes to utilize prompt tuning to guide the perturbed weights back to a normal state and thus helps avoid the degradation of model's generalization ability. \n\nStrengths:\n- Experiments are conducted across multiple datasets with different types of backdoor attacks, demonstrating the effectiveness of the proposed approach\n- The proposed idea is well motivated and intuitive\n\nWeakness:\n- Improvement on experiment results seems marginal\n- Some technical details of the attack setup are unclear\n- Writing of the paper needs improvement",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "pqNHkii84y3",
                "writer": "author",
                "reply_to": "wgoWffC-eSD",
                "title": "Response to further feedback",
                "comment": " Thanks very much for your feedback! Please feel free to contact us if you have any other questions.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Jq7weXsSZIc",
                "writer": "author",
                "reply_to": "ue-VzpP-HmM",
                "title": "Follow Up",
                "comment": " Dear reviewer,\n\nDo you still have any concerns about our manuscript? We are sincerely looking forward to your further feedback!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RSmjAdNUX0P",
                "writer": "author",
                "reply_to": "3KgBygU9fk",
                "title": "Follow Up",
                "comment": " Dear reviewer,\n\nDo you still have any concerns about our manuscript? We are sincerely looking forward to your further feedback!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wgoWffC-eSD",
                "writer": "official_reviewer",
                "reply_to": "1grZVzxVBMD",
                "title": "thanks to the rebuttal",
                "comment": " i like this work and most of my comments has been addressed and thus i keep my score",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "088ANqVY9Ka",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "Official Review of Paper2080 by Reviewer gVmg",
                "comment": "This paper proposes a method to defend against NLP backdoor attacks. The authors propose to calculate the global direction of gradients of loss with respect to input word embeddings and update word embeddings using the global direction. By doing so, rare words can be updated to a \"normal state\" and are expected to be no trigger of attacks anymore. The authors also empirically show the effectiveness of the proposed method. ==========After Rebuttal==========\n\nAfter reading all the comments, I tend to retain my score.\n\n================================\n\n\nStrengths\n1.\tThe proposed method is well-motivated and novel to me. \n2. The proposed method is easy to plug in fine-tuning or prompt pipeline.\n3.\tThe authors conduct experiments and show that the approach can help defend against backdoor attacks, with only a negligible generalization drop.\n\nWeaknesses\n1.\tThe empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.\n2.\tActually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.\n\n\\\nPlus, some technical details are not clear to me. See my questions below.\n\nQuestions:\n1. Should the probability ratio in Eq 4  be inside the $\\sum$? Or shall we use $w'$ inside the $\\sum$?\n2. For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch? \n3. The proposed method can help defend against backdoor attacks with only 1% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.\n4. What is the proposed soft template optimization for prompt? \n\n Given the points listed, I give the current rating here. It would be helpful if the authors can address my concerns.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "1grZVzxVBMD",
                "writer": "author",
                "reply_to": "Iaku8O8MGI4",
                "title": "Response to Reviewer pjTM",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n\n$\\textbf{Q1: [Non-parameterized way]}$ There may be other non-parameterized ways, such as information-entropy smoothed or neighbor cluster, in the repair of backdoored tokens.\n\n$\\textbf{A1:}$ Yes, we regard the information-entropy method or the neighbor cluster method as \"static\" methods, which may be helpful in the end-to-end paradigm. Especially when a trained model is prepared for deployment. Such a method can detect the backdoor tokens and then eliminate them to prevent further damage to the whole system. However, the \"static\" method may not be a good default for transfer learning, where the generalization ability matters most. Any static modification to the pre-trained model could reduce the generalization ability for certain downstream tasks. Also, we assume the pre-training objective is not a good helper for targeting the backdoor trigger for the training objective gap between fine-tuning and pre-training. \n\n----\n\n$\\textbf{Q2: [Backdoor-erasing methods in other domain]}$ Are all the current backdoor-erasing methods or backdoor-outline methods (mainly proposed in CV) not useful in the case of pre-trained language models?\n\n$\\textbf{A2:}$ We have surveyed recent popular backdoor-outline methods or backdoor-erasing methods in Section 2. Their limitations are obvious, mainly designed for end-to-end models, never taking the $\\textbf{inheritance of backdoor}$ into consideration. Also, the mainstream defense methods are designed for continuity input, not for the discrete input in text domain.\n\n----\n\n$\\textbf{Q3: [Variant of distilling?]}$ Can we view the GBA method as a variant of neural distillation or pruning?\n\n$\\textbf{A3:}$ For neural distillation, the core idea is to learn knowledge from a teacher model. The only teacher model in GBA is the model itself. GBA distills the \"normal\" gradient from the class tokens to the rare tokens which may be trigger candidates. To some extent, we may view GBA as a variant of self distillation. For pruning, the core idea is to cut the redundant or harmful part of the model. Instead of disabling the trigger tokens, GBA tends to repair the backdoored tokens while preserving the generalization ability of the original pre-trained model.\n\n----\n\n$\\textbf{Q4: [Performance gap]}$  Figure 2 shows a clean acc drop when increasing the data size of clean data from 0\\% to 1\\%. Does GBA hurt the model performance under few-shot settings more than the former distill-based method?\n\n$\\textbf{A4:}$ We assume the clean acc drop may reveal the influence of generalization ability. In Figure 2, we also observe the performance drop of the former distill-based method NAD. Compared with the negligible performance drop of our proposed GBA, NAD leads to more reduction of generalization ability under all data settings.\n\n----\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ue-VzpP-HmM",
                "writer": "author",
                "reply_to": "088ANqVY9Ka",
                "title": "Response to Reviewer gVmg",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n\n$\\textbf{Q1: [Experiments on more challenging models]}$ The empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.\n\n$\\textbf{A1:}$ Thanks for the constructive suggestion. We use the RoBERTa as the benchmark and explore more experiments in Appendix D.1. We find RoBERTa is more robust to backdoor attacks but still suffers from strong baselines like BadNets. And more importantly, the results also reveal our proposed GBA defense still works well on RoBERTa, reducing the ASR score to nearly zero.\n\n----\n\n$\\textbf{Q2: [The moderated improvement]}$ Actually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.\n\n$\\textbf{A2:}$ Firstly, we would apologize that we have misplaced the results in the previous submission. In the re-submitted paper, we have replaced the original NAD results with the NAD and NAD-C. The first is acquired based on NAD's original assumption, and the second is designed to fully show the NAD's potential performance on settings that are impossible to achieve in reality. Besides, the empirical results show that our proposed methods outperformed the original NAD method.\n\n----\n\n$\\textbf{Q3: [The correction for Eq.(4)]}$ Should the probability ratio in Eq 4 be inside the $\\sum$ ? Or shall we use $w'$ inside the $\\sum$?\n\n$\\textbf{A3:}$ Thanks for sharing your thoughts on this minor mistake. We should use $w'$ inside the $\\sum$. In Eq 4, $Q_{Ew}$ is computed for each token $w$ in the vocab with the average gradient of input example, which noted as $\\sum_{w' \\in x_{input}} \\frac{\\nabla_{Ew'}}{N}$.\n\n----\n\n$\\textbf{Q4: [Updating in the minibatch]}$  For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch?\n\n$\\textbf{A4:}$ Yes, the proposed method will update all the embeddings of words in vocab. The frequency of rare tokens in the batch is nearly zero, we will never update them enough if we just update words present in the current batch. This misunderstanding may come from the wrong presentation of Eq 4, and we have corrected it in this version.\n\n----\n\n$\\textbf{Q5: [The few-shot property]}$ The proposed method can help defend against backdoor attacks with only 1\\% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.\n\n$\\textbf{A5:}$ We include more ablation studies in Appendix D.2. Without the proposed gradient broadcast, the prompt tuning reaches good performance on clean data, which could be credited to the few-shot property of the prompt. However, it loses the defense capability of backdoor attacks. Please refer to that discussion.\n\n----\n\n$\\textbf{Q6: [Soft template optimization]}$  What is the proposed soft template optimization for prompt?\n\n$\\textbf{A6:}$ In traditional hard-coded template optimization, the template of the downstream task is hand-crafted, like \"<S1> It was [MASK] .\" for sentiment classification. <S1> is the input sentence and [MASK] is the target word like \"positive\" or \"negative\". This paradigm needs extensive computation to find the best hand-crafted template. Instead, we use a soft template to replace the hard-coded input sentence with several learnable tokens \"[a]*\". The Adam optimizer is hard to find the proper discrete tokens \"[a]*\". So we fill the template in the embedding-level instead of token-level (discussed in Section 3.3). We concatenate the embeddings of tokens \u201c[a]*\" with the input sentence and optimize the embeddings of the template token to find the best-match soft template that can maximize the downstream task's performance.  \n\n----\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3KgBygU9fk",
                "writer": "author",
                "reply_to": "6zHpX3Tk_lY",
                "title": "Response to Reviewer KqXG",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n$\\textbf{Q1:[ Assumption on more complicated tasks] }$ The approach seems to be based on the idea that the semantics of the tokens in the same sentence are similar. But this might not be true for more complicated tasks. Unfortunately, the experiments only consider tasks with only few classes, mostly two classes, and just one dataset with 4 classes. In this case, the task becomes learning if a word is related to the target class. More complicated tasks would show how this defense would perform in the real world.\n \n$\\textbf{A1:}$ We were deeply sorry for causing inappropriate understanding from the minor issue in Eq.(4). We have reformulated this equation in the re-submission. We design the gradient broadcast under the assumption that the gradient could be used to measure the semantic distance between rare tokens and the inputs, which motivates us to update the target embeddings of the rare tokens following the common tokens' gradients.\n\nIn this way, we assign an extra pulling force $Q_{Ew}$ to update rare tokens in vocab based on the semantic distance. We believe the proposed gradient broadcast method could be extended to more complicated tasks. As a concrete example, we include more results on the GLUE benchmark in Appendix D.2, and our method performs well on both NLI and regression tasks.\n\n----\n\n$\\textbf{Q2: [Questions on BadNets]}$  The description of the attacks used is not sufficient. While the core algorithms can be learned from the cited papers, the experiment setting doesn't explain how the triggers are chosen, what the portion of backdoored samples in the test dataset, or how they are constructed. Thus, it cannot be inferred if the evaluation was done fairly and properly. For example, BadNets (Gu et al., 2017) doesn't discuss poisoning of text models at all, and this necessary information cannot be found.\n\n$\\textbf{A2:}$ Yes, the BadNets are originally proposed and applied in the CV domain. We mimic the original implementation and find a way to apply it in the NLP domain. In this re-submission, we included more detailed descriptions of attacking and defense implementation in Appendix B.\n\n---- \n\n$\\textbf{Q3: [The paper writing]}$ The paper needs major improvement in writing. There are many errors (e.g., we usually takes, state-of-the-art, We are the first ... method, safely adaptation method, a method which do not ...), unnecessarily repeated sentences or words (e.g., all whole vocab, Sec 3.1), and missing explanations/definitions (e.g., V, $\\theta^*$). Missing information can be understood by a domain expert, but the paper should be self-contained as much as possible if not citing a prior work. Also, many statements are over-generalized.\n\n$\\textbf{A3:}$ Thanks for the constructive suggestion. We have revised the draft and rebuilt the paper in this submission. We will try to list all the improvements and revisions in the final review abstract if it is available.\n\n----\n\n$\\textbf{Q4: [How the gradient updating work]}$ If the trigger does not appear in the training data, the embedding still wouldn't be updated as Q is computed and applied per sentence according to Eq 4. So, it is unclear how this update is significantly different from just updating the token with its own gradient only. Comparing the gradient and Q can be interesting. It almost looks like the effect is using a larger learning rate, but there was not sufficient analysis on this.\n\n$\\textbf{A4:}$ Yes, this problem is related to the minor issue in Eq.(4). In the corrected formulation (of the re-submission version), we compute Q for each token in the vocab (including common tokens and rare tokens). Even if the rare tokens never appear in the input batch, the rare tokens' embeddings will also be updated with the pulling force Q. In Section 5.2 (of the re-submission version), we provide a more detailed analysis between Q and the standard gradient update and testify its effectiveness in erasing the triggers.\n\n----\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6zHpX3Tk_lY",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "Official Review of Paper2080 by Reviewer KqXG",
                "comment": "This paper proposes a defense against backdoor attack on pre-trained large language models. The proposed defense computes the average of the gradients per input sentence to contribute to updating all tokens in the sentence. The approach is empirically shown to outperform two baselines. Strengths.\n- S1: The simple proposed approach is shown to defend backdoor attack targeting rare tokens.\n- S2: The approach is evaluated on 5 datasets against 4 attacks.\n\nWeaknesses.\n- W1: The approach seems to be based on the idea that the semantics of the tokens in the same sentence are similar. But this might not be true for more complicated tasks. Unfortunately, the experiments only consider tasks with only few classes, mostly two classes, and just one dataset with 4 classes. In this case, the task becomes learning if a word is related to the target class. More complicated tasks would show how this defense would perform in the real world.\n- W2: The description of the attacks used is not sufficient. While the core algorithms can be learned from the cited papers, the experiment setting doesn't explain how the triggers are chosen, what the portion of backdoored samples in the test dataset, or how they are constructed. Thus, it cannot be inferred if the evaluation was done fairly and properly. For example, BadNets (Gu et al., 2017) doesn't discuss poisoning of text models at all, and this necessary information cannot be found.\n- W3: The paper needs major improvement in writing. There are many errors (e.g., we usually takes, state-of-the-art, We are the first ... method, safely adaptation method, a method which do not ...), unnecessarily repeated sentences or words (e.g., all whole vocab, Sec 3.1), and missing explanations/definitions (e.g., V, \\theta^*). Missing information can be understood by a domain expert, but the paper should be self-contained as much as possible if not citing a prior work. Also, many statements are over-generalized.\n- W4: If the trigger does not appear in the training data, the embedding still wouldn't be updated as Q is computed and applied per sentence according to Eq 4. So, it is unclear how this update is significantly different from just updating the token with its own gradient only. Comparing the gradient and Q can be interesting. It almost looks like the effect is using a larger learning rate, but there was not sufficient analysis on this. This paper proposes a simple defense against backdoor attacks on pre-trained language models. Aside from the large room to improve writing, this paper has many other issues. While the simplicity is fine, this paper failed to show the exact effect of the defense as the rare tokens would be still updated only when it appears in the training data. Also, the evaluation was done for similar tasks where words can be directly grouped into each class. Moreover, the exact poisoning procedure was not explained. Thus, it is not possible to confirm the benefit of the proposed approach.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "Iaku8O8MGI4",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "Official Review of Paper2080 by Reviewer pjTM",
                "comment": "This paper identifies an emerging threat for the prevailing pre-trained models -- the inheritance of backdoor attack, and proposes a simple yet effective defense approach: gradient broadcast adaptation (GBA). Instead of the traditional \u201cerasing triggers\u201d, GBA utilizes the \u201cprompt-tuning\u201d as a tool to guide the \u201cperturbed weights\u201d back to the normal state, which helps avoid the degradation of generalization ability. It provides an exciting and novel analysis of why backdoor attacks could be inherited during the pretraining and tuning procedure. Meanwhile, the authors perform an empirical evaluation of the proposed method against four state-of-the-art backdoor attacks. In the security of pre-trained models, the inheritance of backdoor (adapt backdoored models to various downstream tasks) is an important challenge. Injected with the backdoor, the existing model could be further developed for a long life-circle with backdoor survival. This paper examines this interesting and under-explored topic. It locates the most vulnerable component of pre-trained language models \u2013 word embeddings, which is the reason for the ever-lasting backdoor. It shows that by carefully optimizing the word embeddings, and the malicious backdoor could be erased or repaired, taking no effect on further adaptation.\n\nBy taking inspiration from the \u201cerasing backdoor\u201d viewpoint of \u201cNeural Distillation\u201d prior work, it paves a new way of defense instead of \u201cabandon redundant weights\u201d, which obeys the generalization purpose of pre-training. The tool of \u201cprompt-tuning\u201d is also more natural to pre-trained models.\n\nThe paper proposes a much more intuitive optimization strategy that guides \u201cthose perturbed weights\u201d back to the normal state by joining the adaptation stage, thus erasing backdoors while preserving the generalization ability of pre-trained models. Additionally, as a plugin of the optimizer, the usage scenarios of the proposed approach are unlimited.\n\nExperiments show that just by focusing on the word embeddings, one can disable nearly all backdoor attacks. I suggest the author consider this as a \u201cfirewall\u201d to the standard adaptation operations.\n\nComments:\n\n1. The paper is well organized, and the motivation is clearly written.\n2. The authors try to unleash a wave of security concerns in pre-trained models. Many existing works focus on improving the performance of pre-training and adaptation while neglecting the threats of backdoor attacks/adversarial attacks. Following this paper's problem setting and experiment results, it may be easy to plug GBA into the current pipeline. One of the benefits is that we do not need to worry about performance degradation or computation efficiency.\n3. After reading this paper, I can hardly think about a better alternative for the proposed GBA approach. To the best of my knowledge, the root of backdoor attack inheritance only could be the lazy update of rare tokens. This is consistent with the authors\u2019 claim. However, there may be other non-parameterized ways, such as information-entropy smoothed or neighbor cluster, in the repair of backdoored tokens. I am wondering if it can work, but it deserves a trial. I also have some additional questions to discuss with the authors:\nAre all the current backdoor-erasing methods or backdoor-outline methods (mainly proposed in CV) not useful in the case of pre-trained language models?\nCan we view the GBA method as a variant of neural distillation or pruning? So if I were to turn this into a method that outlines the triggers first and then erases them in a blacklist.\nFigure 2 shows a clean acc drop when increasing the data size of clean data from 0% to 1%. Does GBA hurt the model performance under few-shot settings more than the former distill-based method?\n This paper targets an under-explored fundamental challenge in pre-training and adaptation trends. This is a brave and valuable step in the security research of pre-trained models. I enjoy reading this paper, and this is the first time I have seen a reasonable solution to the inheritance of the backdoor phenomenon. Although some problems need to be corrected, I believe authors should be able to take them in hand.",
                "rating": 8,
                "confidence": 4
            }
        ],
        "label": "val"
    },
    "6lH8nkwKRXV": {
        "paper_id": "iclr_2021_6lH8nkwKRXV",
        "paper_title": "Graph Structural Aggregation for Explainable Learning",
        "paper_abstract": "Graph neural networks have proven to be very efficient to solve several tasks in graphs such as node classification or link prediction. These algorithms that operate by propagating information from vertices to their neighbors allow one to build node embeddings that contain local information. In order to use graph neural networks for graph classification, node embeddings must be aggregated to obtain a graph representation able to discriminate among different graphs (of possibly various sizes). Moreover, in analogy to neural networks for image classification, there is a need for explainability regarding the features that are selected in the graph classification process. To this end, we introduce StructAgg, a simple yet effective aggregation process based on the identification of structural roles for nodes in graphs that we use to create an end-to-end model. Through extensive experiments we show that this architecture can compete with state-of-the-art methods. We show how this aggregation step allows us to cluster together nodes that have comparable structural roles and how these roles provide explainability to this neural network model.\n      ",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing:\ni)  a simple to implement (local) node aggregation approach;\nii) some theoretical support to the proposed approach;\niii) empirical evidence that the proposed approach could be effective.\n\nNotwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact:\ni) reference to literature is missing some important recent contributions to the addressed problem (e.g.  Gated Graph Sequence Neural Networks, GNNExplainer);  if possibile, also experimental comparisons vs those approaches is desirable;\nii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance;\niii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method.\n\nIn summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "J333isNg_89",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "Review comments to Paper 641",
                "comment": "==========Summary==========\n\nIn this paper, the authors investigate how to improve pooling functions in graph neural networks for the purpose of better addressing graph classification problems. The core idea in this paper is to group node representations. In particular, the authors develop StructAgg, a softmax based implementation, to parameterize the grouping process, and the parameters in StructAgg can be learned from downstream supervision signals. Empirical results focus on three aspects: (1) Improvement in graph classification accuracy brought by StructAgg; (2) Visualization on grouped node representations; and (3) Comparison between two variants of StructAgg.\n\n==========Reason for the rating==========\n\nFor the current draft, I am leaning to reject. Although pooling for graph classification is a meaningful problem, it is difficult to see the unique perspective or value in the proposed technique compared with existing approaches. For now, the technical depth in this paper seems to be limited, and more convincing empirical evidences are expected.   \n\n==========Strong points==========\n1. This paper delves into a meaningful problem. Indeed, pooling functions in graph neural networks are critical for graph classification tasks. While existing solutions are simple and efficient, they may be sub-optimal in some cases.\n\n2. The authors propose a grouping-based method to reduce variance in graph representations, which potentially could improve generalization performance.\n\n3. Empirical evidences are provided to confirm the effectiveness and impact from StructAgg.\n\n==========Weak points==========\n1. The idea of \"grouping node representations using softmax for graph classification\" may have been investigated in existing works, e.g., [1]. For the core idea in this paper, the authors may need to discuss its connection with existing literature, and highlight the unique perspective.\n\n2. The technical impact of this paper could be limited. \n    - From the current draft, the proposed StructAgg looks like incremental changes to the existing methods. The authors may provide more theoretical or empirical evidences to motivate the problem, highlight the uniqueness in the technique, and justify their design choices.\n    - The theoretical results in Theorem 1 may need more work. Theorem 1 could aim to answer an important question in StructAgg, but the authors may raise the question first and justify why this is an important/non-trivial question. \n\n3. The empirical evidences could be stronger. \n    - From the current results, StructAgg only performs the best in one of the three datasets. The authors may consider more datasets that can highlight the value of StructAgg.\n    - StructAgg may work with the existing GNNs. The authors may connect StructAgg with the existing GNNs, and demonstrate the incremental gain from StructAgg.\n    - For the evaluation in Table 2, it is unclear why the comparison is limited between GCN and the proposed technique.\n    - For the evaluation in Table 3, the claim on \"node embedding\" may not be sound. Between StructAgg and StructHist, the difference is not just \"node embedding\", and one cannot ignore the impact from \"histogram\". The authors may need to carefully reason the conclusion from the empirical numbers.\n\n==========Questions during rebuttal period==========\n\nPlease address and clarify the weak points above. \n\nIn addition, it will be great if the authors could address the following questions.\n\nQ1. As the authors emphasize the term \"node embedding\", does this paper target on transductive or inductive settings? \n\nQ2. For the entropy minimization discussed in Section 3.3, is it going to bring more overfitting risk?\n\nQ3. For the first condition in Definition 1, it is a bit vague. The authors may give a more accurate mathematical description.\n\nQ4. For the first bullet in Remarks under 3.3, what does \"The structural classes identified by our algorithm are local\" exactly mean?\n\nQ5. In the experiment setup, what do $l_1$ and $l_2$ refer to?\n\n==========Reference==========\n\n[1] Substructure Assembling Network for Graph Classification, AAAI 2018\n\n==========Post rebuttal==========\n\nThe authors' response does not fully address my concerns. I keep the rating as it is. ",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "NREjcrPTEk",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "StructAgg: an ultra-high dimensional node representation in graph neural network that preserves local graph structure for better explanation",
                "comment": "This paper proposed the StructAgg, an aggregation algorithm in convolutional graph neural network that learns the structural roles for nodes in the graph embedding. In this algorithm, a structural representation of node is constructed through concatenation of latest p layers of node presentation in graph neural network, which consists of information from the p-hop neighborhood. The paper is good quality and its demonstration is clear.  The idea behind is original. However, learning embeddings through concatenation of multiple layers of neural representation is not completely new. The contribution of this paper to the community is not ground-breaking as well.  From the experiment results, it is hard to stress its significance as in most of times this method does not beat the state-of-the-art algorithms. \n\nPros: \n1.  As opposed to previous studies, this paper focus on learning node embeddings that preserve local information in a larger p-hop neighborhood. Such idea assumes that the node structural role can be identified via its neighborhood and the node embeddings should carry information regarding its structural roles. To achieve that, the learning algorithms should balance the predictability of the embedding in the node classification as well as the consistency of node representation towards underlying clustering in the graph. \n2.  The proposed representation learned by StructAgg, is able to provide the clustering of nodes as well as the node embeddings that are invariant to the permutation. \n3.  The StructAgg only need to concatenate the existing learned representation in lower layers, thus it is easy to implement. \n\nCons: \n1.  Ulta-high dimensional representation as the size of local neighborhood of interest increases: The concatenation operation expands the dimensionality of embeddings drastically as the local neigborhood expands. As a result, the StructAgg representation can only feasibly represent a structures that are present in small neighborhood graph. In some graph like trees, it is fine. But in complex graphs, with larger local structures, it is not efficient in both storage and computation. \n2. Tradeoff between the classification and clustering:  The proposed method defines the loss function as the combination of supervised node classification loss and unsupervised node cluster assignment loss (in terms of entropy of softmax assignment). The formation of this joint loss function implies that the node label is assigned consistently according to the structural role of the node in the graph with the number of total structural classes in the graph known. It can be imagined that with the number of structural classes increases, the label assignment may not be consistent and will result in decrease of performance. It seems that this method is more suitable for two or three structural classes as shown in the experiment. \n3.  Lack of robustness: The message passing algorithm in small neighborhood tends to preserve not just local information and local noises. In StructAgg, since the aggregation happens only within the same cluster, the noise level in different clusters may varies a lot, depending on the size of cluster. It is more likely to create noisy embeddings in this small clusters and present them in the final embeddings, while in traditional aggregation, such noise level is suppressed due to the involvement of more nodes.\n\nSome question need to clarify:\n1.  In proof of theorem 1, please explain for $\\sum_{i^{'}\\in \\mathcal{N}(i)}\\frac{a_{i,i^{'}}}{\\sqrt{d_{i}d_{i^{'}}}}X_{i^{'}}^{(l)} = \\sum_{j^{'}\\in \\mathcal{N}(j)}\\frac{a_{j,j^{'}}}{\\sqrt{d_{j}d_{j^{'}}}}X_{\\psi(j^{'})}^{(l)}$, what is the relation between coefficients $\\frac{a_{i,i^{'}}}{\\sqrt{d_{i}d_{i^{'}}}}$ and $\\frac{a_{j,j^{'}}}{\\sqrt{d_{j}d_{j^{'}}}}$. Also should it be like $\\sum_{j^{'}\\in \\mathcal{N}(j)}\\frac{a_{\\psi(j),\\psi(j^{'})}}{\\sqrt{d_{\\psi(j)}d_{\\psi(j^{'})}}}X_{\\psi(j^{'})}^{(l)}$ ?\n\n2. In proof of proposition 2, please explain in detail the reason behind $(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}PP^{T}PX^{L}$. Here i believe it should be $(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}P^{T}PX^{L}$, with a mistakenly added $P$ in original draft.  The reason is that the softmax is row operation, a row permutation $P^{T}$ does not affect the result of $softmax(\\cdot)$ at each row but their ordering. So  $softmax(P^{T}\\cdot)= P^{T}softmax(\\cdot)$, thus $(softmax(P^{T}PX_{struct}p)) = (P^{T}softmax(PX_{struct}p))$. This leaves \n$(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}P^{T}PX^{L} \n= (softmax(PX_{struct}p))^{T}PP^{T}PX^{L}  = (softmax(PX_{struct}p))^{T}PX^{L} $",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "LMlllUkBIN",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "This work tries to identify and explore local topological roles in explainable graph learning. Writing needs improvement. Experimental results not conclusive.",
                "comment": "This work tries to identify  local patterns that can be used to provide explainability to GNN models. Different from previous work using simple pooling function to generate graph representation, this work clusters nodes into a predefined number of clusters using their embeddings from L-hop neighbors. Each cluster produces a pooling representation. The representations of all clusters are concatenated to form the representation of the whole graph. However, the manuscript was not well written with some confusing notations and logical problems. It should be beneficial to readers if the manuscript can first clearly demonstrate problems in existing graph classification problems. Some analyses are not solid. The proposed method performed obviously worse than several baseline methods.\n\nThis work assumes that similar node embeddings should have similar structural roles and thus should be clustered together. Nodes with the same local structure may have the same embeddings, but not the other way around may not be true. The structural roles are defined is based on embedding similarity, which doesn\u2019t guarantee to enhance explainability. The author(s) claimed that most GNNs were not explainable. This is not quite true. Recently, the attention mechanism was used in GNNs to enhance explainability. There exist some other works, like GNNExplainer, aiming at explainable GNNs.\n\nThe baseline models compared in this paper were proposed several years ago and not mainly focused on graph readout function. More typical methods for global pooling were missing in comparison. For instance, GlobalAttention (https://arxiv.org/abs/1511.05493) is also an explainable readout function. ASAPooling (https://arxiv.org/abs/1911.07979) is a recent work that also learns a sparse soft cluster assignment for nodes in the pooling phase. In addition, the proposed method performed worse than many baseline models in many cases. For experiments on OGB datasets, it only compares with original GCN model, but there has been a lot of state-of-art models proposed several month ago, achieving much better results than this paper. You can find the latest results on https://ogb.stanford.edu/docs/leader_graphprop/. \n\n\nSome notations need improvements.\n1.\tEquation (2): The trainable weight matrix is missing in GCN(A, X).\n2.\tDefinition 1 denotes two nodes as u and v, but the notation becomes i and j when representing their l-hop neighbors. X is first used as the feature matrix but later used to indicate node vector.\n\nSome claims are not correct.\n1.\tIn Section 3.2, \u201cEmbedding nodes with MP creates embeddings that are close for nodes that are structurally equivalent.\u201d When many GCN layers are stacked together, final node embeddings tend to become similar.\n2.\t \u201cWhen computing the distance between two graphs, if those two graphs have the same distribution of structural roles, they will have embeddings that are close. \u201d This may be true, but the overall graph topology information is omitted. Here structural roles represent local patterns. \n",
                "rating": 3,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "A4DqtLXz4-",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "Official Blind Review #3",
                "comment": "This paper focuses on deriving explainable features for use in graph classification. To that end, they propose StructAgg that is essentially an aggregation process based on the structural roles of nodes that is then used in an end-to-end model. Experiments demonstrate the effectiveness of the proposed approach as it provides comparable performance while providing some explainability. This is an important unsolved problem, and this work provides one such approach to obtain more explainable and intuitive features/embeddings for graph classification.\n\nThe example provided in Figure 1 (a) is trivial in the sense that one can assign structural roles to nodes based on degree. For instance, red nodes all have degree 1, orange nodes have degree 3, and yellow nodes have degree 2. Please provide a better example such as the one used in the recent survey paper \u201cOn Proximity and Structural Role-based Embeddings in Networks\u201d (see Figure 1) where this is not the case, and one must look at the actual structural properties to discover roles. Also, I suggest showing actual meaningful features in (b) as this would make the example more complete. There are a few papers related to structural roles that are missing and should be referenced appropriately. For instance, using roles for graph similarity and comparison is discussed in \u201cRole Discovery in Networks\u201d. There are also incorrect references to a paper from 2018 for structural roles that need to be fixed. Overall, this paper is interesting, the problem is important, and the approach is shown to be effective.\n",
                "rating": 7,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "train"
    },
    "VAeAUWHNrty": {
        "paper_id": "nips_2022_VAeAUWHNrty",
        "paper_title": "Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising",
        "paper_abstract": "Recent advances in differentiable rendering have enabled high-quality reconstruc\u0002tion of 3D scenes from multi-view images. Most methods rely on simple rendering algorithms: pre-filtered direct lighting or learned representations of irradiance. We show that a more realistic shading model, incorporating ray tracing and Monte Carlo integration, substantially improves decomposition into shape, materials & lighting. Unfortunately, Monte Carlo integration provides estimates with significant noise, even at large sample counts, which makes gradient-based inverse rendering very challenging. To address this, we incorporate multiple importance sampling and denoising in a novel inverse rendering pipeline. This improves convergence and enables gradient-based optimization at low sample counts. We present an efficient method to jointly reconstruct geometry (explicit triangle meshes), materials, and lighting, which substantially improves material and light separation compared to previous work. We argue that denoising can become an integral part of high quality inverse rendering pipelines.",
        "paper_acceptance": "Accept",
        "meta_review": "The paper addresses the task of reconstructing 3D meshes, materials and lighting from multi-view images. To this end Monte Carlo ray tracing is used in combination with denoisers for training efficiency. Experiments show improvement w.r.t. SOTA nvdiffrec and nerfactor.\nReviewers like the combination of denoisers for training and MC rays tracing, the thorough evaluation and ablation studies. All reviewers recommend the paper for acceptance and so do I.",
        "meta_review_title": "Meta Review of Paper8741 by Area Chair Kibw",
        "reviews": [
            {
                "review_id": "AB7EbZciig",
                "writer": "author",
                "reply_to": "1nSmiGmD5m",
                "title": "1u76 individual questions",
                "comment": " ## **Evaluations primarily focus on novel rendering/relighting but lacks in individual intrinsic components such as 3D geometry (e.g. depth, chamfer, normal errors etc) and environment maps**\n\nPlease see the common section for additional evaluations.\n\n## **Comparison with recent Monte Carlo inverse rendering method [31,37] especially [37] would strengthen this paper**\n\nAs mentioned, [31] does not optimize geometry, and [37] assumes known lighting (a point light co-located with the camera).\nThe visibility gradients of [31] require efficient silhouette detection, which is non-trivial to extend beyond\nprimary visibility (requires complex 5D data structures for secondary rays, see e.g., \"Unbiased Warped-Area Sampling for Differentiable Rendering\" for a detailed discussion).\n\nOur core contribution is the joint optimization of shape, materials, and environment lighting using a Monte-Carlo renderer.\nTo get a tractable optimization task (in terms of iteration times and noise levels), we restricted the renderer to direct illumination and added (differentiable) denoising to the pipeline.\nWhile there is currently no Monte Carlo path tracing system that tackles the same joint optimization task,\nthe variance-reduction ideas introduced in this paper can be directly applied to future MC inverse rendering pipelines, once the computational resources are available.\n\n## **I wonder to what extent the proposed method could be used to handle inter-reflections and light refraction in e.g. translucent objects**\n\nWhile the paper focuses on direct illumination, we discuss extensions to full path tracing briefly in the Conclusion section. It is a clear avenue for future work, but comes with additional challenges in\nincreased noise-levels, visibility gradients through specular chains, and drastically increased iteration times. The denoising step is applicable as is. Please see common section for further details.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K_GwdogxRQZ",
                "writer": "author",
                "reply_to": "3tewi_Yke7p",
                "title": "EEfj individual questions",
                "comment": " ## **The choice of the importance sampling techniques is given with any discussion**\n\nThe three importance sampling techniques we apply are well-proven in the computer graphics community, and applied in most modern production path tracers. Thus, we kept discussion very brief and cited the relevant methods.\nOn request, we can add a more detailed summary of each sampling technique to the supplemental material to make the paper more self-contained.\nApplying these strategies in an inverse rendering pipeline is less explored. We use detached versions of these samplers.\nRecent work by Zeltner et al. [69] studies this problem in detail, and it is a nascent, active research field.\n\n## **How much the work is coupled with NVDIFFREC. Could the suggested techniques be incorporated into methods like PhySG [1] as well?**\n\nOur denoising step works in image space and is directly compatible with any renderer. Similarly, our sampling works with any geometric representation supporting ray tracing (e.g. triangles, SDFs, ...).\nPhySG uses sphere tracing to evaluate the shape (SDF) and Spherical Gaussians to evaluate shading, w/o shadows.\nReplacing the PhySG renderer with a MC tracer was recently done in the paper \"Differentiable Signed Distance Function Rendering\" by Vicini et al.\nIt drastically increases the noise levels compared to low-frequency SG representation of shading, which suggests that our sampling and denoising strategies could be helpful. Vicini et al. did not include results where the environment lighting is jointly optimized with shape and materials.\n\n## **The discussion on the bias introduced (154-162) is unclear. The seems like an important issue that can also promote further research but is very shortly discussed**\n\nThe variance-bias trade-off is indeed a very important topic. We purposely used biased rendering in our pipeline to get a tractable\noptimization problem. Note also that all denoisers inevitably introduce bias.\nWe discuss it a bit further in the supplemental material (Fig 6, Section 4, Fig 10).\n\n## **The effect of the new regularizer is not tested in an ablation study**\n\nThough not a full ablation, please refer to Fig 1 in supplemental material. We additionally show a full breakdown of the NeRF dataset in the newly added Figure 18 in supplemental material.\n\nNote that the regularizer is particularly well suited for scenes with more complex geometry and self-shadowing, such as Hotdog and Lego.\nThe diffuse lighting encodes most of the shadows, while the Kd/Albedo term contains mostly chrominance, as expected.\nOur regularizer is less suited for scenes with simple geometry and complex materials. We still note that the albedo textures look somewhat improved, \ne.g., for the Materials scene. The Chair scene can be considered a failure case, as some material patterns are baked into lighting, still the material parameters look reasonable.\n\nRecent work on image delighting and shadow removal through neural networks is a promising alternative. While such methods will likely win in the long term,\nthey did not perform well enough on our datasets in our initial experiments.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QiL3qb7ok-s",
                "writer": "author",
                "reply_to": "PAv7OQgQ1Sh",
                "title": "JQ7r individual questions",
                "comment": " ## **The reconstruction accuracy of individual attributes (such as normal or specular components) was not quantitatively evaluated**\n\nPlease see the common section.\n\n## **Therefore, it is very helpful to clarify which kinds of scenes the existing method is not good at and the proposed method is good at handling so that a user can choose the proper method depending on the scene**\n\nIf material & light separation is important, we argue that this approach is preferable (by tracing shadow\nrays, we avoid shadows getting baked into the albedo texture). If view interpolation is the end goal, then NeRF does an excellent job.\n\n## **What does \u201cscale linearly\u201d mean?**\n\nThe computational cost of evaluating shading increases linearly with the number of samples (not counting denoising and geometry evaluation, which is a fixed cost, regardless of sample count). We use GPU ray tracing and measured the overall runtime of the system.  \n\n## **Can the proposed method be applied to the recovery of translucent or transparent objects with some realistic constraints (e.g., known background)?**\n\nIt can likely be extended to support translucency and transparency, but we have left that for future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kp32RWrab_y",
                "writer": "author",
                "reply_to": "IkiMWlRRDvO",
                "title": "Hmte individual questions",
                "comment": " ## **Q1: Multi-bounce**\n\u200b\nPlease see the common section.\n\n## **Q 2 & 4: Light leakage / How robust is the denoiser in early optimization**\n\u200b\nNote that the light leakage term is not strongly related to shadow (silhouettes/boundary) gradients, but is rather a trick to avoid gradient discontinuities in early training, similar to the denoiser footprint schedule.\n\nThe images indeed do not make much sense in early training, and large filters / strong shadows can cause geometry optimization (in particular) to get stuck in a local minima.\nE.g., optimization may fail to carve out geometry, because a strong shadow will temporarily be added until neighboring geometry is removed.\nWe schedule light leakage and filter footprint in tandem, with a simple linear ramp over the first 1750 iterations of the first optimization pass:\\\n$\\mathrm{light\\_leak} = \\max(0, 1 - i / 1750)$, \\\n$\\sigma = \\sigma_{\\mathrm{max}} * \\min(1, i / 1750)$, \\\nwhere $i$ is the iteration number. We used the *exact same schedule* for all scenes in the paper.\nGiven the large diversity of geometric complexity in our scenes, we consider it robust.\n\n## **Q3: Shadow gradients**\n\u200b\nNote that we did not use visibility gradients for shadow rays in the main paper, as we didn't see a clear benefit in our multi-view setting (as discussed in Section 3.1). For the ablation in the supplemental, we used an extension of the nvdiffrast AA test, extended to 3D, to compute shadow gradients, and we expect similar results using other recent gradients options (warped area/edge sampling). A limited evaluation with visibility gradients based on Warped-Area Sampling indicates the same behavior.  \n\u200b\n## **Q5: During testing, can we get high-quality noise-free results by increasing the number of samples without using denoisers?**\n\u200b\nPlease see the common section.\n\n## **Q6: The paper says view interpolation degrades with improved material and lighting estimations. More materials/evidence are needed to support this claim**\n\u200b\nOur claim is too strong and we will reformulate it. We enforce material/light separation through additional regularization. Without the regularizer loss terms we would optimize for the same (PSNR) image loss used for validation, which should yield a better view interpolation result assuming optimization finds the global minimum. Compared to NeRF, all previous work which provides material separation (NeRD, NeRFactor, nvdiffrec, PhySG) reduces quality, but there is no strict correlation between improved material separation and decreased view interpolation quality.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9hUbeFhfKW7",
                "writer": "author",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Common section",
                "comment": " We thank all the reviewers for the great feedback. In this section, we address some common concerns. Please also refer to the individual responses. To help other researchers reproduce our results, we intend to release source code upon acceptance.\n\n## **Multi-bounce results**\n\nOur paper presents results on direct illumination (environment lighting with shadows). As discussed in our conclusions, multi-bounce\npath tracing is a clear (and exciting) avenue for future work, but comes with additional challenges in increased noise-levels, visibility gradients through specular chains, and drastically increased iteration times.\n\nWe leverage importance sampling and denoising techniques from production path tracers, and argue that\nthese techniques will be important building blocks for future high quality inverse rendering pipelines,\ne.g., the denoising step is applicable as is.\nOur approach is a small step towards multi-bounce MC inverse rendering pipelines.  \n\nFurthermore, neural materials pose an engineering challenge. To make optimization tractable in terms of memory consumption, we need to apply \"path replay\",\n(see \"Path Replay Backpropagation: Differentiating Light Paths using Constant Memory and Linear Time\" by Vicini et al. for details) which require fusing the path tracing and MLP kernels, which is non-trivial in the tinycudann system we currently apply for neural material evaluation.\n\n## **Additional terms in evaluation, e.g. depth, chamfer, normal errors, environment maps**\n\nWe present additional quantitative results below for individual terms. Please also refer to Figure 8 in the paper, where we show a visual breakdown of material parameters and normals compared to nvdiffrec for three scenes. We also added visual results to the supplemental material (Fig 17-18)\nto show the geometry quality and regularizer impact.\n\n### **Albedo texture**\n\nAs noted by NeRFactor & nvdiffrec, there is an indeterminable scaling constant per (R, G, B) value between texture albedo and the environment light (bright material and dim light or vice versa), which makes quantitative evaluation of textured albedo quality challenging.\nThis constant can often completely dominate the error. To sidestep this issue, following NeRFactor and nvdiffrec, in the table below we normalize albedo by the average intensity of the reference albedo.\n\nNeRFactor dataset:\n| PSNR (dB)       |  Drums |  Ficus | Hotdog |   Lego |\n|-----------------|--------|--------|--------|--------|\n| nvdiffrec       |   20.7 |   31.6 |   22.8 |   19.1 |\n| Our             |   20.9 |   32.8 |   22.2 |   22.3 |\n\nNeRF dataset:\n| PSNR (dB)       |  Chair | Hotdog |   Lego |    Mic |\n|-----------------|--------|--------|--------|--------|\n| nvdiffrec       |   24.2 |   18.9 |   19.0 |   28.3 |\n| Our             |   25.5 |   18.3 |   21.2 |   27.1 |\n\nThe aforementioned scaling factors make traditional image metrics a poor measure of material quality.\nIn the newly added Figure 18 in supplemental material we show a material component breakdown for the NeRF scenes.\nWhile the PSNR albedo scores for the Hotdog scene are relatively similar, we argue that our desaturated albedo would\nbe easier for an artist to cleanup/edit.\n\n### **Normal**\n\nWe evaluate normal quality in image space based on the rendered normal g-buffer (as shown in Figure 8). For the NeRFactor dataset\nwe note minor differences in normal quality compared to nvdiffrec.\n\nNeRFactor dataset\n| PSNR (dB)       |  Drums |  Ficus | Hotdog |   Lego |\n|-----------------|--------|--------|--------|--------|\n| NVDIFFREC       |   20.0 |   24.8 |   20.9 |   14.5 |\n| Our             |   19.9 |   24.6 |   20.8 |   15.0 |\n\n### **Chamfer loss**\n\nIn the table below we relate our method to Table 8 (supplemental) of the nvdiffrec paper, and note similar geometric quality as nvdiffrec.\nThe Lego scene is an outlier caused by our normal smoothness regularizer. The scene is particularly challenging for geometric smoothing\nsince it contains very complex geometry full of holes and sharp edges.\n\n|                 |  Chair | Hotdog |   Lego |  Mats. |   Mic |\n|-----------------|--------|--------|--------|--------|-------|\n| PhySG           | 0.1341 | 0.2420 | 0.2592 |    N/A | 0.2712|\n| NeRF (w/o mask) | 0.0185 | 4.6010 | 0.0184 | 0.0057 | 0.0124|\n| NeRF (w/ mask)  | 0.0435 | 0.0436 | 0.0201 | 0.0082 | 0.0122|\n| nvdiffrec       | 0.0574 | 0.0272 | 0.0267 | 0.0180 | 0.0098|\n| Our             | 0.0566 | 0.0297 | 0.0583 | 0.0162 | 0.0151|\n\nPlease refer to Figure 17 in the updated supplemental work for a visual comparison.\n\n## **During testing, can we get high-quality noise-free results by increasing the number of samples without using denoisers?**\n\u200b\nWe will update the paper to be more clear. At test time, all view interpolation results are generated **without** denoising at high sample counts. All relighting results are rendered in Blender with moderate-to-high sample counts and using Blenders denoising algorithm (which is different from ours).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IkiMWlRRDvO",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Official Review of Paper8741 by Reviewer Hmte",
                "comment": " The paper proposes a method to estimate the geometry, material and lighting \nof an object from a set of multi-view images. It builds on top of previous work \nnvdiffrec. Different from nvdiffrec that applies split-sum approximation for \ndirect lighting, the proposed method applies Monte Carlo ray tracing  and make the rendering more physically accurate. To reduce the \nnoise caused by Monte Carlo integration when\nthere are few samples, the paper proposed to use denoisers to reduce the\nvariance and make the training more efficient. The experiment results show that \nthe proposed method generates better results than baseline methods such as\nnvdiffrec and nerfactor.\n Strengths:\n\nThe paper combines neural inverse rendering and traditional Monte Carlo ray\ntracing, which enjoys the benefits of both world.\nThe idea of using denoisers to reduce variance in Monte Carlo integration \nis interesting and helps make the training more efficient. The paper did\nmultiple experiments to show that it helps improve material, geometry and \nlighting estimation.\n\nWeakness/Questions:\n1. As mentioned in the paper, while using Monte Carlo integration, the paper \ndidn't consider multi-bounces or shadow gradients, which makes the method fail\nto handle indirect illumination. As a stress test, it would be interesting to\nsee how it works when increasing the number of bounces.\n\n2. For shadow gradients, how the light leakage term is set? Is it a fixed\nparameter for all experiments? \n\n3. In Section 3 of the supp, it's not clear to me how the visibility gradients\nare calculated?  Is the paper using the methods based on warped area/edge sampling? \nIt's worth adding a more detailed discussion here to differentiate the referred\nmethods and the method used by the paper.\n\n4. How robust is the denoiser in the optimization? At the early stage of the\noptimization, when the rendering images don't make too much sense, will denoiser\nmake the optimization worse? The paper talks about ramping up the spatial\nfootprint and linearly blending the noisy and denoised images in supp (Line 60), \nhow robust are those operations? Do they need to be fine-tuned for each scene?\n\n5. During optimization, since it minimizes the difference between the denoised\nimages and the GT images, I am wondering whether it's possible that the noisy \nimages have some artifacts that are not caused by low samples but incorrect\nestimations. Such artifacts will be covered by the denoiser, and will be visible\nduring testing. Is such a case possible? During testing, can we get high-quality\nnoise-free results by increasing the number of samples without using denoisers? \n\n\n6. In Line 266, the paper says view interpolation degrades with improved\nmaterial and lighting estimations. More materials/evidence are needed to support this\nclaim. \n\nOverall, I think the idea of combining neural inverse rendering, traditional \nMonte Carlo integration and denoisers is interesting and novel. The paper did \nthorough evaluations on the method and performs detailed ablation study. \nTherefore, I vote for accepting the paper.\n\n See above. The limitations are well discussed.",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "PAv7OQgQ1Sh",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Official Review of Paper8741 by Reviewer JQ7r",
                "comment": " This paper presents an algorithm for the inverse rendering based on the neural fields. Unlike existing neural-fields-based inverse rendering relies on the simple shading model, the paper introduced the ray tracing with Monte Carlo integration that enables the inverse rendering more general and practical. To make the optimization tractable, the paper proposed to introduce multiple importance sampling and denoising for the differentiable ray tracing. The extensive evaluation shows that the proposed method can accurately recover physical attributes in more physically interpretable manner. Strength:\n\n- The introduction of this work is very clear which is supported by an extensive survey of previous studies. \n\n- Utilizing the denoiser and importance sampling for stable and efficient differentiable rendering makes a lot of sense. I expect that this research will lead the computer vision community the active utilization of real-time rendering technics, which are being actively researched and developed in the field of computer graphics.\n\n- Though most of the techniques for forward-rendering in this paper came from previous works, but their application to inverse rendering is not obvious. I think the beauty of this paper is that in every technical component, the reader can learn why it was introduced, including its surroundings thanks to the proper citations.\n\n- The evaluation successfully demonstrated the benefit of the proposed denoising.\n\n- The extensive ablation study justifies the algorithmic choice. It is really helpful to me to see the comparison of denoisers with detailed discussions in the supplementary material.\n\nWeakness:\n\n- As described in the paper, the method still considers the single bounce of light, therefore essentially inter-reflection is still a problem.\n\n- Though qualitatively demonstrated, the reconstruction accuracy of individual attributes (such as normal or specular components) was not quantitatively evaluated.\n\n- As the authors repeatedly mentioned, the training time is not as fast as expected even with the small sample count per pixel.\n\n- Experiments show that the proposed method does a better job of recovering materials and lighting than the existing methods, but on the other hand, the increase in computational cost is an issue. Therefore, it is very helpful to clarify which kinds of scenes the existing method is not good at and the proposed method is good at handling so that a user can choose the proper method depending on the scene.\n\n- The writing of the paper is very good, and the proposed method is undoubtedly very effective in recent inverse rendering based on Neural Fields. On the other hand, however, I had the impression that the paper uses existing techniques at a very high level rather than making innovative proposals, so I have slightly downgraded it from the highest evaluation. - The paper claimed that the proposed method scales linearly with sample count but the Table at line 275 shows the iteration time doesn\u2019t linear to the sample count. What does \u201cscale linearly\u201d mean?\n\n- Can the proposed method be applied to the recovery of translucent or transparent objects with some realistic constraints (e.g., known background)?\n The limitation is properly described.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "3tewi_Yke7p",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Official Review of Paper8741 by Reviewer EEfj",
                "comment": " The paper tackles the challenge of learning the shape, light & material properties of a scene from multiview images. The main idea is to consider a general lighting model for the rendering equation, which in turn is approximated using Monte Carlo (MC). As MC introduces large variance gradient estimates for optimization, the paper suggests importance sampling techniques. In addition, an image denoising model is incorporated into the system. The method is evaluated on standard scene datasets. Strengths\n-------------\nThe paper is well written and easy to follow. The introduction is concisely informative.\nThe suggested system seems to improve previous work in learning scene properties decomposition.\nThe evaluation of synthetic data and real data is adequate.\n\nWeaknesses\n----------------\n\nThe paper focuses on the different solutions the proposed system consists of. However, I am missing a discussion on some of the challenges or design choices made in designing such a system. For example, the choice of the importance sampling techniques is given with any discussion. Some questions that arise are: Can the variance reduction be somehow quantified (bounded above)? Are there other existing techniques?\nAnother example of a missed discussion is how much the work is coupled with NVDIFFREC. Could the suggested techniques be incorporated into methods like [1] as well? \n\nIn addition, the discussion on the bias introduced (154-162) is unclear. The seems like an important issue that can also promote further research but is very shortly discussed. \n\nThe paper could be improved by being more self-contained. For example, the sampling techniques used are only referenced.\n\nThe effect of the new regularizer is not tested in an ablation study. \n\n[1]: PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting, CVPR 2021.\n[2]:  Please address the weakness stated above. The paper does not include such a discussion.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "1nSmiGmD5m",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Official Review of Paper8741 by Reviewer 1u76",
                "comment": " The paper proposes an inverse rendering pipeline for recovering geometry, reflectance and environment map from a set of multi-view images. The key contributions is a denoising step that significantly reduces variance of Monte Carlo light integration at lower sampling rates. The method achieved superior results compared to a direct lighting baseline (nvdiffrec) and Nerfactor. Strengths:\n\n- The idea to denoise Monte Carlo rendering is well motivated and well implemented. To the best of my knowledge this idea is original.\n- Results are convincing. Particularly the comparison with nvdiffrec baseline shows considerable improvement in novel rendering and relighting. Compared to SG and SH representations, the recovered environment map contains high fidelity details, evident in Figure 2 in the appendix.\n- Limitations have been discussed in detail. I appreciate authors honesty in this.\n\n\nWeakness:\n- Evaluations primarily focus on novel rendering/relighting but lacks in individual intrinsic components such as 3D geometry (e.g. depth, chamfer, normal errors etc) and environment maps. \n- Comparison with recent Monte Carlo inverse rendering method [31,37] especially [37] would strengthen this paper. (Although it should be noted [31] does not solve for geometry, while [37] assumes known lighting and does not have full code release).\n- I think the paper is better suited for CV or graphics venues than NeurIPS. There are machine learning components in this paper but the main components (denoising/Monte Carlo rendering) do not seem very well aligned with NeurIPS. I wonder to what extent the proposed method could be used to handle inter-reflections and light refraction in e.g. translucent objects. To me this seems to be one of the biggest advantage of Monte Carlo integration versus SH/SG but I could not find relevant discussions in paper. Yes limitations have been well discussed.",
                "rating": 5,
                "confidence": 3
            }
        ],
        "label": "train"
    }
}