{
    "index_91": {
        "paper_id": "iclr_2018_rJLTTe-0W",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt\u2013Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJ0Hc82gM",
                "content": "\n\nSummary:\n\nThis paper develops a state space time series forecasting model in the Bayesian framework, jointly detects anomaly and change points. Integrated with an iterative MCMC method, the authors develop an efficient algorithm and use both synthetic and real data set to demonstrate that their algorithms outperform many other state-of-art algorithms. \n\nMajor comments:\nIn the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance. However, in classic formulation for change-point or anomaly detection, usually there is also a mean shift other than the variance change. For example, we might assume $r_t \\sim N(\\theta, \\sigma_r^2)$ for some $\\theta>0$ to demonstrate the positive mean shift. I believe that this kind of mean shift is more efficient to model the structure of change-point. \n\nMy main concern is with the novelty. The work does not seem to be very novel.\n\nMinor comments:\n\n1. In the end of the page 2, the last panel is the residual, not the spikes. \n\n2. In page 12, the caption of figure 5 should be (left) and (right), not (top) and (bottom).",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJhn9OtxG",
                "content": "The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data.\n\nOn the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR. The introduced model is elegant; the algorithms for inference are non-trivial.\n\nFrom a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection. In some situations the methodology proposed here will work better than previous approaches (particularly in the situation where the data comes from the Bayesian model itself - in that case, there clearly is no better approach), in other cases (which the paper might have put less emphasis on), previous approaches will work better. To position this kind of work, I think it is important the authors discuss the limitations of their approach. Some guidelines on when or when not to use it would be valuable. Clearly, these days one cannot introduce methodology in this area and expect it to outperform existing methods under all circumstances (and hence practitioners to always choose it over any other existing method).\n\nWhat is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.\n\nSummary:\n+ Methodology appears to be a significant, solid contribution.\n- Experiments are not conclusive as to when or when not to choose this approach over existing methods\n- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Mehtods')",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJL1pxqeG",
                "content": "Minor comments:\n- page 3. \u201cThe observation equation and transition equations together (i.e., Equation (1,2,3)) together define \u201c - one \u201ctogether\u201d should be removed\n- page 4. \u201cFrom Figure 2, the joint distribution (i.e., the likelihood function \u201d - there should be additional bracket\n- page 7. \u201cWe can further integral out \u03b1n \u201c -> integrate out\n\nMajor comments:\nThe paper is well-written. The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.\n\nSuch type of parametric models are widely considered in econometric literature, see e.g.\n[1] Jalles, Jo\u00e3o Tovar, Structural Time Series Models and the Kalman Filter: A Concise Review (June 19, 2009). FEUNL Working Paper No. 541. Available at SSRN: https://ssrn.com/abstract=1496864 or http://dx.doi.org/10.2139/ssrn.1496864 \n[2] Jacques J. F. Commandeur, Siem Jan Koopman, Marius Ooms. Statistical Software for State Space Methods // May 2011, Volume 41, Issue 1.\n[3] Scott, Steven L. and Varian, Hal R., Predicting the Present with Bayesian Structural Time Series (June 28, 2013). Available at SSRN: https://ssrn.com/abstract=2304426 or http://dx.doi.org/10.2139/ssrn.2304426 \n[4] Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, Farshid Vahid-Araghi, Forecasting time series with multiple seasonal patterns, In European Journal of Operational Research, Volume 191, Issue 1, 2008, Pages 207-222, ISSN 0377-2217, https://doi.org/10.1016/j.ejor.2007.08.024.\n[5] A.C. Harvey, S. Peters. Estimation Procedures for structural time series models // Journal of Forecasting, Vol. 9, 89-108, 1990\n[6] A. Harvey, S.J. Koopman, J. Penzer. Messy Time Series: A Unified approach // Advances in Econometrics, Vol. 13, pp. 103-143.\n\nThey also use Kalman filter and MCMC-based approaches to sample posterior to estimate hidden components.\n\nThere are also non-parametric approaches to extraction of components from quasi-periodic time-series, see e.g.\n[7] Artemov A., Burnaev E. Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence // 16th International Conference on Data Mining Workshops (ICDMW), IEEE Conference Publications, pp. 29 - 36, 2016. DOI: 10.1109/ICDMW.2016.0013\n[8] Alexey Artemov, Evgeny Burnaev and Andrey Lokot. Nonparametric Decomposition of Quasi-periodic Time Series for Change-point Detection // Proc. SPIE 9875, Eighth International Conference on Machine Vision, 987520 (December 8, 2015); 5 P. doi:10.1117/12.2228370;http://dx.doi.org/10.1117/12.2228370\n\nIn some of these papers models of structural brakes and change-points are also considered, see e.g. \n- page 118 in [6]\n- papers [7, 8]\n\nThere were also Bayesian approaches for change-point detection, which are similar to the model of change-point, proposed in the considered paper, e.g.\n[9] Ryan Prescott Adams, David J.C. MacKay. Bayesian Online Changepoint Detection // https://arxiv.org/abs/0710.3742\n[10] Ryan Turner, Yunus Saat\u00e7i, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. In Za\u00efd Harchaoui, editor, NIPS Workshop on Temporal Segmentation, Whistler, BC, Canada, December 2009.\n\nThus,\n- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,\n- the proposed model itself looks very similar to what can be found across econometric literature,\n- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\nTherefore, also the paper is well-written, \n- it lacks novelty,\n- its topic does not perfectly fit topics of interest for ICLR,\nSo, I do not recommend this paper to be published.",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "rkAJXOnQz",
                "content": "Dear Reviewer,\n\nThank you for your comments. We have addressed them accordingly. Please see below for our response point by point.\n\n\n\nMinor comments:\n- page 3. \u201cThe observation equation and transition equations together (i.e., Equation (1,2,3)) together define \u201c - one \u201ctogether\u201d should be removed\n- page 4. \u201cFrom Figure 2, the joint distribution (i.e., the likelihood function \u201d - there should be additional bracket\n- page 7. \u201cWe can further integral out \u03b1n \u201c -> integrate out\n\n>> Thanks, we corrected the typos. \n\nThus,\n- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n\n>> Econometric literature of refs [1, 2, 3, 4, 5]  do not properly consider and process the changing point and anomalies although they perform time-series forecasting.\n\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,\n\n>> We compared state-of-the-art  Bayesian Structural Time Series (BSTS), Prophet R package by Taylor & Letham (2017),  , Exponential Smoothing State Space Model (ETS). The results are shown in Tables 2-6.  The idea of [9--10] are quite similar to them. \n\n- the proposed model itself looks very similar to what can be found across econometric literature,\n\n>> The econometric literature ignores the proper treatment of changing point and anomalies. Bayesian modeling part is also different regarding estimation of posterior for hidden components given different prior distributions.\n>>We add the \u201crelated work section\u201d to illustrate the differences between our work and the existing works.\n\n- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\n>> The experimental study demonstrates that our method outperforms the other methods as well on other benchmarks for anomaly detection. Our ultimate goal is time series forecasting conditional on structure changes. It might not be that meaningful to compare in Numenta Anomaly Detection Benchmark since Anomaly Detection is kind of secondary endpoint.\n\nTherefore, also the paper is well-written, \n- it lacks novelty,\n- its topic does not perfectly fit topics of interest for ICLR,\n\n>> There are three goals of our work: (1) time series forecasting; (2) change point detection; (3) anomalies detection;  these three goals are jointly put in one unified framework by modeling using state-space bayesian modeling.  The change point and anomalies are detected for better forecasting giving time series (structure) input.  Due to the strong description power of bayesian state-space model, the results of model prediction and abnormal and change points detection are mutually improved. Compared to the existing bayesian modeling, our work is novel by  sampling posterior to estimate hidden components given the individual Bernoulli prior of changing point and anomalies.\n>> The paper is related to structure input representation and state-space modeling, which in fact is relevant to ICLR audience. We also highlighted the novelty of the work in \u201ccontribution of this work\u201d on Page 2 in the updated version.  \n",
                "reply_to": "HJL1pxqeG"
            },
            {
                "review_id": "BJljaSkzf",
                "content": "Dear Reviewer,\n\nThank you for your time and effort on reviewing papers. Unfortunately it seems like you uploaded a WRONG review. This is possibly a review for some other paper titled \"Deformation of Bregman divergence and its application\", not for ours.",
                "reply_to": "HJ0Hc82gM"
            },
            {
                "review_id": "rkqRmuhQz",
                "content": "Dear Reviewer,\n\nThank you for reviewing our paper and thank you for appreciating our work. We have made changes following your suggestions. Please see below for our response point by point. Thank you.\n\n\nThe paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data.\n\n>> Thanks for the appreciation of our work. \n\nFrom a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection. In some situations the methodology proposed here will work better than previous approaches (particularly in the situation where the data comes from the Bayesian model itself - in that case, there clearly is no better approach), in other cases (which the paper might have put less emphasis on), previous approaches will work better. To position this kind of work, I think it is important the authors discuss the limitations of their approach. \n\n>> As most (if not all) of the time series works, our method cannot work in every case. For example, when the time series does not have clear decomposition structure as modeled in Eqs.(1-3), the model may not correctly recover the hidden components and correspondingly perform forecasting.\n\nSome guidelines on when or when not to use it would be valuable. Clearly, these days one cannot introduce methodology in this area and expect it to outperform existing methods under all circumstances (and hence practitioners to always choose it over any other existing method).\n\nWhat is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.\n\n>>  BSTS fails in some cases due to the mismatch between model assumptions and actual data distribution and generation process. Usually more complicated a model is, more likely it will fail when the data structure does not satisfy its underlying assumptions. In those cases, simple approaches may achieve better performance, which is not surprising. Nevertheless, our proposed method obtains the best result.\n\nSummary:\n+ Methodology appears to be a significant, solid contribution.\n- Experiments are not conclusive as to when or when not to choose this approach over existing methods\n- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Methods')\n\n>> We will incorporate the discussions regarding model strength, application conditions, and the limitations in final version. \n>> We already fix the typos in updated version. \n",
                "reply_to": "HJhn9OtxG"
            }
        ],
        "generations": {
            "human_reference": "Thank you for submitting you paper to ICLR.  The consensus from the reviewers is that this is not quite ready for publication. There is also concern about whether ICLR, with its focus on representational learning, is the right venue for this work.\n\nOne of the reviewers initially submitted an incorrect review, but this mistake has now been rectified. Apologies that this was not done sooner in order to allow you to address their concerns.",
            "random_reference": "All in all, while the reviewers found that the problem at hand is interesting to study, the submission's contributions in terms of significance/novelty did not rise to the standards for acceptance. The reasoning is most succinctly discussed by R3 who argues that IFS and EFS are basically feature selection and applying them to feature attribution is not particularly novel from a methodological point of view. ",
            "modular_llama3": "This manuscript proposes a novel Bayesian state space time series model that demonstrates strong methodological contributions, capturing change points, anomaly points, trend, and seasonality, and outperforming existing methods in time series forecasting, change point detection, and anomaly detection. However, its novelty has been questioned by some reviewers due to similarities with existing econometric literature. While the methodological contribution is solid, the practical significance is limited, and the experimental evaluation is inconclusive, lacking comparison with relevant advanced methods. Furthermore, the writing quality is inconsistent, with some grammatical errors and typos that require improvement. Overall, the paper's topic may not be the best fit for ICLR, with concerns over missing comparisons with relevant literature and limited novelty.",
            "llama3_pr_chunk": "This paper proposes a novel Bayesian state space time series model to capture changes, anomalies, trends, and seasonality, addressing limitations of traditional methods. The model outperforms existing methods in synthetic and real data applications, but reviewers raised concerns about the novelty of the work, suggesting improvements and comparing it with existing econometric literature and advanced change-point detection methods. The authors argued the model's novelty in unifying time series forecasting, change point detection, and anomalies detection using state-space Bayesian modeling, but reviewers remained unconvinced due to the model's similarity to existing work and the limited practical impact.",
            "llama3_pr_consolidation": "The reviewers praised the paper's solid methodology, sound Bayesian model for time series with anomaly and change points, and its significant contribution to time series forecasting. However, concerns were raised regarding the paper's novelty, as the proposed model is similar to existing econometric literature and lacks comparison with relevant advanced change-point detection methods. The reviewers suggested that the authors discuss the limitations of their approach, provide guidelines on when to use it, and improve the writing by fixing grammatical errors and typos. The experimental results were not conclusive in determining when to choose the proposed approach over existing methods. The authors responded to the comments and revised the paper accordingly. Overall, the reviewers appreciated the paper's contribution but had mixed opinions about its novelty and overall quality.\n\nThe paper has a strong positive sentiment for its methodology, but negative sentiments for its novelty, with some reviewers considering it not novel enough and lacking comparison with relevant literature. The overall quality of the paper is considered positive, but with some reservations. The convincingness level of the paper is highly convincing for its methodology, but slightly convincing for its novelty. \n\nIn terms of criteria facets, the paper scores high on Soundness and Clarity, but lower on Novelty and Advancement. The Overall quality is considered positive, but with some reservations.",
            "llama3_pr_knowledge": "The paper proposes a Bayesian state space time series model that captures change points, anomaly points, trend, and seasonality. It uses an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter, and Kalman smoothing. The methodology is solid and significant, but the novelty is lacking, and the writing needs improvement. The experimental results show that the proposed method outperforms existing methods in some cases, but not in all circumstances. The authors are suggested to discuss the limitations of their approach, provide guidelines on when to use their method, and improve the writing quality.",
            "llama3_pr_decomposed": "In conclusion, this paper makes significant contributions to the field by introducing a novel approach that addresses the limitations of existing methods. The authors have successfully provided additional context and comparisons to highlight the unique aspects of their work. While the paper has made substantial progress, potential limitations and areas for future improvement include [insert limitations or areas for future research]. Future studies can build upon this research by exploring [insert potential avenues for future research], ultimately further advancing the field.",
            "modular_gpt4": "The submitted paper introduces a Bayesian state space model for time series forecasting, change point detection, and anomaly detection and demonstrates promising results on both synthetic and real datasets. Despite its strengths, including a well-written methodology and solid technical foundations, reviewers consistently highlight concerns regarding its novelty, alignment with ICLR's scope, and the need for more comprehensive comparisons with advanced econometric methods and relevant literature. While the model shows superior performance in certain benchmarks, it does not consistently outperform simpler methods like ETS and STL. Additionally, suggestions are made to address numerous grammatical errors and typos to improve the writing quality. Given these points, the overall contribution is deemed insufficiently innovative and relevant for publication at ICLR."
        }
    },
    "index_56": {
        "paper_id": "iclr_2022_bl9zYxOVwa",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Although current adversarial training (AT) methods can effectively improve the robustness on adversarial examples, \nthey usually lead to a decrease in accuracy, called the robustness-accuracy trade-off. In addition, researchers have recently discovered a robust fairness phenomenon in the AT model; that is, not all categories of the dataset have experienced a serious decline in accuracy with the introduction of AT methods. In this paper, we explore the relationship between the robustness-accuracy tradeoff and robust fairness for the first time.  Empirically, we have found that AT will cause a substantial increase in the inter-class similarity, which could be the root cause of these two phenomena. We argue that the label smoothing (LS) is more than a trick in AT. The smoothness learned from LS can help reduce the excessive inter-class similarity caused by AT, and also reduce the intra-class variance, thereby significantly improving accuracy.  Then, we explored the effect of another classic smoothing regularizer, namely, the maximum entropy (ME), and we have found ME can also help reduce both inter-class similarity and intra-class variance. Additionally, we revealed that TRADES actually implies the function of ME, \nwhich can explain why TRADES usually performs better than PGD-AT on robustness. Finally, we proposed the maximum entropy PGD-AT (ME-AT) and the maximum entropy TRADES (ME-TRADES), and experimental results show that our methods can significantly mitigate both tradeoff and robust fairness.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "a76mAzAMbiA",
                "content": "This paper investigates why adversarial training can sometimes present a worse trade-off between robustness and accuracy, and found one root cause could be that AT causes a substantial increase in the inter-class similarity. The paper then proposes combining both AT and TRADES with two smoothing techniques, label-smoothing, and maximum entropy, and found the resulting methods yield a better trade-off over natural accuracy and robustness. Strengths:\n- The problem is well motivated, and the authors presented a very thorough analysis on the per-class accuracy and inter-class similarity on why AT could lead to a trade-off over accuracy and robustness. The reasoning train from section 1 through section 2 is interesting and well supported by experiments and analysis, and the method in section 3 is well motivated.\n\n- The empirical results are strong on CIFAR-10 and demonstrate a better accuracy-robustness trade-off compared to two defense baselines (AT and TRADES).\n\nWeakness: \n- Currently all the results/analysis are on CIFAR-10 only. More experiment on other datasets would be useful to show the hypothesis (AT hurts inter-class similarity) holds across multiple scenarios. \n\n- The authors should add more discussion on related works. A few works have shown that the improved adversarial robustness from label smoothing might be the effect of gradient-masking and thus LS could be volatile to other attacks. [1] Please see the discussion in: https://openreview.net/forum?id=BJlr0j0ctX; and [2] Fu et al. Label Smoothing and Adversarial Robustness. https://arxiv.org/pdf/2009.08233.pdf\n\nGiven the current paper claims more on the robustness-accuracy trade-off side, complete adversarial robustness might not be a goal, but it would be great if the authors can add more discussion on this front.\n\nMinor:\n- Figure 2 b, please pair the colors with class categories;\n- Section 3.2.1, please add the citation back for ME learning (as in Section 1.2)\n- Table 5 is informative and puts the results in perspective, consider moving it to main text. This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off. The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy-tradeoff by quite a margin. \n\nOn the other hand, the authors should present more results on other datasets to show the results hold across scenarios. Also more discussion on related works would be useful to clarify the benefits of label smoothing in terms of adversarial robustness (it's known that LS improves accuracy).",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "2b9c0E_O5ro",
                "content": "This paper identifies how adversarial training (AT) algorithms for robustness may negatively affect the notion of robust fairness and proposes two methods ME-AT and ME-TRADES, which combine existing AT methods with a maximum entropy (ME) term, to improve the accuracy-robustness tradeoff and robustness fairness. Although AT algorithms improve model robustness, they also increase inter-class similarities, which make certain classes more difficult to classify, leading to unfair accuracies. The paper then shows that label smoothing (LS) mitigates this effect and in particular investigates the ME technique. The authors show that a method called TRADES outperforms another method called PGD-AT because it is a special version of ME. Experiments show that combining ME with these AT methods outperforms PGD-AT. <Strong points>\n\n* Improving the fairness of robust training is a timely problem that is being actively studied.\n* The empirical results on how adversarial training increases inter-class similarities, which reduces robust fairness, is convincing. \n* The experiments show that ME-AT and ME-TRADES indeed improves the robust-accuracy tradeoff and robust fairness.\n\n<Weak points>\n\n* Overall the paper uses too many pages on motivating the problem while missing critical content as explained in the following weak points. The explanation on why AT algorithms increase inter-class similarities goes on for 5 pages, but the content is often redundant and can be significantly reduced while being just as convincing. The remaining 4 pages seem too short for the rest of the material.\n* In the Introduction, the PGT-AT and TRADES techniques appear without much explanation. How do these techniques work and why are they the important ones to consider for fair robust training? Moreover, there is no related work section, and the authors should compare their techniques with the following recent fair robust training works:\n   * Zhang and Davidson, \"Towards Fair Deep Anomaly Detection\", FAccT 2021.\n   * Khani and Liang, \"Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately\", FAccT 2021. \n* Section 3.1 is critical where it explains why LS prevents inter-class similarities from going up. However, the explanation is not convincing where it only shows a few experiments on one dataset that is not even well described (is it the dataset mentioned in the Introduction?). As LS is an important technique, it should be explained in the paper instead of just adding a citation. Most importantly, there needs to be some convincing analysis showing why LS reduces similarity and variance for any dataset in general. \n* The flow from Section 3.1 to 3.2 is not clear. If a few LS techniques work well empirically, why does that lead you to investigate ME? Do all LS techniques work well? Is there no LS technique that works better than ME? What if ME works well for different reasons? Among the ME methods, why only consider the TRADES method? If TRADES is already a special ME technique, how is ME-TRADES an improvement?\n* The experiments are not extensive enough where only the CIFAR-10 dataset is used. Instead, there must be at least two or three datasets to make sure the results are general. The same comment goes for the experiments in the Introduction. In addition, the authors do not compare their techniques with FRL by Xu et al. because it is said to solve an easier problem and is thus \"not appropriate\". I am not sure if I agree. FRL is also one of the SOTA methods and has the similar goal of improving robustness against adversarial data while adding constraints to reduce accuracy disparities between groups. IMHO the authors should definitely make an extensive comparison with FRL to emphasize the effectiveness of ME-TRADES.\n* In Section 3.2.2, please clearly define PGD-20 and the exact measure used to produce the Table 2 values.\n* The role of intra-class variance is not clear. The inter-class similarity seems to be the main cause of accuracy-robustness tradeoff and robust fairness in the paper. In addition, existing AT methods already decrease the intra-class variance compared to standard training. Hence it is unclear how important intra-class variance is.\n While the paper solves a timely and important problem, it can also be improved by shortening its introduction to the problem, better explaining why LS works and why we should focus on ME and TRADES, citing and comparing with more related work, and making its evaluations more extensive.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "cIw-I_mLx4",
                "content": "Motivated by the empirical observations, the authors argue that there is a high correlation between the drop of robust accuracy, robust fairness and inter-class similarity. The authors then proposed to augment these two existing methods with the corresponding label smoothing and the maximum-entropy regularizer.\n The paper starts with an empirical observation that large inter-class similarity highly correlates with the drop in accuracy as well as the accuracy parity between different classes. This observation is quite natural, since if the features of two classes are class to each other, then it makes it harder to classify for any predictors based on these features, leading to dropped accuracy of these classes. Motivated by this empirical observation, the authors proposed to use label smoothing (LS) to decrease the inter-class similarity, in order to alleviate the robustness-accuracy tradeoff problem. \n\nBesides the label smoothing, the authors also argued that using the maximum entropy regularizer could help reduce the inter-class similarity, and then proposed two variants of existing works, i.e., adversarial training and TRADES, with the additional maximum entropy regularizer. \n\nOverall the paper is quite clear and easy to follow. However, the current manuscript does not have a related work section and indeed fails to discuss a line of works in the fairness literature. In particular, the problem studied in this work is closely related to the accuracy parity problem in the fairness literature, yet there is no discussion in this line of works in the related works. Some of them include [1-3] and the references therein.\n\n[1].    Demonstrating accuracy equity and predictive parity performance of the compas risk scales in broward county.\n\n[2].    Understanding and Mitigating Accuracy Disparity in Regression\n\n[3].    Fair regression: Quantitative definitions and reduction based algorithms. \n\nThe organization of the paper, however, could be improved. For example, Section 3 directly jumps to the discussion of label smoothing without a proper introduction on what is LS. The same also applies to the maximum entropy regularization as well.\n\nTechnically, although the empirical observation is fine, I am interested in understanding whether this phenomenon could generalize to other datasets, or is this specific to the one studied in this paper. To answer this question, it would be good to have some theoretical justification to link the relationship between robust accuracy and inter-class similarity. This is quite important, as it means the empirical observation so far is not just a coincidence but holds in general. \n\nI would suggest the authors soften the use of the word \"proven\" in the description of the related works in the first paragraph of the Introduction. Technically, none of the existing methods has been \"proven\" to be the best but verified empirically to be effective. To me, claiming these empirical results to be proven is quite misleading. \n\nOther questions:\n-   In Fig. 2, I would image the features from the penultimate layer to be a high-dimensional vector, so the corresponding measure should be a covariance matrix rather than a scalar variance. What's the specific measure used in Fig. 2(a)?\n\nMinor:\n-   In the caption of Fig. 1, better to explain what's PGD-10 error, since this is not a standard term. \n Given the lack of technical contributions and the discussion to a closely related line of works in the fairness literature, I would vote for rejection.\n",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "Q0O48osAz5e",
                "content": "This paper investigates inter-class similarity and intra-class variance, and corroborates that AT will cause an increase in inter-class similarity, which could be the root of both the robustness-accuracy tradeoff and robust fairness phenomena. The authors first considers Label Smoothing (LS) as the regularizer, and concludes that LS will cause a loss of information in the logits, and hence weaken the discriminative power of the trained models. The authors then confirms that ME can help reduce the excessive inter-class similarity in robust models, and also provides a lower intra-class variance, which is the remedy of previous AT methods. Experiments partially support the conclusions of the paper. Strength:\n1. The paper provides extensive experimental analysis (e.g., single adversarial training) for the potential reason of trade-off between robustness and accuracy and robust fairness. The paper concludes that high inter-class similarity might be the main cause.\n2. The paper investigates the insight behind why TRADES performs better than PGD-AT.\n3. The paper proposed new ME-based methods to further improve the adversarial robustness over baseline.\n\nWeakness:\n1. The improvement by ME is marginal compared with non-ME methods. Given that AA might not be the proxy of strongest adversarial attack, the true improvement might be smaller or even negative.\n2. Regarding writing, in Section 3.1 the method of LS is not introduced. For those who are not familiar with LS, it is hard to understand the section.\n3. The paper misses certain references which study the trade-off between robustness and accuracy, e.g., [1].\n[1] A Closer Look at Accuracy vs. Robustness, NeurIPS 2020 Overall, I think the paper is on borderline in its current version. The main weakness of the paper is its technical novelty. But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade-off between robustness and accuracy and robust fairness. Extra theoretical analysis in support of the empirical discovery will significantly improve the paper and weight my rate towards recommending full acceptance.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            }
        ],
        "generations": {
            "human_reference": "The paper argues that adversarial training increases inter-class similarities, therby increasingly the misclassification of some classes and lowering accuracy parity across classes. It proposes to combine existing adversarial training methodologies, PGD-AT and TRADES, with a maximum entropy term to improve the classification fairness while remaining robust.\n\nWhile they agree that the problem is timely and important, the reviewers identify the following issues that place the current iteration of the paper below the bar of acceptance: the comparison to other works on fair robust training and accuracy parity is incomplete; experimental evaluation is conducted only on CIFAR10, making the generalizability of the paper's claims about performance unclear; and the proposed methodology has low technical novelty.",
            "random_reference": "This paper introduces a new method to synthesize tabular datasets with a latent diffusion model.\nIt natively handles mixed continuous/categorical features by first learning a continuous embedding of all of them to then also exploit interactions between them in a VAE followed by a diffusion model. The method yields very strong results in a comprehensive empirical evaluation. It also makes a contribution to the training algorithm by scheduling the beta of the beta-VAE.\nThe paper has many strengths, including the inclusion of clean and well-documented source code, a new benchmark with five different evaluation metrics, strong performance in all of these metrics, clarity of writing and clear illustrations, and a single hyperparameter setting used throughout to avoid cherry picking.\nWeaknesses identified by the reviewers are that the scientific contribution is somewhat incremental, that it is questionable whether hyperparameter optimization would have helped the baselines more, and the downside of introducing a new benchmark that does not subsume previous benchmarks but might become a competing benchmark (leading to fragmentation and incomparability of results).\nOverall, this is a very strong paper. While tabular data has not traditionally been a focus at ICLR (since traditional ML methods have been competitive), I believe it would be good to change this, seeing that there are now many successful applications of deep learning for tabular data. I therefore recommend this paper for an oral.",
            "modular_llama3": "This manuscript explores the relationship between robustness-accuracy tradeoff and robust fairness, proposing novel methods to improve the tradeoff by combining label smoothing and maximum entropy with adversarial training. While the paper presents well-structured and clear content, addressing the robustness-accuracy trade-off issue, it has several limitations. The empirical results, mainly limited to the CIFAR-10 dataset, require more extensive analysis and theoretical support for generalization. Furthermore, the manuscript lacks a related work section, comparisons with recent fair robust training works, and sufficient explanations, definitions, and references. Although the paper presents convincing results, its technical novelty and contributions are limited, making it a borderline case for acceptance. Overall, the authors need to improve the paper's organization, content, and theoretical support to enhance its clarity, flow, and contributions.",
            "llama3_pr_chunk": "This paper examines the relationship between robustness-accuracy trade-off and robust fairness in adversarial training, finding increased inter-class similarity. It proposes ME-AT and ME-TRADES methods, combining adversarial training with maximum entropy to mitigate the trade-off and robust fairness, outperforming existing methods.",
            "llama3_pr_consolidation": "This manuscript explores the relationship between robustness-accuracy trade-off and robust fairness in adversarial training (AT) methods, discovering that AT causes a substantial increase in inter-class similarity, which could be the root cause of these phenomena. The authors propose combining AT and TRADES with label smoothing (LS) and maximum entropy (ME) to reduce inter-class similarity and intra-class variance, resulting in improved accuracy. The empirical results on CIFAR-10 demonstrate a better accuracy-robustness trade-off compared to two defense baselines.\n\nHowever, the manuscript has several limitations. Firstly, the results are only presented on CIFAR-10, and more experiments on other datasets would be necessary to show the hypothesis holds across multiple scenarios. Secondly, the authors should add more discussion on related works, including the benefits of label smoothing in terms of adversarial robustness and comparisons with other recent fair robust training works. Furthermore, the explanation of why LS works and why ME is chosen is not convincing, and more theoretical justification is needed to link the relationship between robust accuracy and inter-class similarity.\n\nIn terms of writing, the manuscript could be improved by shortening the introduction, adding more citations, and clarifying the role of intra-class variance. The flow between sections could also be improved, and more extensive evaluations on multiple datasets would strengthen the results.\n\nOverall, the manuscript provides an interesting observation and a potentially reasonable explanation for the trade-off between robustness and accuracy and robust fairness. However, the technical novelty is limited, and extra theoretical analysis and more extensive experiments are needed to support the empirical discovery.",
            "llama3_pr_knowledge": "**Review Summary**\n\nThe article explores the relationship between the robustness-accuracy tradeoff and robust fairness in adversarial training (AT). The authors propose that AT causes a substantial increase in inter-class similarity, leading to a decrease in accuracy. They argue that label smoothing (LS) and maximum entropy (ME) can help reduce inter-class similarity and improve the accuracy-robustness tradeoff.\n\n**Advancement**: The paper investigates a timely and important problem in the field of adversarial robustness, providing a well-motivated analysis of the per-class accuracy and inter-class similarity. The proposed methods, ME-AT and ME-TRADES, demonstrate a better accuracy-robustness tradeoff compared to baseline methods.\n\n**Clarity**: The writing is generally clear, but some sections could be improved. The organization of the paper could be better, with some sections lacking proper introductions. The use of technical terms, such as PGD-10 error, should be explained.\n\n**Compliance**: The paper meets the publication requirements, but lacks a related work section and fails to discuss relevant works in the fairness literature.\n\n**Soundness**: The empirical results are strong, but the paper could benefit from more theoretical analysis to support the empirical discovery. The experiments are not extensive enough, with only one dataset used. The authors should add more discussion on related works and provide more convincing analysis on why LS and ME work.\n\n**Novelty**: The paper provides a new insight into the relationship between inter-class similarity and robustness-accuracy tradeoff, but the technical novelty is limited. The proposed methods are based on existing techniques, and the improvement over baseline methods is marginal.\n\n**Recommendation**: The paper is borderline in its current version. With additional theoretical analysis, more extensive experiments, and a better organization of the paper, it could be improved significantly.",
            "llama3_pr_decomposed": "The paper has the potential to make a significant impact in the field by shedding light on the trade-off between robustness and accuracy, and proposing methods that can improve fairness in robust training.",
            "modular_gpt4": "The paper presents a timely solution to the accuracy-robustness trade-off in adversarial training (AT) by integrating label smoothing (LS) and maximum entropy (ME) techniques, resulting in new methods ME-AT and ME-TRADES. These methods demonstrate improved performance on CIFAR-10 compared to existing approaches like PGD-AT and TRADES. While the experiments and empirical results are robust, questions remain regarding the generalizability to other datasets and the need for theoretical justification. The paper could benefit from better explanation of key concepts, more extensive dataset evaluations, clearer organization, and additional related work comparisons. Despite its limited technical novelty, the paper is well-written and its contributions are significant, particularly in improving robust fairness. The overall recommendation is to accept with minor revisions to address these concerns."
        }
    },
    "index_39": {
        "paper_id": "iclr_2024_vVoWRFV5Y4",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary. Out of sample, our solutions are on average within 7.5% of a high-quality local search baseline, and even outperform it on 1.2% of instances.",
                "reply_to": "iclr_2024_vVoWRFV5Y4"
            },
            {
                "review_id": "JlR14QD7hX",
                "content": " Thank you for the feedback. While we focus on a facility location motivation, it is true that the quadratic assignment problem can be viewed as a generalization of several other combinatorial optimization problems such as the traveling salesman problem and some vehicle routing problems \u2013 indeed, we feel this is motivation for developing a DRL methodology to solve it. We have added the suggested references to our literature review, and we are working to implement these suggestions (as well as those made by other referees) to improve performance and narrow the gap with our baseline methods.",
                "reply_to": "hASFBkaYh1"
            },
            {
                "review_id": "gJqjRKlfTk",
                "content": " Thank you for your feedback. We really appreciate the constructive suggestions as to how we can improve model performance and narrow the gap with existing methods. We have added references to the suggested works, and are working to integrate some of these ideas into our model. In particular, we are thinking about how to improve the performance of our critic network, as well as how to select a more problem-specific attention mechanism.",
                "reply_to": "cVlXMNEbZK"
            },
            {
                "review_id": "D8jf8Gekkm",
                "content": " Thank you for your feedback. We have expanded the literature review to include some additional relevant references suggested by the referees, as well as clarified the motivation and contribution in the introduction. Regarding the evaluation, we feel that adding further baselines is difficult, since to our knowledge there are no other RL frameworks for QAP. We are working on improving performance relative to the swap heuristic by making tweaks to our model architecture, following the suggestions of other reviewers. We are also working to improve scalability by reducing model size where it is possible to do so without compromising performance.",
                "reply_to": "H1CtCCjgvq"
            },
            {
                "review_id": "yFdRO05X7i",
                "content": " Thank you for the feedback. Regarding baselines: we chose the swap heuristic because it provides near-optimal solutions (within about 1% of the provable optimum found by Gurobi) in a very short time. The gap provided is therefore a (nearly tight) lower bound and good approximation of the standard MIP gap. Regarding scalability, we are working on reducing the number of model parameters to generalize to larger sizes; while scalability is indeed a concern, this is even more true for Gurobi, which is not able to solve the problem in hours for sizes larger than 20. Regarding training time, we clarify that the reported training times of 20 minutes and 1 hour were per training epoch.",
                "reply_to": "CpW57f0z9B"
            },
            {
                "review_id": "CpW57f0z9B",
                "content": "The Quadratic Assignment Problem (QAP) is an NP-hard problem with significant challenges, especially for larger instances. The paper proposes using deep reinforcement learning (DRL) to address the QAP, introducing a novel double pointer network to tackle the Koopmans-Beckman formulation of QAP. The method is trained using A2C on synthetic datasets, and its performance is benchmarked against a swap-based local search heuristic. Using deep reinforcement learning to address the QAP is an innovative method, setting it apart from traditional optimization techniques. The double pointer network alternates between selecting locations and facilities, providing a dynamic solution approach. The model is trained on a large dataset of synthetic instances, making it robust and generalizable. The method is tested primarily for QAP instances up to size 20, highlighting a potential scalability concern. While the DRL approach shows promise, there is still a performance gap when compared to the swap heuristic and the Gurobi solver. Unlike traditional optimization methods which provide a solution and its optimality certificate, this DRL approach only gives a solution.",
                "reply_to": "iclr_2024_vVoWRFV5Y4"
            },
            {
                "review_id": "H1CtCCjgvq",
                "content": "This paper proposes an RL method to solve quadratic assignment problems. The authors reformulate the original QAP as a seq2seq problem. However, the paper is not easy to follow. The motivations and contributions of the work are not clear.  The details of the method are missing. The experimental evaluation is not good enough. Therefore, I think the paper is under the bar of ICLR in its current form. The topic of using RL to solve the quadratic assignment problem is interesting.\n\nThe authors reformulate the original QAP as a seq2seq problem. The motivation and contribution of the work is not clear.\n\nThe paper is not easy to follow.\n\nThe literature review is not good enough. A lot of classic literature is missing.\n\nThe experimental evaluation is not enough. Only Gurobi and swap-based local search heuristic is compared. Which makes the paper less convincing. \n\nThe size of QAP used in the experiment is quite small. \n\nThe performance is quite poor, which cannot outperform simple heuristics such as SWAP.",
                "reply_to": "iclr_2024_vVoWRFV5Y4"
            },
            {
                "review_id": "cVlXMNEbZK",
                "content": "This paper delves into the end-to-end application of Deep RL for solving the QAP. Because the QAP necessitates assignments between a set of facilities and locations, the authors introduce a novel autoregressive model that sequentially selects a location and then a facility (to make a pair), and then repeats this process until the assignment is done. The results are promising. The derived solutions are, on average, within 7.5% of those generated by a heuristic method. This paper represents the pioneering effort to tackle the QAP using a neural network-based approach, marking an important milestone in the field. The paper is well-structured, with contents presented in a manner that's easy to follow and understand. The results are too weak for ICLR publication. While one could argue that the considerable optimality gap might be attributed to the intrinsic complexity of the QAP, it's evident that a notable portion arises from the authors' dependence on a neural net model and the training methods that closely mirror the early contributions of Bello et al. that might not be the best fit.\n\nTo draw a parallel, when the pointer network was initially applied to solve the TSP in an end-to-end manner, there existed an optimality gap of around 7% for 100-node TSPs. However, with subsequent refinements in models and training techniques, that gap has been narrowed to almost 0% now. \n\nIt's plausible that similar methodological advancements could significantly benefit the QAP approach presented here. I'd recommend reconsidering the use of the Critic network for such combinatorial optimization tasks, especially given the challenges of predicting the final outcome midway through solution construction.\n\nA few references that could be helpful are:\n[model]\nMatrix Encoding Networks for Neural Combinatorial Optimization (Kwon, et al.)\nLearning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer (Ma, et al.)\n[RL method]\nPOMO: Policy Optimization with Multiple Optima for Reinforcement Learning (Kwon, et al.)",
                "reply_to": "iclr_2024_vVoWRFV5Y4"
            },
            {
                "review_id": "hASFBkaYh1",
                "content": "In this paper, the authors investigated the Quadratic Assignment Problem (QAP) solving using deep reinforcement learning with double pointer networks, where an upper pointer network selects locations and a lower pointer network selects facilities. And the proposed method is evaluated on synthetic QAP instances. 1- This paper is well motivated as the QAP has been rarely studied using DRL;\n2- It seems the proposed method works on the self-generated dataset. 1- It seems that from the DRL based heuristic perspective, the objective function in QAP does not make difference compared with the ones in ILP, both of which are just captured by a reward in DRL. In this sense, the QAP in this paper is almost the same as the vehicle and customer assignment in VRPs with linear objectives, such as\n[a] Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem. IEEE T Cybernetics;\n[b] Learning to Solve Vehicle Routing Problems with Time Windows through Joint Attention. Arxiv;\n[c] Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. Arxiv.\nThose works also involve sequentially or parallelly selecting vehicles and customers, while using (advanced) Transformers rather than pointer network.\n\n2- Besides, the Transformers in 'Learning Improvement Heuristics for Solving Routing Problems. TNNLS' outputs a probability matrix, which could also be tailored to the probability of facility and location pair with proper masking.\n\n3- The evaluation is quite simple, which solely focuses on one single problem with randomly generated instances. And the baselines are also insufficient and not strong enough.\n\nOverall, the method falls short of novelty, and the evaluation is inadequate, which is below the standard of an ICLR publication.",
                "reply_to": "iclr_2024_vVoWRFV5Y4"
            }
        ],
        "generations": {
            "human_reference": "The authors develop an RL based approach for solving a specific class of combinatorial optimization problems, the quadratic assignment problem, and perform evaluations comparing to a local search heuristic. Reviewers agree that the evaluation is not sufficient to justify the value of the method, and hence I recommend rejection.",
            "random_reference": "This paper addresses domain generalization with CLIP models, proposing image encoder tuning (Im-Tuning) to enhance feature separability. While the approach is well-presented and effective, some concerns about the lack of in-depth analysis on the separability issue and limited comparisons to other multimodal models impact the overall contribution. Additionally, clarity in argumentation and potential efficiency concerns with increased parameters need attention. Further exploration of novel aspects and addressing weaknesses will strengthen the paper's significance in the context of domain generalization with CLIP models.\n\nAfter rebuttal, none of the \"negative\" reviewers were convinced. The authors acknowledged that they failed to compare with more sota visual prompt tuning models and AC believes that it is not proper to ask the definitive question that \"Prompt tuning is not all we need\" before a comprehensive literature review.",
            "modular_llama3": "The reviewers have mixed opinions on the manuscript, with some acknowledging its pioneering effort in applying deep reinforcement learning to the quadratic assignment problem, while others raise concerns about its novelty, methodology, and experimental evaluation. The proposed approach shows promise but has a significant performance gap compared to traditional methods, and lacks clarity in methodology, motivations, and contributions. Additionally, the reviewers agree that the experimental evaluation is limited and the results are too weak, falling below the standard of an ICLR publication. The manuscript's weaknesses in literature review and optimality certificate also need to be addressed. Overall, the reviewers suggest improving the methodology, experimental evaluation, and results to elevate the manuscript to the standard of an ICLR publication.",
            "llama3_pr_chunk": "A novel deep reinforcement learning approach using a double pointer network is proposed to solve the Quadratic Assignment Problem (QAP), achieving solutions within 7.5% of a high-quality local search baseline on average, but with limitations in performance, scalability, and evaluation methods.",
            "llama3_pr_consolidation": "The manuscript proposes a novel approach to solving the Quadratic Assignment Problem (QAP) using deep reinforcement learning (DRL) with a double pointer network. While the idea of applying DRL to QAP is innovative and well-motivated, the execution falls short in several aspects.\n\nThe novelty of the approach is a significant strength, as it represents a pioneering effort to tackle QAP using a neural network-based approach. However, the method's soundness and overall quality are compromised by its reliance on a simple critic network and training methods that may not be the best fit for the task. The performance gap between the proposed method and the swap heuristic and Gurobi solver is notable, and the scalability of the approach is a concern, particularly for larger instances.\n\nThe clarity of the paper is variable, with some reviewers finding it well-structured and easy to follow, while others found it difficult to understand, particularly with regards to the motivations and contributions of the work. The literature review is also incomplete, missing relevant classic studies.\n\nThe evaluation of the method is another area of weakness, with a limited set of baselines and a focus on small-sized QAP instances. The comparison to other methods, such as those using Transformers, highlights the potential for improvement in the proposed approach.\n\nOverall, while the manuscript shows promise, it requires significant revisions to address the concerns around soundness, overall quality, and evaluation. The authors should reconsider their approach to the critic network and training methods, and provide a more comprehensive evaluation of their method. With further development and refinement, this work has the potential to make a meaningful contribution to the field.",
            "llama3_pr_knowledge": "**Advancement**: The manuscript is important to the discipline, as it addresses the Quadratic Assignment Problem (QAP), a challenging NP-hard problem with significant applications. The contributions are significant, as it proposes a novel method using deep reinforcement learning (DRL) to solve the QAP. However, some reviewers question the novelty of the idea, as similar approaches have been used in other combinatorial optimization problems.\n\n**Clarity**: The writing is clear, and the structure is well-organized. However, some reviewers find the paper difficult to follow, and the motivations and contributions are not clearly presented. The literature review is also considered insufficient by some reviewers.\n\n**Compliance**: The manuscript fits the venue, and all ethical and publication requirements are met.\n\n**Soundness**: The empirical soundness is good, as the experiments are well-designed, and the results are promising. However, some reviewers question the scalability of the method and the performance gap compared to traditional optimization techniques. The theoretical soundness is also good, as the DRL approach is well-supported by theoretical analysis.\n\n**Novelty**: The novelty of the idea is questioned by some reviewers, as similar approaches have been used in other combinatorial optimization problems. However, the application of DRL to the QAP is considered innovative and pioneering by other reviewers.",
            "llama3_pr_decomposed": "A novel application of Deep Reinforcement Learning (DRL) has demonstrated promising results in solving the Quadratic Assignment Problem (QAP), offering a potential breakthrough in this complex optimization challenge.",
            "modular_gpt4": "The manuscript introduces a deep reinforcement learning approach with a double pointer network to address the quadratic assignment problem (QAP). Although this is an innovative application of neural networks to a combinatorial optimization problem, the reviewers collectively find the method's novelty and evaluation inadequate for ICLR standards. The proposed method, while pioneering, underperforms against traditional techniques such as Gurobi and swap heuristics, and faces significant scalability issues for larger problem instances. Furthermore, the experimental evaluation lacks sufficient comparative baselines, which diminishes the paper's impact. The paper, despite being well-structured and clear, does not meet the necessary criteria for publication at ICLR due to these performance shortcomings and inadequate evaluation."
        }
    },
    "index_21": {
        "paper_id": "iclr_2024_48CXLrx7K3",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "We evaluate the information that can unintentionally leak into the low dimensional output of a neural network, by reconstructing an input image from a 40- or 32-element feature vector that intends to only describe abstract attributes of a facial portrait. The reconstruction uses blackbox-access to the image encoder which generates the feature vector. Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art. Our strategy uses a pretrained StyleGAN and a new loss function that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. Additionally, we present a new technique that fuses the output of an ensemble, to deliberately generate specific aspects of the recreated image.",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "mHO6aNkUXQ",
                "content": " Thank you for your time and review. We have now improved our paper according to your suggestions:\n\n* We have incorporated the respective similarity scores into the qualitative evaluation (Figure 8 and 9). Further, we acknowledge that the recreation illustrated in the last column of Figure 4 serves as an example of instances where our method may encounter challenges, and we have included a brief comment on this in the evaluation. It's important to note that the target images were randomly chosen, not cherry-picked. However, we think that the reconstructability of many (though not all) images still poses a potential privacy risk to the subjects depicted.\n  \n* Considering that standard image metrics are not always good enough to evaluate the perceptual similarity of faces, we now additionally present the results of a user study (see Table 2) - proving that the reconstructions can be accurate enough to be identified with a high probability.\n\n* Regarding the evaluation method used in [A], we appreciate the merit of demonstrating information leakage by establishing that \"the distance between a real image and a generated image is, on average, smaller than the distance between two real images\" as suggested by their approach. To align with this concept, we computed the average distance between images from the test and training sets of our target dataset, CelebA, as a baseline. We have included a dedicated paragraph on baseline dataset statistics into the experiments chapter.\n\n* Publication [B] and [C] are examining the topic of conditional image generation, an issue that can appear similar, but is digressing from our research in some key aspects. Seeing that this area is nontheless interesting, especially when delving deeper into the subject, we appended a section about conditional image generation, and the difference to our paper, to the appendix.\n\nWe would also like to answer a few of your questions directly:\n\n* Regarding your inquiry about different facial embeddings, we deliberately exclude facial embeddings as a target for our attack within the scope of our research. This  exclusion is due to the considerably higher dimensionality of facial embeddings, explicitly designed to represent the entire identity of the depicted person. Reconstructing an identity from a facial embedding is notably easier and has been repeatedly and successfully accomplished (refer to our related work in Table 2 for examples).\nIf you are suggesting that our facial embedding loss could have been based on a different embedding network than FaceNet, your point is certainly valid. FaceNet serves as a robust initial method, given its relatively small network requirements and the availability of reliable pretrained models. While exploring alternative embedding networks and even different image generators than StyleGAN may be a promising avenue for future research, incorporating a more complex embedding network with significantly more training time is not possible within the rebuttal time. \n\n* We make it more clear now in the results section that it implicitely incorporates an ablation study. We showcase the outcomes of our method for (A) solely Mean Squared Error (MSE), (B) FaceNet loss based on MSE, and (C) the combined version. It's crucial to emphasize that (C) is not the result of additional training but rather a new inference method. The intentional demonstration of the combined version (C) outperforming the results obtained by training for the two losses sequentially (B) serves as evidence that our combination method enhances overall performance.\n\n* Indeed, all our attacks are directed at a trained ResNet. While the two ResNets we employed were trained on different datasets, we haven't specifically evaluated potential differences stemming from distinct encoder structures. Analyzing various architectures is undoubtedly a crucial research question. However, it may divert attention from the primary message we aim to convey in our paper: the existence of a widely used network, that is susceptible to unintended information leaks. Even if this vulnerability might not apply universally to all images and encoder networks, the possibility of information leakage, to the extent of identifying individuals, challenges the assumption that an abstract feature vector can be considered a secure representation of an image. The logical next step would be to develop encoders that are resilient against such attacks.",
                "reply_to": "8S5FMudbgO"
            },
            {
                "review_id": "Xkbp3OuwRZ",
                "content": " Thank you for your valuable advice concerning the conclusion. We made a major revision of the conclusion and hope, that the main points of our study become clearer now. Note that we also performed an additional user study to underline our main message: a reconstruction can be accurate enough to identify the subject, even though the attribute vector was not meant to provide such information.",
                "reply_to": "k1f7HXsK7R"
            },
            {
                "review_id": "XR1ZeaZz2c",
                "content": " Thank you for your detailed feedback. We have updated our paper to include the suggested improvements:\n\n* \u201cThe experimental evaluation is highly inadequate\u201d:\nWe have incorporated the quantitative evaluation of (Yang et al., 2019) into our paper. However, a notable issue with their metric is its inclination towards favoring blurred images, which in fact do not provide distinguishing features. This might be missleading and is the reason why we have chosen to present these results in a dedicated paragraph. Assessing the perceptual differences between the two distinct image distributions proves challenging for standard image comparison metrics. We now add a user study, since this seems indeed to be best comparison or even gold standard for the purpose of our work. We appreciate your suggestion.\n   \n  \n* \u201cThe paper has also inadequacies in terms of discussing and citing prior art.\u201d:\nWe have chosen to confine the detailed section on related work in the main paper to research conducted within the same setting as our work. Strategies like those presented by (Razzhigaev et al. 2020) deviate fundamentally since they aim to reconstruct from facial embeddings, which are designed exactly for encoding distinctive facial features and are significantly higher-dimensional compared to our low-dimensional attribute vectors. In this respect our problem is much harder so solve. While we acknowledge the significance of publications addressing slightly different requirements, we believe that providing a more detailed description in the main body might divert attention from the primary message of our paper. Nonetheless, recognizing the interest these publications may hold, we have included a more comprehensive discussion of these less closely related papers in the Appendix for the benefit of inquisitive readers.\n\n* Ethics Concerns:\nThe Appendix now additionally includes a Section about ethical concerns. \n\n* Concerning the technical novelty:\nWe see the key elements of our contribution in two aspects: firstly, the fusion of ensemble outputs through stylemixing, and secondly, the innovation of mapping images into a more representative latent space during the training process. Our research has not revealed prior instances of these concepts in our specific research domain.",
                "reply_to": "j6W1i4PDWz"
            },
            {
                "review_id": "j6W1i4PDWz",
                "content": "This paper studies the unintentional information leakage that can happen in deep encoder networks that extract latent representations with abstract attributes from face images. The paper proposes a method that is capable to reconstruct an input face image from a feature vector representation using only black box access to the image encoder. The method is based on the StyleGAN formulation, which is extended with an additional loss that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. The purpose of this paper is to raise awareness about the relevant security issues of existing deep learning systems for face analysis. + This paper deals with an interesting and important problem that has attracted limited attention from the computer vision community. It is particularly important for reasons related to security and preservation of privacy. \n\n+ The proposed pipeline is intuitive and sound, building upon the formulation of the StyleGAN model. - The technical novelty of the proposed method is relatively limited. It only describes a small extension of the loss function of the StyleGAN model. It is mostly interesting as an application of the GAN-based formulations, but I think that it lacks sufficient contributions for a paper accepted in ICLR. Other venues might be more appropriate for such paper. \n\n- The experimental evaluation is highly inadequate. The only quantitative evaluation is the one presented in Table 1. However, this corresponds to an internal evaluation of the proposed method, without any comparison with other SOTA methods. Closely related methods like (Yang et al., 2019) and (Zhao et al. 2021) should have been included in the quantitative comparisons. In addition, a perceptual user study should have been included in the experiments, in order to quantify the performance of the proposed method and other compared methods, in terms of whether the reconstructed faces are perceived by humans to have the same identity as the original real faces. \n\n- The paper has also inadequacies in terms of discussing and citing prior art. First, Some closely-related works, like (Razzhigaev et al. 2020) are only presented in Table 2 of the Appendix. However, such works should have been presented in the main paper, with discussion about their similarities and differences from the proposed method. Furthermore, the paper has not cited some closely-related works like the following:\n\nKhosravy, M., Nakamura, K., Hirose, Y., Nitta, N. and Babaguchi, N., 2022. Model inversion attack by integration of deep generative models: Privacy-sensitive face generation from a face recognition system. IEEE Transactions on Information Forensics and Security, 17, pp.357-372.\n\nKhosravy, M., Nakamura, K., Nitta, N. and Babaguchi, N., 2020, December. Deep face recognizer privacy attack: Model inversion initialization by a deep generative adversarial data space discriminator. In 2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) (pp. 1400-1405). IEEE.",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "k1f7HXsK7R",
                "content": "The paper investigate unintentional leakeage from a feature used for face recognition in terms of soft biometrics. The problem is approached using a StyleGAN to build the invertible networks. The method is evaluated on two scenarios in conjunction  with CelebA database. 1. The idea to investigate leakage in face recognition modules is very interesting. Approach and results are also welcomed \n\n2. Additional results  from the appendix help to understand better the problematic. 1. The paper has no \"Conclusion\" section. While it formally exists, it repeats the main steps of the paper. It is not clear to me what is the lesson learned. The paper is entitled \"REVEALING UNINTENTIONAL INFORMATION LEAKAGE ...\" and after reading I do not have a clearer view of what has been leaked and what is not\n\nMinor comments:\n - section 2 \"Thread\" - \"threat\" ?",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "8S5FMudbgO",
                "content": "This paper introduces a novel face generation method from a low-dimensional (32 / 40-element) feature vector. The low dimensional feature vector is intended to only describe the attribute information, however the proposed method is able to generate a face that has high similarity with the target image from the encoded feature vector. Thus, it is able to reconstruct the unintentional private information, such as identity. The proposed method utilizes style-GAN and a new loss function based on the face perceptual similarity. In addition, a novel technique to fuse multiple layers of Style-GAN is introduced. + The proposed method tackles a difficult problem where it wants to reconstruct a target face image from a low-dimensional feature vector. The method works because it effectively designs the framework so that it limits the search space. The feature vector (f) is mapped into a generator input (n) so that the generator (G) is able to produce consistent face image. Thus, the mapping function (P) is trained so that the feature vector can be used to generate similar images as the target image. \n\n+ The loss functions consist of pixel-wise and Facenet embedding losses. While the pixel-wise loss is used to maintain the overall image similarity, the Facenet embedding loss is used to preserve the facial features similarity, such as shape of the eyes, lips, etc. As each block in the StyleGAN defines different features of an image, the mapping function (P) is trained using different losses for each block as mentioned in Section 3.4. Thus, Facenet embedding loss is applied to second and third block which are in charge of the facial features. \n\n+ Experimental results show that the proposed method overcomes the predecessor (Yang et al - 2019) qualitatively. It is able to reconstruct clear and not-blurred face image that has similarity with the target image. In addition, the experiments show that the proposed method achieves low embedding error of the extracted VGG-Face and OpenFace feature embedding. It shows that the generated face has high similarity with the target image. - The paper only measures the overall error of the generated dataset. Figure 4 & 5 needs to add the similarity score to give a rough idea about how similar the generated image with the target image. It is unclear how the last column in the figures can be noted as similar person. Note that the paper needs to justify that the private information is leaked in the reconstructed image. \n\n- The paper utilizes different loss functions for each block in StyleGAN. However, there is a lack of justification of the results. The ablation study is required to justify the decision.\n\n- The paper shows that the training of D and E on different dataset is more difficult than on the same dataset. While the quantitative shows the differences, however it is not reflected in qualitative evaluation. In addition, it leads to the question whether the OpenFace and VGG-Face embedding is suitable for measuring the similarity. It is recommended to follow [A] procedure to measure the possibility of identity leakage in the encoder. \n\n- As mentioned in [A], different encoder (face feature generator) might lead to different conclusion in terms of identity leakage. Thus, it is important for the authors to measure the possibility of different face attribute encoder methods. In addition, it is also important to use the more challenging face embedding encoder, like ArcFace or SphereFace to evaluate the similarity between images. \n\n- There are several references that are able to reconstruct face from attribute feature vector [B, C]. Those are might be used for better methods to compare.  \n\nAdditional references:\n* [A] This Face Does Not Exist... But It Might Be Yours! Identity Leakage in Generative Models, WACV 2021\n* [B] Attributes Aware Face Generation with Generative Adversarial Networks, ICPR 2020\n* [C] Latent Vector Prototypes Guided Conditional Face Synthesis, ICIP 2022",
                "reply_to": "iclr_2024_48CXLrx7K3"
            }
        ],
        "generations": {
            "human_reference": "This paper proposes a method that converts a low-dimensional feature vector from a black-box encoder into a portrait image. Specifically, given a feature vector from an encoder, it learns a mapping function that maps the vector into the latent space of a pre-trained StyleGAN model, then convert the latent code to a portrait image. The main novelties include: a) introducing StyleGAN as a strong prior, b) incorporating different loss functions and compare the generation and ground truth images in different prior spaces (e.g., face recognition models) and c) specialize the mapping function for different layers in StyleGAN.\nThis paper raises an important and interesting question about unintentional identity leaking in current portrait auto-encoders. It also proposes a valid solution to prove this opinion. These strengths have been recognized by all reviewers.\nHowever, reviewer nZtS 5Uwq both raised reasonable issues of the submission including: a) limited technical novelty, b) missing evaluations.",
            "random_reference": "This paper presents a universal spike-count model for flexibly describing both over-dispersion and under- dispersion in neural spiking data. The model is presented with an efficient variational fitting method, and can be applied to data without known stimuli, as is often the case for overdispersion metrics. The reviewers recognized the novelty of the model, and the topic fits nicely in the probabilistic modeling and neuroscience focuses in NeurIPS. Moreover spike-count models are an ongoing discussion in the computational neuroscience community (heavily represented at NeurIPS) and this work presents a new and interesting addition to the literature. I therefore recommend this work be accepted in NeurIPS.",
            "modular_llama3": "This manuscript proposes a novel method for reconstructing a target face image from a low-dimensional feature vector, addressing an important problem related to security and privacy preservation in the computer vision community. While the approach is interesting and leverages recent knowledge in image generation and facial similarity, outperforming the current state-of-the-art, the paper has several significant inadequacies. The experimental evaluation is inadequate, lacking comparison with state-of-the-art methods and clear conclusions, which raises questions about the validity of the findings. Additionally, the discussion of prior art is insufficient, and the structure and content of the paper are not suitable for this venue. The authors need to provide more justification and ablation studies for their approach to strengthen the manuscript.",
            "llama3_pr_chunk": "This paper evaluates information leakage from neural networks by reconstructing input images from feature vectors using StyleGAN, FaceNet embedding, and a novel loss function. The authors propose a new method for image reconstruction and introduce a technique to generate specific aspects of the recreated image. The paper highlights security issues in deep learning systems for face analysis, but its technical novelty and experimental evaluation have been criticized. The authors have revised the paper based on reviewer suggestions, adding user studies, quantitative evaluation metrics, and discussion of prior art. The proposed method outperforms its predecessor in reconstructing target face images, but further studies are recommended to measure identity leakage and evaluate similarity using more challenging face embedding encoders.",
            "llama3_pr_consolidation": "This manuscript presents a method to reconstruct face images from low-dimensional feature vectors, raising concerns about unintentional information leakage in deep encoder networks. Reviewers acknowledge the importance of the problem and the soundness of the proposed pipeline, which leverages StyleGAN and a new loss function to preserve facial features similarity. However, the technical novelty of the method is considered limited, and some reviewers question its suitability for a top-tier conference.\n\nThe experimental evaluation is a major concern, with reviewers pointing out the lack of comparison with state-of-the-art methods, inadequate quantitative evaluation, and the need for a perceptual user study to quantify the performance of the proposed method. Additionally, the paper's discussion of prior art is considered inadequate, with some closely related works not properly cited or discussed.\n\nDespite these concerns, reviewers appreciate the introduction of a novel technique to fuse multiple layers of StyleGAN and the use of FaceNet embedding loss to preserve facial features similarity. The experimental results show that the proposed method can reconstruct clear and similar face images, but the evaluation could be improved by adding similarity scores and justifying the decision to use different loss functions for each block in StyleGAN.\n\nOverall, the manuscript raises important security concerns, but its technical contributions and experimental evaluation need to be strengthened to meet the expectations of a top-tier conference.",
            "llama3_pr_knowledge": "**Advancement:** The reviewers found the problem investigated in the paper to be interesting and important for security and preservation of privacy, but the technical novelty of the proposed method is relatively limited. The paper's contributions are mostly interesting as an application of GAN-based formulations.\n\n**Clarity:** The reviewers pointed out several issues with the presentation, including the lack of a clear conclusion section, unclear justification of results, and the need for more detailed explanations of the loss functions and ablation study. However, the reviewers also praised the clarity of the proposed pipeline and the effectiveness of the framework in limiting the search space.\n\n**Compliance:** The reviewers noted that the paper has inadequacies in terms of discussing and citing prior art, with some closely-related works only presented in the appendix or not cited at all.\n\n**Soundness:** The reviewers found the proposed method to be sound, with the use of StyleGAN and a new loss function based on face perceptual similarity being a good approach to the problem. However, the reviewers also pointed out the need for more rigorous evaluation, including a perceptual user study and comparison with other SOTA methods.\n\n**Novelty:** The reviewers found the idea of investigating unintentional information leakage in face recognition modules to be novel and interesting, but the technical novelty of the proposed method is relatively limited. The paper's contributions are mostly interesting as an application of GAN-based formulations.",
            "llama3_pr_decomposed": "This paper contributes to the field of secure encoding methods, particularly in the context of face recognition and privacy, by demonstrating the effectiveness of a novel encoding approach that can resist state-of-the-art attacks, thereby providing a crucial step towards protecting sensitive facial information and upholding individual privacy.",
            "modular_gpt4": "The manuscript introduces a novel method for face generation from low-dimensional feature vectors, emphasizing advances in image generation and facial similarity. The primary contributions include fusing ensemble outputs via stylemixing and mapping images into a representative latent space. While the approach is technically robust and addresses information leakage effectively, it may be better suited for venues other than ICLR and lacks comprehensive comparisons with existing methods. The method outperforms current state-of-the-art techniques and is supported by user studies and additional metrics that underscore its accuracy and privacy implications. However, the paper still requires a more thorough conclusion, discussion on related works and alternatives, and detailed clarifications on methods, particularly concerning facial embeddings and loss functions. Ethical concerns are addressed, though more appropriate venues might be recommended. Minor textual corrections are also needed."
        }
    },
    "index_24": {
        "paper_id": "iclr_2020_HkejNgBtPB",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "rkxYRkH3sH",
                "content": "Thanks very much for your valuable comments.\n\nQ: It would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. \n\nA: We compare the training and testing time cost on the WIKI dataset, and with raw data added, VTM spends more time on training but the same time on generation as Table2seq. Here is the detailed time spent ( train and test on single Tesla V100 GPU), for test computational cost, we record how much time to generate 72k sentences.\n\n                |          Table2seq              |       VTM-noraw               |              VTM\n--------------------------------------------------------------------------------------------------------------------\nTrain        |  \uff5e30 mins (6 epochs)  | \uff5e30 mins (6 epochs)  |  \uff5e160 mins (15 epochs)\n--------------------------------------------------------------------------------------------------------------------\nTest         |          ~80min                  |         ~80min                     |           ~80min \n\nVTM gives the same speed for generating sentences, but it takes more time for training, which are cost to learn the large-scaled unlabeled data, and is acceptable.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In the experiments (result see Section 4.3, Figure 3), we control the same decoding strategy under the same temperature, and plot their BLEU and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. \n",
                "reply_to": "B1x-h8r9dH"
            },
            {
                "review_id": "HJepPJHniB",
                "content": "Thanks a lot for your insightful comments. In the following parts, we will response to your questions one by one.\n\nQ: Quality-diversity trade-off.\nA: \nQuality and diversity is a trade-off in the text generation. As we can find in Table 3 and 6, beam search (or greedy) always receives a higher quality and low diversity whereas forward sampling method can diversify the generation but with a relatively lower quality. Decoding strategy largely interferes our judgment. A fairer comparison is to keep the same decoding strategy then comparing the quality and diversity.  \n\nTherefore, we add extensive experiments in Section 4.3 (results refer Figure 4), by applying same decoding method -- sampling under the same softmax temperature, we plot their BLEU scores and Self-BLEU scores in Figure 3, which shows that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\n========\n\nQ: Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\nA: \nThanks very much for your kind notes, We have updated the paper, adding extensive experiments on other sampling methods and human evaluation. Results show that our method consistently outperforms the baseline model (Table2Seq).\n\nFirstly, we sample from the softmax with different temperatures (from 0.1 to 1.0) and plot the BLEU and self-BLEU trade-off curve in Figure 3. The trade-off curves show that compared to Table2seq, VTM always give better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\nSecondly, human evaluation also shows that VTM can generate sentences with better accuracy and coherence. Besides, comparing to the model without raw data (VTM-no raw), there is a significantly large improvement in diversity. \nAlthough Table2seq with forward sampling has the highest diversity rating, its quality much worse than VTM. High quality is meaningful when the output quality is good enough.\n\n========\nQ: A way to incorporate this unlabeled data to Table2Seq is by first pretraining the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM? \nA: \nYes, pretraining the decoder with large-scaled unlabeled data can be another alternative for including the effectiveness of the large scaled unlabeled data. However, we did a quick run and experimental results in Table 3 and Table 6 show that directly applying decoder pretraining does not get performance gain (even worse than the baseline), which might be caused by the gap between pretraining the generator as a language model and the data-to-text task.\nHere we list the BLEU and self-BLEU scores: \n\nDataset |           Model             |      BLEU    |   Self-BLEU\n-----------------------------------------------------------------------------\nWIKI      |  Table2seq-beam    |     26.74     |     92.00\n               | Table2seq-pretrain|     25.43     |     99.88   \n               |            VTM               |     25.22     |     74.86\n-----------------------------------------------------------------------------\nSPNLG   | Table2seq-beam      |     40.61    |     97.14   \n               | Table2seq-pretrain  |     40.56    |    100.00  \n               |            VTM                 |      40.04   |     88.77\n\n========\n\nQ: In the conclusion section, what does this VAE model refer to?\nA: Sorry for misleading, VAE model refers to the VTM without using raw data (i.e. VTM-noraw). We've fixed it in the updated version.\n\n=======\n\nQ: I am not convinced that the proposed method is a significant development based on the results presented in the paper. \nA: \nAs introduced in the quality-diversity trade-off, VTM tends to generate more diverse outputs with the same quality (diversity is important for text generation). \nOur proposed VTM can make full use of the raw data to learn an informative template space, and largely enrich the template of generated sentences, thus boost the diversity. VTM can be also a new approach to include unlabeled data for text generation in the VAE framework.\nTo our best knowledge, there is no related work using the similar idea in the data-to-text generation.\n\n=======\n\nQ: There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.\nA: Thanks, we will proof-read carefully and fix typos in the next version.\n",
                "reply_to": "HJlIc6pdYH"
            },
            {
                "review_id": "HJg5xRE3jS",
                "content": "Thanks very much for your valuable comments. \n\nQ: How does the method generalize to other languages? How does it scale with (the lack of) resources?\nA: Our method could be easily generalized to other languages because no language-specific processing or resources are used. Additionally, our proposed VTM may well fit languages with fewer resources, in which case the VTM model with massive raw data (usually cheap to obtain) may significantly boost the finally performances when labeled data are hard to get.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In these experiments (result see Section 4.3, Figure 3), we control the decoding strategy with the same temperature, and plot their BLEU scores and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. ",
                "reply_to": "SkeetYT-5H"
            },
            {
                "review_id": "ryxcKa4hsr",
                "content": "Hi, all. Thanks for reviewing my paper. We've uploaded a new version of our draft, adding more experiments:\n- Experiments on the computational cost of the models. (Table 4, Page 7)\n- Experiments on quality-diversity trade-off. (Figure 3, Page 6)\n- Human evaluation on generation accuracy, coherence and diversity. (Table 7, Page 9)\n Please take a look\uff01",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "B1x-h8r9dH",
                "content": "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.\nThe proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments. \nThe derivations are correct; this concerns both the model definition and the algorithmic derivations of model training and inference.\nThe experimental evaluation is adequate: it compares to many popular approaches and on several datasets; the outcomes are convincing.\nIt would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. ",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "HJlIc6pdYH",
                "content": "This paper proposes Variational Template Machine (VTM), a generative model to generate textual descriptions from structured data (i.e., tables). VTM is derived from the variational autoencoder, where the input is a row entry from a table and the output is the text associated with this entry. The authors introduce two latent variables to model contents and templates. The content variable is conditioned on the table entry, and generates the textual output together with the template variable. The model is trained on both paired table-to-text examples as well as unpaired (text only) examples. Experiments on the Wiki and SpNLG datasets show that models generate diverse sentences, and the overall performance in terms of BLEU is only slightly below the best baseline Table2Seq model that does not generate diverse sentences. The results also show that additional losses for preserving contents and templates introduced by the authors play an important role in the overall model performance.\u00a0\n\nI have several questions regarding the experiments:\n- For the Table2Seq baseline, how was the beam size chosen? Did it have any effect on the performance of the baseline model?\n- Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\n- VTM is only able to achieve comparable performance to Table2Seq in terms of BLEU after including the unlabeled corpus, especially on the Wiki dataset. A way to incorporate this unlabeled data to Table2Seq is by first pretraining\u00a0the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM?\n- In the conclusion section, the authors mentioned that VTM outperforms VAE both in terms of diversity and generation quality. What does this VAE model refer to? The experiments show that VTM is comparable to Table2Seq in terms of quality and is better in terms of diversity.\u00a0\n\nGenerating text from structured data is an interesting research area. However, I am not convinced that the proposed method is a significant development based on the results presented in the paper. There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "SkeetYT-5H",
                "content": "The paper proposes an approach to generate textual descriptions from structured data organized in tables, by using a \"variational template machine\" (VTM), which is essentially a generative model to separately represent template and content as disentangled latent variables to control the generation.\n\nThe contribution is well-written and well-motivated, the model exposition is clear, and the results are convincing. The experiment setup, depth, and breadth are particularly convincing. I see no reason to not accept this paper.\n\nRemarks:\n- It should be clearly stated which languages feature in the paper. From what I gather, it's only English. How does the method generalize to other languages? How does it scale with (the lack of) resources?",
                "reply_to": "iclr_2020_HkejNgBtPB"
            }
        ],
        "generations": {
            "human_reference": "The paper addresses the problem of generating descriptions from structured data. In particular a Variational Template Machine  which explicitly disentangles templates from semantic content. They empirically demonstrate that their model performs better than existing methods on different methods. \n\nThis paper has received a strong acceptance from two reviewers. In particular, the reviewers have appreciated the novelty and empirical evaluation of the proposed approach. R3 has raised quite a few concerns but I feel they were adequately addressed by the reviewers. Hence, I recommend that the paper be accepted. ",
            "random_reference": "This paper studies the problem of decongestion by representation, designing the information representations for items in online market places to improve social welfare. Through a differential proxy for the objective, the authors propose to learn the design of representations from past customer data. The proposed approach is examined through extensive experiments both on simulated data and real data.\n\nOverall all reviewers agree that the paper is addressing a practically relevant question in online market places. The proposed approach is sound and the evaluations are generally convincing. There have been various comments and suggestions regarding the framing and presentations of the paper, however, they all seem addressable in the revisions, as demonstrated in the author responses.",
            "modular_llama3": "This manuscript proposes a novel Variational Template Machine (VTM) approach for data-to-text generation, addressing the issue of lacking diversity in existing methods. The proposed method is well-written and well-motivated, with technical derivations correctly presented. Experimental results demonstrate the effectiveness of VTM in generating diverse and high-quality outputs, outperforming existing methods and showing a favorable quality-diversity trade-off. However, the significance of the proposed method is debated among reviewers, with some finding it convincing and others not entirely convinced by the results. Additionally, the manuscript requires improvement in terms of grammar, with numerous errors noted by reviewers. Overall, the manuscript has potential, but the authors should address the grammatical errors and provide further clarification to strengthen the significance of their contribution.",
            "llama3_pr_chunk": "The Variational Template Machine (VTM) is a novel method for generating text descriptions from data tables using reusable templates learned from paired and non-paired data. The authors addressed reviewer comments, providing additional experiments and results that show VTM outperforms the baseline model, Table2Seq, in terms of quality and diversity. The paper demonstrates VTM's generalizability to other languages and its ability to generate sentences at a comparable speed to Table2Seq. While the paper is well-written and convincingly presented, reviewers suggest improvements, including addressing grammatical errors and exploring alternative sampling methods.",
            "llama3_pr_consolidation": "This paper proposes a Variational Template Machine (VTM) for generating textual descriptions from structured data in tables, aiming to address the issue of lacking diversity in existing approaches. The authors' contributions include a novel model architecture and losses to disentangle text template and semantic content information, as well as utilizing both paired and non-paired data to enrich template learning. The experiments on various datasets show that VTM generates more diverse texts while maintaining good fluency and quality.\n\nThe reviewers generally find the paper well-motivated, well-written, and convincing. They appreciate the clear model exposition, the convincing experimental results, and the comparison to popular approaches. However, some reviewers suggest that the paper could be improved by providing an analysis of computational costs, exploring other sampling methods for the baseline model, and clarifying the writing.\n\nOne reviewer questions the significance of the proposed method, considering the results presented in the paper. They also point out grammatical errors that need to be fixed. Another reviewer suggests experimenting with pretraining the LSTM generator on unlabeled data before training it on paired data, which could potentially improve the baseline model's performance.\n\nThe authors respond to the reviewers' comments by providing additional experiments, including an analysis of computational costs, exploring other sampling methods, and human evaluation. They also clarify that their method can be easily generalized to other languages and can perform well with limited resources.\n\nOverall, the reviewers find the paper solid and convincing, with some minor suggestions for improvement. They believe that the paper contributes to the field of text generation from structured data and that the proposed method has potential applications.",
            "llama3_pr_knowledge": "**Summary of Review Aspects**\n\n**Advancement**: The proposed Variational Template Machine (VTM) is a novel approach for generating text descriptions from structured data, with a significant contribution to the field. However, one reviewer questions the significance of the development based on the results presented.\n\n**Clarity**: The writing of the paper is clear and well-motivated, but some grammatical errors are present. The model exposition is clear, and the results are convincing. However, the paper could benefit from proof-reading to fix typos and improve the writing.\n\n**Compliance**: The paper meets the venue's requirements, and all ethical and publication requirements are met.\n\n**Soundness**: The proposed model is sound and appropriate, relying on state-of-the-art methodological arguments. The derivations are correct, and the experimental evaluation is adequate.\n\n**Novelty**: The VTM approach is original, and the idea of using a variational autoencoder to model contents and templates is novel. The results show that VTM outperforms baseline models in terms of diversity and quality.",
            "llama3_pr_decomposed": "The proposed method, VTM, represents a significant development in the field of data-to-text generation, as it effectively utilizes raw data to learn an informative template space, thereby enriching the template of generated sentences.",
            "modular_gpt4": "The manuscript introduces the Variational Template Machine (VTM), a novel method for generating text descriptions from data tables by separating text templates and semantic content in latent spaces. Reviewers praise its innovative approach, leveraging both parallel and raw data to enhance template learning and sentence diversity, which addresses an underexplored problem in the data-to-text generation field. Experiments detail its superior performance, consistently showing improved text diversity and quality compared to existing methods like Table2Seq and Table2text. Despite some concerns about its significance and longer training times, the model's soundness and adaptability to various languages are highlighted. Writing improvements and additional clarifications on language usage are suggested. Overall, the paper is recognized for its significant contribution and is recommended for publication with minor revisions."
        }
    },
    "index_85": {
        "paper_id": "iclr_2024_Hh0Cg4epYY",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems. Utilizing f-divergence bounds to gauge the dissimilarity between distinct distributions, we establish an upper bound for Bayes error. This bound serves as a criterion for neural network training and test data classification. We showcase this technique's applicability to both binary and multi-class cases, examining the network output against a specific threshold for classification. Experimental results substantiate the method's effectiveness in approximating Bayes error. These experiments focus on Gaussian distributions with disparate means but identical variance, comparing the outcomes with theoretical Bayes error. Finally, the paper explores the potential applications of this approach in Generative Adversarial Networks (GANs), offering a promising avenue for future research.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "Wx2gxGMlC4",
                "content": " Dear Reviewers,\n\nThank you for your valuable feedback on our manuscript. We have diligently revised the paper to address your concerns and enhance its overall quality. Here is a summary of the significant changes we have implemented:\n\n1. Enhanced Clarity and Detail: We have comprehensively revised the technical content for improved clarity. Definitions and detailed explanations have been added, especially for mathematical quantities and equations, to make the methodology and objectives of our work more understandable.\n\n2. Terminology Adjustment: In the abstract, we have replaced the term \"groundbreaking\" with \"novel,\" aligning the description more accurately with the nature of our work.\n\n3. Organizational Structure: An \"Organization\" paragraph has been included at the end of the Introduction section, outlining the structure and content of each subsequent section, providing a clear roadmap for readers.\n\n4. Theoretical Results as Theorems: Key theoretical results have been presented as formal \"theorems,\" with precise assumptions and conclusions, to emphasize their significance and align with academic standards.\n\n5. Resolution of Specific Queries: We have addressed all specific questions about our methodology and equations to ensure each aspect of our approach is clear and justified.\n\n6. Goals and Contribution Clarity with Bayes GAN Finding: The goals and contributions of our paper have been further clarified, especially highlighting our findings with the Bayes GAN method. Our work in estimating Bayes error using neural networks is a significant advancement, as Bayes error is the optimal statistical error achievable in classification tasks. This neural estimation of Bayes error underscores the theoretical and practical significance of our method. In the domain of Generative Adversarial Networks (GANs), our Bayes GAN method demonstrates a notable achievement by consistently achieving lower FID (Fr\u00e9chet Inception Distance) scores on the MNIST dataset, signifying a closer resemblance to real image distributions and therefore enhanced realism in generated images. This underscores our method's practical utility in advancing generative models.\n\nWe believe that these revisions effectively address the points raised in your reviews, enhancing both the clarity and the substantive contribution of our paper. We appreciate your guidance and are confident that our responses and the amendments made reflect our dedication to contributing meaningful and high-quality research to the field.\n\nThank you for your time and consideration.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "wIXISWv7ld",
                "content": "The paper is obviously incomplete and more like a research note. I do not see any merit in the current format. None The paper is obviously incomplete and more like a research note. I do not see any merit in the current format.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "N53343EVgK",
                "content": "The paper considers bounding the Bayes error via f-divergence. The paper is incomplete and merely a draft. The problem of upper bounding Bayes error is somewhat interesting, though I am not sure how important it is. The paper does not present sufficient evidence in handling it. The math in the paper is very unclear.\nBayes error is the minimum achievable error. What is the usage of characterizing Bayes error, for model selection? \nUpper bounding generalization error via f-divergence has been previously well-studied.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "mpzI6nvJ1c",
                "content": "In the context of binary and multi-class classification, this paper derives an upper bound onthe Bayes risk in terms of an $f$-divergence (unfortunately, I couldn't quite understand the inputs to the $f$-divergence). The paper then proposes to train neural networks for classification, and GANs for generative modeling, using this $f$-divergence. Experimental results demonstrate that the proposed methods work (a) for classifying two Gaussian classes, (b) for the MNIST digit classification task, and (c) for generating novel MNIST samples. The overall approach seems like an interesting idea, as it suggests a new objective for optimizing complex classifiers, such as neural networks. Overall, the paper was insufficiently detailed for me to understand either its technical content or its high-level goals.\n\nMany quantities are used without being defined (e.g., in Eq. (1), what are $P$, $Q$, and $X$, what conditions is $f$ assumed to satisfy, etc.).\n\nAt a higher level, it is not clear to me what the goal of this paper is, in relation to the larger literature on machine learning (e.g., what gap or limitation of existing methods does this seek to fill?). Providing some quantitative evaluation comparing the proposed method and existing state-of-the-art methods might be one way to help address this.\n\nSome suggestions on improving the presentation:\n1) In the first sentence of the abstract, \"groundbreaking\" is probably too strong of a word; \"novel\" would be more appropriate.\n2) Typically, square brackets (e.g., \"[7]\") are used for numerical literature citations, while parentheses are reserved for referencing equations (e.g., \"Eq. (7)\").\n3) I suggest adding an \"Organization\" paragraph at the end of the Introduction Section explaining the content and goals of each of the subsequent sections of the paper.\n4) The key novel theoretical results (e.g., new upper bounds such as Eqs. (15) or (23)) should be presented as a self-contained \"theorem\" (e.g., using ```\\begin{theorem} ... \\end{theorem}```), including the precise assumptions made and conclusions drawn.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            }
        ],
        "generations": {
            "human_reference": "This paper studies how to approximate the upper limit of Bayes error in classification tasks. The reviewers raised significant concerns about the technical quality, novelty, evaluations, and completeness of this submission. Specifically:\n\n(1) Multiple reviewers found the mathematical development unclear or insufficiently detailed. \n(2) It is unclear how the proposed method relates to or improves upon existing techniques for bounding generalization error.\n(3) No experiments quantitatively compare to prior state-of-the-art methods.\n(4) Reviewers commented that the submission resembles an incomplete research note more than a full paper. \n\nWhile I appreciate the authors' efforts, given these major issues, I do not believe this work is suitable for presentation at this time.",
            "random_reference": "This paper studies the impact of network width and depth on the robustness of a model through adversarial training. Different from previous works that looked into the impact by varying the overall width and depth of a network, the authors perform stage-level experiments in which they modify the width and depth of different stages within a model. The findings, which indicate that decreasing the depth and width at the last stage can lead to higher robustness while decreasing depth and width at earlier stages of the network hurts robustness, are useful to the community. However, the upper bound on the Lipschitz constant proposed in this work does not seem to explain the empirical results well. We encourage the authors to discuss the consistency between the theoretical and empirical results more if the paper is finally accepted.",
            "modular_llama3": "The manuscript proposes a novel method for estimating the upper bound of Bayes error in classification tasks using f-divergence bounds and neural networks, offering a significant advancement in approximating optimal statistical error and demonstrating potential applications in Generative Adversarial Networks (GANs). However, despite improvements in clarity and methodology justification, the paper still requires substantial revisions to address several key issues. Specifically, the manuscript needs further quantitative evaluation, clearer mathematical notation, and a more self-contained presentation of novel theoretical results. Additionally, some reviewers found the paper incomplete and unclear, suggesting the need for enhanced presentation and comparison with state-of-the-art methods. To fully realize its potential, the authors should provide more detail and evidence, define key quantities, and formally present key results as theorems.",
            "llama3_pr_chunk": "This paper proposes a technique to approximate the upper limit of Bayes error in classification tasks using f-divergence bounds and demonstrates its effectiveness in experiments, with potential applications in GANs. However, the paper is incomplete, lacks clarity, and has insufficient evidence to support its claims, requiring significant revisions for improvement.",
            "llama3_pr_consolidation": "This manuscript has undergone significant revisions to address initial concerns, resulting in improved clarity and detail. The authors have reorganized the structure, provided clear definitions, and presented key theoretical results as formal theorems. The paper now effectively communicates its novel approach to approximating Bayes error using f-divergence bounds for classification tasks, including binary and multi-class problems.\n\nThe authors have demonstrated the applicability of their technique in various experiments, showcasing its effectiveness in approximating Bayes error. The exploration of potential applications in Generative Adversarial Networks (GANs) is particularly promising, with the Bayes GAN method achieving lower FID scores on the MNIST dataset. This underscores the method's practical utility in advancing generative models.\n\nHowever, some initial reviewers questioned the paper's completeness and the significance of upper bounding Bayes error. The usage of characterizing Bayes error for model selection was also unclear. Additionally, the math was initially unclear, and the paper's connection to the larger literature on machine learning was not well-established.\n\nOverall, the revised manuscript has addressed many of these concerns, presenting a more comprehensive and well-structured work. The authors have effectively clarified their goals and contributions, highlighting the theoretical and practical significance of their method. While some reviewers remain skeptical about the paper's overall impact, the revisions have substantially improved its quality and coherence.",
            "llama3_pr_knowledge": "The reviewers' feedback highlighted several areas for improvement in the manuscript, including its overall completeness, clarity, and contribution to the field. Initially, the paper was deemed incomplete and lacking in merit, with unclear math and insufficient evidence to handle the problem of upper bounding Bayes error. The reviewers also questioned the importance of characterizing Bayes error for model selection and noted that upper bounding generalization error via f-divergence had been previously well-studied. \n\nHowever, the reviewers also acknowledged the paper's potential, suggesting that the approach could be an interesting idea for optimizing complex classifiers and proposing a new objective for neural networks and GANs. To improve the paper, the reviewers recommended enhancing clarity, defining technical terms, and providing a clear roadmap for the reader. They also suggested presenting key theoretical results as formal theorems, addressing specific queries about the methodology, and clarifying the goals and contributions of the paper.\n\nIn response to the feedback, the authors made significant revisions to enhance clarity, detail, and overall quality. They replaced the term \"groundbreaking\" with \"novel\" in the abstract, added an \"Organization\" paragraph to the introduction, presented key theoretical results as formal theorems, and addressed specific queries about the methodology. The authors also clarified the goals and contributions of the paper, highlighting the significance of their method in estimating Bayes error using neural networks and its potential applications in Generative Adversarial Networks (GANs).",
            "llama3_pr_decomposed": "Unfortunately, I have no text to summarize. Please provide the necessary text, and I will be happy to assist by distilling the summary into a concise and clear paragraph that captures the essence of the paper and its significance.",
            "modular_gpt4": "The manuscript introduces a novel technique for estimating the upper limit of Bayes error in classification tasks, particularly within the context of GANs, resulting in lower FID scores and enhanced realism of generated images on the MNIST dataset. While the paper demonstrates notable theoretical advancements and practical implications for neural network estimation, reviewers unanimously suggest that significant revisions are necessary for clarity and completeness. Key issues include the need for clearer articulation of the paper\u2019s goals, improved structuring of theoretical contributions, more detailed explanations, and high-level contextualization within existing machine learning literature. The paper is also advised to include quantitative evaluations against state-of-the-art methods. Addressing these critiques and providing reflections on identified limitations will substantially improve the manuscript, making it a noteworthy contribution to the field."
        }
    },
    "index_9": {
        "paper_id": "iclr_2020_B1xmOgrFPS",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "rJe_tNnijS",
                "content": "Thanks for the comments! We agree with your concerns, and would like to offer clarifications for a clearer understanding. \n\nTo do a novel few-shot detection task, a prior needs to be acquired from some base data (e.g. meta train data in our case). To acquire this prior, we can follow two approaches: 1) Train a traditional model (e.g. a detector or classifier), and then fine tune on the novel few-shot task; OR 2) Acquire a prior via meta-learning on the base data, and learn a model that is trained to do few-shot learning. \n\nLSTD follows the first paradigm, while our proposed Meta-RCNN follows the second paradigm. Note that both methods have access to the exact same base data, i.e., they have access to the same information. They differ only in the learning algorithm. Then, a novel few-shot task is given to the algorithm, and the algorithm makes the prediction.\n\nSince both models have access to the same information, and make predictions on the same few-shot test task, the comparison is fair.\n\nData Split Difference\nMeta-learning literature (Vinyals et al. 2016, Finn et al. 2017, Snell et al. 2017, etc.) evaluates few-shot performance over multiple tasks drawn from a test task distribution, i.e., the few-shot performance is measured and averaged over multiple tasks. This is a more reliable metric than evaluating performance on only one few-shot task. LSTD data split considers evaluation on only one few-shot task in their data split. We train LSTD on appropriate base data, and then evaluate its performance over multiple tasks, and compare this performance with our method. \n",
                "reply_to": "BygTZ2FdYH"
            },
            {
                "review_id": "Bkeg5r3sir",
                "content": "Thanks for the comments! We do agree with some of your concerns, but do think that most of the suggested issues are addressable. \n\n1. Implementation details with high-res images\n\nThanks for the suggestions, and we apologise for lack details.\n\nDuring meta-training, we train the model using 5-way-1shot tasks, and only 5 query images (1 query image per class). This results in a total of 10 images for one task. With this, implementing the meta-training is not too difficult. Using this trained model, we evaluate performance on various settings (e.g. 5-way 1-shot, and 5-way 5-shot) meta test tasks. We apologize for the lack of clarity in the first RPN - this is not a critical component, and just a minor trick we use to make the prototype more robust (instead of constructing prototypes of the support objects by directly using ground truth bounding boxes and labels, we also use the proposals generated by the RPN, and if it has sufficient overlap with the ground truth, it is used for constructing the prototype). The main contribution in RPN is the one that is trained to generate support(class)-specific proposals.\n\nWe will definitely release the code.\n\n\n2 & 3. Regarding RepMet\n\nThanks for these comments regarding RepMet. \ni) We have improved the presentation to not call it RepMet, but to call it FRCN-PN(baseline), and have changed the written section describing the relation of FRCN-PN with RepMet.\nii) FRCN-PN shares a similar principle as RepMet (traditional detector training + replacing the object classifier with a meta-learner), and thus is a baseline we considered for our work.\niii) We would have liked to reproduce RepMet and compare directly with the original method, however, we were not able to find the code for it. As a result, we decided to implement the method based on this principle ourselves as a baseline. \niv) The code for RepMet: The arxiv version and the published version do not have a working link for the code being available online. We found a not well tested/incomplete version of the code  ( https://github.com/HaydenFaulkner/pytorch.repmet ) done by a third party, which has not yet reproduced the results. \n\n\n4. Comparison with Meta-RCNN in ICCV2019\n\nThanks for suggesting the reference. We believe this work was done in parallel with our work. We would like to highlight that this work was made available on arxiv (28th September) a few days after the ICLR submission deadline (25th September). Moreover, it appeared in ICCV even more recently (27th October).\n\nThis work does share some similarities as our work (principle of class-attentive module), however, there is a fundamental difference in the training approach, specifically for the RPN. In contrast to the reference paper, our RPN is meta-trained and is tailored to generate proposals for the few-shot setting. \n\nWe train the RPN in the meta-learning paradigm (meta-RPN), whereas the RPN training in the ICCV paper is trained using the traditional setting. This difference is extremely crucial for few-shot detection. Traditional RPNs will detect all objects in the image (including objects not of interest, i.e., they will even detect objects that are not available in given support set). Our meta-trained RPN generates proposals for an object from classes only belonging to the support set (i.e., it generates class-specific proposals). \n\nFinally, we would also like to highlight that following the meta-learning literature, we have evaluated the performance of the object detector on \u201cmultiple\u201d few-shot detection tasks. Our reported few-shot performance is average performance over these tasks, in contrast to the existing reference which evaluates result on exactly one few-shot task.\n\nAs regards empirical comparisons, it would be slightly time consuming to do this given different settings (e.g. hyperparameters, backbone, data splits, different approach for using the meta-train dataset, etc.). We do aim to do this in the future. \n",
                "reply_to": "HJeEHAb6tS"
            },
            {
                "review_id": "rygUTX2isH",
                "content": "Thank you for your review! We were delighted with your comments! \n\nAs regards novelty, we would like to highlight that it is not trivial to adapt meta-learning for object detection, and to the best of our knowledge, ours is the first work that trains both the object classifier and the RPN in a meta-learning paradigm, making all the components tailored for  few-shot detection.\n\nThanks for identifying the writing issues, we have fixed them in the current version. \n",
                "reply_to": "ryg6pPtQ5r"
            },
            {
                "review_id": "BygTZ2FdYH",
                "content": "In this paper, authors propose a meta-learning based approach for low-shot object detection. Specifically, they use prototype in the support set as attention guidance, and learn the category-specific representation for each query image. Subsequently, they use the style of Faster RCNN for object detection.\n\nIt is an OK paper with good structure. The idea is somewhat novel, in terms of meta-learning based low-shot detection framework. My main concern is about experiment. First, the data setting is branch new. Why not use the data setting in the literature, e.g., COCO to VOC in LSTD (Chen et al., 2018)? As a result, how to make a fair comparison bothers me a little. Furthermore, LSTD is a non-episodic approach. How to make it in a meta-learning way? Please clarify the implementation details for all other related works in the comparison.  ",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "HJeEHAb6tS",
                "content": "The paper proposes a method for few-shot object detection (FSOD), a variant of few-shot learning (FSL) where using a support set of few training images for novel categories (usually 1 or 5) not only the correct category labels are predicted on the query images, but also the object instances from the novel categories are localized and their bounding boxes are predicted. The method proposes a network architecture where the sliding window features that enter the RPN are first attenuated using support classes prototypes discovered using (a different?) RPN and found as matching to the few provided box annotations on the support images. The attenuation is by channel wise multiplication of the feature map and concatenation of the resulting feature maps (one per support class). After the RPN, ROI-pooling is applied on the concatenated feature map that is reduced using 1x1 convolution and original feature map (before attenuation) being added to the result. Following this a two FC layer classifier is fine-tuned on the support data to form the final \nRCNN head of the few-shot detector. The whole network is claimed to be meta-trained end to end following COCO or ImageNet (LOC? DET?) pre-training. The method is tested on a split of PASCAL VOC07 into two sets of 10 categories, one for meta-training and the other for meta-testing. In addition, experiments are carried out on ImageNet-LOC animals subset. In both cases, the result are compared to some baselines, and some prior work.\n\nAlthough FSOD is an important emerging problem, and advances on it are very important, I believe there are still certain gaps in the current paper that need to be fixed before it is accepted. Specifically:\n\n1. Some important details are missing from the description. For example, detectors are usually trained on high resolution images (e.g. 1000 x 1000) and hence are problematic to train with large batches, yet in the proposed approach it is claimed that the proposed model is meta-trained with batch size 5 on 5 way tasks with 10 queries each, so even in 1-shot case, does it mean that 5 x 15 = 75 high resolution images enter the GPU at each batch? I doubt that even in parallel mode with 5 GPUs and 15 high res image per GPU it is possible for claimed backbone architectures (ResNet-50 and VGG16).\nAs another example, the details of fine-tuning during meta-training seem to be left out, is the model optimized with an inner loop? Details of the RPN that is used to select the support categories prototypes are not specified, where it comes from and how is it trained (clearly as the \"main\" RPN relies on attenuated features, it cannot be it)? Some additional technical details are not very clear and hinder the reproducibility of the paper (no code seem to be promised?), in general I suggest the authors to improve the writing and clarity of the paper.\n\n2. In VOC07 experiment, FRCN-PN is very vaguely described and being claimed that it stands for RepMet (Karlinksy et al., CVPR 2019). It is not clear what it is and its training procedure on VOC07 is not clearly described.\nIt is also claimed in ImageNet experiment that the real RepMet is \"more carefully designed then FRCN-PN\" and has a better backbone, hence it is not clear why FRCN-PN should stand for it.\nI suggest the authors to either do a direct comparison or remove their claim of comparison.\n\n3. RepMet paper has proposed an additional benchmark on ImageNet-LOC with 5-way 1/5/10-shot episodes, and afaik it is reproducible as its code is released, so I am wondering as to why it was not used for \nevaluation given that the authors made the effort of reproducing another ImageNet-LOC test on the same categories? It should be evaluated for a fair comparison.\n\n4. Although they don't strictly have to compare to it, I am wondering if the authors would be willing to relate to a similar approach that was proposed for the upcoming ICCV 19: \n\"Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning\", by Yan et al. Their approach is more similar to RepMet in a sense that the meta-learning is done in the classifier head,\nand better results are reported on VOC07 benchmark (and except for 1-shot, higher results are reported for the 3 and 5 shot FRCNN fine-tuning).",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "ryg6pPtQ5r",
                "content": "This paper is about the task of object detection in the setting of few-shots dataset. The problem is addressed in the learning scheme of meta-learning paradigm: the proposed meta-rcnn trains the popular faster-rcnn on several tasks of few shots object detection while the RPN and the object classification networks are meta-learned among the tasks. Compared to previous work the paper introduces the meta learning framework and several changes to the faster rcnn detector. A prototype representation is derived from the standard RPN network and its proposed bounding box. An attention mechanism choose the object of interest and is used to train the final RPN and classification network. Experiments on the popular Pascal Voc 2007 and ImageNet-FSOD show that the proposed system have state of the art performance.\n\nThe paper is very well written, easy to read and of excellent presentation. The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting. The experiments are solid and show state of the art performance. As a result I recommend this paper to be accepted.\n\nMinor issues:\n- in caption of Fig1: avialable -> available\n- in 4.1: \u201cCompared to other variants...\u201d please add a reference to the specific methods you are comparing to.",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            }
        ],
        "generations": {
            "human_reference": "This paper develops a meta-learning approach for few-shot object detection. This paper is borderline and the reviewers are split. The problem is important, albeit somewhat specific to computer vision applications. The main concerns were that it was lacking a head-to-head comparison to RepMet and that it was missing important details (e.g. the image resolution was not clarified, nor was the paper updated to include the details). The authors suggested that the RepMet code was not available, but I was able to find the official code for RepMet via a simple Google search:\nhttps://github.com/jshtok/RepMet\nReviewers also brought up concerns about an ICCV 2019 paper, though this should be considered as concurrent work, as it was not publicly available at the time of submission.\nOverall, I think the paper is borderline. Given that many meta-learning papers compare on rather synthetic benchmarks, the study of a more realistic problem setting is refreshing. That said, it's unclear if the insights from this paper would transfer to other machine learning problem settings of interest to the ICLR community.\nWith all of this in mind, the paper is slightly below the bar for acceptance at ICLR.",
            "random_reference": "After discussions with the authors, the reviewers have converged to a positive view of this paper. Everyone agrees that the bound is a significant improvement and I do agree about the importance of cross-learning (esp for practical scenarios, like auctions). The reviewers have pointed out some writeup changes and I am satisfied that the authors can address them easily. Overall, the changes needed are within the scope of a final version revision and I recommend acceptance.",
            "modular_llama3": "This manuscript proposes a novel Meta-RCNN framework for few-shot object detection, achieving state-of-the-art performance by leveraging meta-learning. The framework is well-suited for few-shot detection tasks and demonstrates effectiveness in generalizing from small amounts of labelled data. However, the manuscript requires revisions to improve its experimental design, comparisons with existing works, and reproducibility. Specifically, the comparison with RepMet is unclear and lacks direct evaluation, while several technical details need to be clarified. Additionally, the manuscript would benefit from using established data settings, providing implementation details, and increasing reproducibility by providing code and clearer writing. Overall, with minor revisions to address these concerns, the manuscript has the potential to make a valuable contribution to the field of few-shot object detection.",
            "llama3_pr_chunk": "Meta-RCNN is a proposed meta-learning framework for few-shot object detection, which learns to perform detection with limited annotated data. It is built on Faster RCNN and meta-learns the Region Proposal Network and object classification branch. The model achieves promising results on the Pascal VOC dataset. The authors address comments and concerns, providing implementation details and clarifying differences with other methods, such as LSTD and RepMet. The paper is well-written and presents solid experiments, making it recommendable for acceptance with minor revisions.",
            "llama3_pr_consolidation": "This paper proposes a novel meta-learning framework for few-shot object detection, named Meta-RCNN, which demonstrates state-of-the-art performance on Pascal VOC and ImageNet-FSOD datasets. The approach is considered incremental in novelty but interesting, and the experiments are solid. However, some reviewers raise concerns about the clarity of the paper, particularly regarding implementation details and comparisons to existing works. The authors address these concerns in the discussion, providing clarifications and justifications for their approach. While some minor issues are noted, such as typos and missing references, the overall quality of the paper is considered excellent. The proposed meta-learning framework is deemed a significant contribution to the field, making the paper a strong candidate for acceptance.",
            "llama3_pr_knowledge": "**Reviews Summary:**\n\nThe proposed manuscript, \"Meta-RCNN: A Meta-learning Framework for Object Detection,\" has garnered mixed reviews from the scientific community. \n\n**Advancement and Novelty:** The majority of reviewers agree that the manuscript presents a novel approach to few-shot object detection using meta-learning, which is a significant contribution to the field. However, one reviewer questions the novelty of the approach, stating that it is an incremental but interesting contribution.\n\n**Clarity and Presentation:** The manuscript's clarity and presentation have received mixed reviews. One reviewer praises the manuscript's writing style, stating that it is well-written and easy to read. However, others have raised concerns about the lack of clarity in certain sections, such as the implementation details and the comparison with other methods.\n\n**Compliance and Soundness:** Reviewers have raised concerns about the manuscript's compliance with the venue's requirements and the soundness of the methodology. One reviewer questions the fairness of the comparison with other methods, while another reviewer raises concerns about the lack of implementation details and the reproducibility of the results.\n\n**Comparison with Other Methods:** Reviewers have raised concerns about the comparison with other methods, particularly RepMet and Meta R-CNN. One reviewer suggests that the authors should either do a direct comparison or remove their claim of comparison, while another reviewer raises concerns about the lack of implementation details and the reproducibility of the results.\n\n**Minor Issues:** Reviewers have raised several minor issues, including typos, grammatical errors, and the need for additional references.\n\n**Recommendation:** One reviewer recommends the manuscript for acceptance, while others suggest that the authors should address the raised concerns before the manuscript can be accepted.",
            "llama3_pr_decomposed": "Based on the reviewers' comments, the overall assessment of the paper is mixed. \n\nIn terms of writing style, the reviewers found the paper to be well-structured and clear in its presentation of ideas, but some sections were considered too technical and difficult to follow. One reviewer suggested that the authors could improve the paper's readability by providing more explanations and examples to support complex concepts.\n\nThe presentation of the paper was generally considered to be good, with the reviewers praising the authors' use of tables, figures, and headings to organize the content. However, some reviewers noted that the paper could benefit from more visual aids, such as diagrams or flowcharts, to help illustrate key points.\n\nRegarding the potential for acceptance, the reviewers were divided in their opinions. Some reviewers believed that the paper made a significant contribution to the field and was worthy of publication, while others raised concerns about the paper's methodology and the validity of its findings. One reviewer suggested that the authors could strengthen their argument by providing more empirical evidence to support their claims.\n\nOverall, the reviewers' feedback suggests that the paper has some strengths, but also needs some revisions to address the concerns raised. With some revisions to improve the writing style, presentation, and methodology, the paper may be considered for publication.",
            "modular_gpt4": "The manuscript proposes the Meta-RCNN framework, a novel meta-learning approach for few-shot object detection. By meta-learning both the Region Proposal Network (RPN) and object classification branches, the approach demonstrates improved class-specific proposal generation and performance on benchmark datasets such as PASCAL VOC07 and ImageNet-LOC. Despite its strengths and incremental advancements over existing methods, reviewers highlighted the need for more detailed implementation specifics, clarity on training procedures, and addressing reproducibility concerns. While the framework's promising results and innovative loss objectives make it a significant contribution, the paper should include a more thorough discussion of its limitations. Overall, the manuscript is well-written and structurally sound, and it is recommended for acceptance with minor revisions to address these identified gaps."
        }
    },
    "index_7": {
        "paper_id": "iclr_2020_BJx7N1SKvB",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features F=f(WX+B) for a random weight matrix W and random bias vector B, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.",
                "reply_to": "iclr_2020_BJx7N1SKvB"
            },
            {
                "review_id": "HyxOtaQviB",
                "content": "We are grateful to all reviewers for their constructive feedback and for the time they took to review our work. We have uploaded a new version of the paper and added detailed responses to their comments below. In particular, we address some technical questions about our paper, and explain the overall merits of our work. We hope this encourages reviewers 2 and 3 to reconsider their scores.",
                "reply_to": "iclr_2020_BJx7N1SKvB"
            },
            {
                "review_id": "rkxjQeEwiH",
                "content": "2. As we outline in Subsection 1.1, our work generalizes substantially over previous work as we analyze nontrivial data distributions and NNs with biases. In particular, previous formulae for the spectrum would agree poorly on real datasets like MNIST and CIFAR, whereas we get good agreement as evidenced in Fig. 2. Moreover, to achieve these generalizations, we had to introduce a new method of proof rather than the moment method employed in Pennington & Worah (2017). \n\n3. While Fig. 4 is indeed important and illustrates a main conclusion of our paper about mixtures of nonlinearities, the other figures are also significant as they demonstrate the validity of the mathematical machinery we use to predict the spectrum on complex datasets. To help provide context on these other figures, we have reworded their captions and made sure that each figure is referenced in the main text.\n\n4, 5, and 6. We have rewritten these sentence to clarify our meaning and fix any grammatical errors.",
                "reply_to": "SJgdT5JY5S"
            },
            {
                "review_id": "Byg3S1NPjH",
                "content": "1. The formula for E_train in Eq. (17) is exact, since the training error is a random variable that converges in probability to a deterministic quantity. We have updated the statement of Cor. 1 to clarify this.\n\n2. The only assumption on f is that it leads to finite moments up to third order under Gaussian expectation. There is also some discussion of the differentiability of f in Remark 4: while we assume f is differentiable for convenience, it is in fact not a necessary assumption as the derivative is only used in a Gaussian expectation, which inherently smooths the function. This implies in particular that ReLU activations are included in the behavior we identify. See Figs. 1 and 2, for example, where we find excellent agreement when f is ReLU.\n\n3. The object G(\\gamma) is indeed the same as G defined in Eq. (1); the only difference is that the function is evaluated at z=-\\gamma.",
                "reply_to": "rklsQVQCtS"
            },
            {
                "review_id": "BylmyJNwjB",
                "content": "1. This is a natural question and one we want to respond to in detail. Augmenting X as [X, 1] does not allow us to address both models in a unified manner. In fact, if it did, our theorem would be a special case of earlier work -- in particular, the final spectral distribution would depend on only the two parameters \\eta and \\zeta. However, the class of spectral distributions we find is nonparametric and thus cannot be expressed in this way.\n\nThe issue is that the previous work only applies if the augmented coordinate [--, 1] is transformed by weights of the same order of magnitude as the other features in X, i.e. the bias is of order O(1/\\sqrt{n}) per output feature -- an effect that disappears in the large m limit. Instead, our formulation f(WX+b) allows for an O(1) bias term per output feature. One can certainly reformulate this algebraically as f( [W, b] . [X, 1] ), but the scaling assumptions of prior work are (significantly) broken since W and b are on different scales. Addressing these issues is a main contribution of our derivation.\n\nWe have included a brief discussion of this point in the new version of the paper.\n\n2. In our setting the training error is a way to quantify the capacity of the function class. Indeed the test error is also interesting, but analyzing it requires specifying a model for the joint distribution between the data points and labels, and substantially more analysis. Unfortunately, this analysis is outside of the scope of this paper.",
                "reply_to": "rklsQVQCtS"
            },
            {
                "review_id": "rylKZCXDiS",
                "content": "(1) We focused on the linear model because it is the simplest (nontrivial) learning task. Moreover, the analysis follows directly from the paper\u2019s main theorem, whereas more complex learning tasks require substantially more calculation\u2014generally beyond the scope of this initial work.\n\n(2) We are happy to include experiments on additional datasets in a final version of the paper. ",
                "reply_to": "Bkl2KwYTtH"
            },
            {
                "review_id": "Bkl2KwYTtH",
                "content": "In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. I think it is a solid work and vote for acceptance. \nPros: \n(1) This paper has a solid theoretical foundation. Although I have not checked in detail, I think the deduction is clear and the contribution is well-established.\n(2) It extends some traditional bounds to more general cases. I think it will provide useful guidance to real applications, such as the network design in deep learning.\n(3) The authors have explained the results in a clear way. Thus, it will benefit the following readers and give deep insights about the related research areas.\nMinor comments:\n(1) I think some assumptions should be explained. For example, why the authors focus only on linear model. Due to the simplicity or the requirement from real applications?\n(2) More experimental results on large data sets should be added to validate the effectiveness.",
                "reply_to": "iclr_2020_BJx7N1SKvB"
            },
            {
                "review_id": "rklsQVQCtS",
                "content": "This paper investigates the asymptotic spectral density of a random feature model F(Wx + B).  This is an extension of existing result that analyzed a model without the bias term, i.e., F(WX). This extension requires a modification of the proof technique. In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators.\n\nPros:\n- This paper investigates an interesting problem and it successfully extends the existing work. The theoretical curve well matches the simulated curve.\n- The finding that mixture of nonlinearities gives better expected training error is interesting.\n\nCons:\n- The extension to the model with bias seems a bit incremental. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis.\n- The effect of mixture of activation functions is investigated in the \"training error,\" but I don't see much significance on investigating the training error thoroughly. Instead, people are interested in the test error. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. Hence, more expositions about why the training error is investigated should be provided.\n\nMore minor comment:\n- I guess the definition of Etrain  (Eq.(17)) requires an expectation with respect to the training data.\n- Assumptions of the activation function f should be provided; is it just assumed to be differentiable?, ReLU is included?\n- The definition of G(\\gamma) in page 6 had better to be consistent to that in previous pages.\n\n",
                "reply_to": "iclr_2020_BJx7N1SKvB"
            },
            {
                "review_id": "SJgdT5JY5S",
                "content": "This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks.\n\nComments:\n1.The paper is well written and provides sound derivation for the theories.\n\n2. Since this area is out of my expertise, I\u2019m not sure whether merely extending the work of Pennington & Worah (2017) to non-Gaussian data distributions is significant enough or not.\n\n3. Except for Fig 4, the other figures seem out of the context. There is no explanation for the purpose of those figures in the main contents. It is a bit hard for the audience to figure out what to look at in the figures or what the figures try to prove. \n\n4. In \u201c..., and our analysis actually extends to general such distributions, ... \u201d, \u201cgeneral\u201d should be \u201cgeneralize\u201d.\n\n5. In \u201cAnd whether these products generate a medical diagnosis or a navigation decision or some other important output, ..\u201d, \u201cwhether\u201d should be \u201cno matter\u201d.\n\n6. \u201c..., they may not be large in comparison to the number of constraints they are designed asked satisfy.\u201d should be \u201c...  they are designed to satisfy\u201d.\n",
                "reply_to": "iclr_2020_BJx7N1SKvB"
            }
        ],
        "generations": {
            "human_reference": "In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.\n\nUnfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold.",
            "random_reference": "This paper received mixed reviews from the reviewers. Its strength is that it seems successful in proposing a budget constrained offline reinforcement learning and evaluation framework for the coupon allocation problem on e-commerce platforms. The drawbacks of the paper are the limited novelty of the approach (it appears to combine/unify several prior methods) and the specificity of the application. Given that there is no evidence of wider applicability of the proposed method, I recommend rejecting this paper for NeurIPS. However, the authors are encouraged to submit the manuscript to a more applied venue.",
            "modular_llama3": "This manuscript presents a significant generalization of previous work on the asymptotic spectral density of random feature models by extending the analysis to non-trivial data distributions and neural networks with biases. The paper provides a solid theoretical foundation and clear deductions for analyzing the asymptotic training error of neural networks. Reviewers agree that the paper is well-written with sound derivations, offers useful insights for deep learning applications, and makes a solid contribution to the field by investigating an interesting problem. However, minor revisions are suggested to clarify assumptions, provide consistent definitions, and better explain the purpose of some figures. One reviewer questions the significance of the work due to its similarity to previous research, but overall, the manuscript is considered worthy of acceptance with minor revisions.",
            "llama3_pr_chunk": "Researchers analyzed a high-dimensional regression model, deriving a formula for asymptotic training error in a noisy autoencoding task and suggesting mixtures of nonlinearities may outperform single nonlinearities. The authors addressed reviewer comments, clarifying the formula's exactness and assumptions. Reviewers praised the paper's solid theoretical foundation and clear deductions but suggested explaining assumptions and providing more experimental results. The work extends existing research by analyzing the asymptotic spectral density of a random feature model with a bias term and mixture of linear and nonlinear activation functions, finding improved expected training error for ridge regression estimators.",
            "llama3_pr_consolidation": "This manuscript presents a theoretical analysis of the asymptotic training error of a simple regression model trained on random features for a noisy autoencoding task. The authors extend previous work by incorporating a bias term and non-Gaussian data distributions, and provide a solid theoretical foundation for their results.\n\nThe strengths of the manuscript include its clear and well-established contribution, which provides useful guidance for real applications such as network design in deep learning. The authors have also explained their results in a clear and concise manner, making it easy for readers to understand and gain insights into the related research areas.\n\nHowever, some reviewers have raised concerns about the incremental nature of the extension to the model with bias, and the lack of discussion on why a trivial argument cannot be applied in the analysis. Additionally, some reviewers have questioned the significance of investigating the training error, as opposed to the test error, and have suggested that more experimental results on large datasets should be added to validate the effectiveness of the model.\n\nOverall, the manuscript has received positive reviews, with most reviewers agreeing that it is a solid work that extends previous results and provides useful insights into the behavior of neural networks. However, some minor revisions are suggested to address the concerns raised by the reviewers.",
            "llama3_pr_knowledge": "**Summary of Reviews on Scientific Article**\n\nThe reviewers generally praise the paper for its solid theoretical foundation, clear explanation of results, and contribution to the field. They appreciate the extension of traditional bounds to more general cases and the analysis of a mixture of linear and non-linear activation functions. However, some reviewers raise concerns about the significance of the extension to the model with bias, the focus on training error instead of test error, and the lack of explanation for some assumptions and figures.\n\nThe reviewers suggest that the authors should provide more discussion on why the extension to the model with bias is necessary, why the training error is investigated instead of the test error, and why the linear model is focused on. They also recommend adding more experimental results on large datasets to validate the effectiveness of the approach.\n\nThe authors respond to the reviewers' comments by explaining the necessity of the extension to the model with bias, the importance of the training error in quantifying the capacity of the function class, and the simplicity of the linear model. They also clarify some assumptions and provide more explanations for the figures.\n\nOverall, the reviewers consider the paper to be a solid work with a well-established contribution, and they vote for acceptance with some minor revisions.",
            "llama3_pr_decomposed": "The reviewers recommended that the paper be strengthened with supplementary experimental results to validate the proposed approach, and clearer explanations of the underlying assumptions to enhance clarity and readability.",
            "modular_gpt4": "The manuscript provides a significant contribution to the understanding of neural network architectures in high-dimensional data settings. By exploring mixtures of nonlinearities versus single nonlinearities, the authors extend previous research with new proof methods and analyze nontrivial data distributions, yielding insights relevant to real datasets and neural network design. The theoretical foundation is robust, supported by exact asymptotic training error formulae and experimental validation, although further analysis of test error and improved clarity in figures are recommended. Revisions based on feedback have improved the manuscript's clarity and grammatical correctness. While minor suggestions such as discussing assumptions of activation functions and ensuring consistent notation remain, the work\u2019s technical soundness and clear presentation warrant its acceptance. Reviewers collectively appreciate the paper's methodological rigor, theoretical extensions, and practical implications for deep learning."
        }
    },
    "index_26": {
        "paper_id": "iclr_2023_4orJ47he7WV",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning. To this end, We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness. In Lux, we build our agents through the standard reinforcement learning algorithm in curriculum learning phases and leverage centralized control via a pixel-to-pixel policy network. As agents co-evolve through self-play, we observe several stages of intelligence, from the acquisition of atomic skills to the development of group strategies. Since these learned group strategies arise from individual decisions without an explicit coordination mechanism, we claim that artificial collective intelligence emerges from massive-agent cooperation and competition. We further analyze the emergence of various learned strategies through metrics and ablation studies, aiming to provide insights for reinforcement learning implementations in massive-agent environments.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "P2shM3giQ6m",
                "content": "The paper seems to be deanonymized. \"To this end, we propose Lux (Doerschuk-Tiberi & Tao, 2021)\" which links to this github repo: \"https://github.com/Lux-AI-Challenge/Lux-Design-2021\". If this is not the case, I will provide a full review.   TBD TBD TBD",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "xXqhXQdw4I",
                "content": "This paper introduces a massively multi-agent (thousands of agents) learning environment, Lux, in which teams compete in a custom Real-Time Strategy (RTS) grid world with limited resources. There are two types of agents on each team, Workers and CityTiles, with distinct actions available. The main contributions of the paper, as indicated by the authors, are (1) the introduction of the Lux learning environment; (2) evidence that there are emergent collective behaviors including local strategies such as regional coordination and global strategies such as sustainable development; (3) the implementation details of a particular algorithm using curriculum training that appears to have a very high success rate against the winner of a competition in developing agents for this environment.  Strengths:\n- This paper demonstrates multi-agent reinforcement learning in a complex domain with limited resources.\n- The paper is written clearly and the appendix has many details aiding reproducibility.\n- The Lux environment is very compelling and could definitely be a source of important new results in multi-agent reinforcement learning, especially in the context of limited resources.\n\n\nWeaknesses:\n- One of the biggest weaknesses is the nature of the first contribution (introducing the environment itself). This paper mentions introducing this environment, but then cites the Doerschuk-Tiberi and Tao (2021) GitHub page and associated competition. If this paper is coming from the authors of the repo, then this is clearly breaking anonymity. Otherwise, it is not clear why the authors are stating this as a contribution.\n- The paper does not adequately explain why the algorithm that is used here has an \"up to 90% win rate\" against the Isaiah et al. (2021) policy; it appears that this is entirely due to the curriculum training, as mentioned in the caption to Figure 10; otherwise the win rate is only 20%. If that is the case, is the contribution about the effectiveness of curriculum training? If so, how would curriculum training benefit another algorithm in this environment, such as the Isaiah et al. (2021) policy?\n- While using a central policy for all of the agents in the solution makes sense from a practical perspective, it primarily makes the action space enormous but removes the individual independence of agents. Would it be possible to train individual agents in this framework, and what would this look like?\n- The particular methodology described in Section 4 for a particular neural architecture does not make it clear how difficult it would be to train different neural architectures or to use different algorithms with this framework.\n- The accompanying video is not sufficiently self-contained; some text in the appendix explaining what the video demonstrates would be useful. As mentioned in the weaknesses above, it is very important that the authors clarify their role relative to the existing competition. If this is not a double-blind submission, and the nature of the competition that has already occurred would make anonymity impossible, it may be better suited to a single-blind venue such as a Datasets and Benchmarks track (regularly at NeurIPS) or similar. By itself, the algorithmic contributions are not clear and limited to the design of the curriculum training approach, which appears to be very successful. Is the main result that curriculum training is very important for massively multi-agent settings? If so, then this should be emphasized, along with a study of how this approach is useful when paired with other algorithms/approaches. Or is the architecture that is proposed here better-suited for curriculum training specifically? What are the limitations of the centralized policy adopted in the proposed solution?\n\nMinor point: there is a missing reference in line 4 of the Introduction. This paper needs to clarify what the contributions are (environment vs. curriculum training). If it is the environment, then this should be either submitted to a different venue or anonymized properly before submission. Also, there are a number of questions about what it would take to train agents in this environment, presumably already described in the competition rules referenced in the Appendix. If it is the curriculum training approach, then there needs to be further study of the effect that this has relative to \"the base algorithm\" it is used with.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "-3eKRfJ1Of",
                "content": "The paper proposes a new benchmark to train a large number of agents in an environment with limited available resources. It also explores the connection of learning atomic / low-level skills with that of learning social strategies such as cooperation, coordination, and competition.  The proposed environment captures the essential elements of learning diverse social behaviors required to sustain in a society with limited resources.\n\nOne concern I have with the work is the lack of comparison to prior frameworks. I'm skeptical as to what additional features Lux provides that are not already there in previous massive-agent frameworks like MAgent. The comparisons done in the paper do not highlight the main advantage of using Lux over previous frameworks. The paper is clearly written and is easy to read. The proposed environment is a novel contribution and there are a number of ablation studies performed to study the various strategies that are evolved throughout the learning. The paper proposes a new massive-agent RL framework that could act as a benchmark for but testing agents' ability to cooperate, coordinate or learn different kinds of social behaviors.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "UvEvnY14xpQ",
                "content": "In this paper, the authors present Lux, an environment for multi-agent reinforcement learning at scale. The environment is evaluated through simulations using a curriculum learning solution. The main contribution of this work resides in the environment itself, which allows for a simulation of multi-agent interactions at scale. However, the dynamics of the game itself does not appear particularly insightful. The implementation of the game itself is also rather standard.  General Note\n\nIt appears that the paper is not anonymized since the authors say \"To this end, we propose Lux (Doerschuk-Tiberi & Tao, 2021), a cooperative and competitive environment where hundreds of agents in two populations scramble for limited resources and fight off the darkness\". It appears that the authors of the work are linked to Doerschuk-Tiberi & Tao.\n\nStrengths\n\n- The release of a platform for the research community is commendable.\n- The emerging patterns are interesting, but somehow expected since they are the direct consequence of the reward structure of the game.\n\nWeaknesses\n\n- The paper is not self-contained. Key parts of the paper are in the Appendix. It is not possible to understand the dynamics of the game without checking the Appendix. Several key experimental results are in the Appendix.\n- It is difficult to see a clear scientific contribution in this work. The reviewer understands that the authors are making available a new environment for the community, but \n- Most of the description of the technical details of the implementation of Lux and its evaluation are in the Appendix. It is worth noting that the some of the notation itself cannot be found in the main paper.\n- The actual contribution of this work is unclear, since it mixes the presentation of the platform and the evaluation with a specific curriculum learning algorithm.\n- The curriculum learning solution used by the authors is rather standard. The results are not particularly insightful.\n \n\nThe paper is not well structured. Key parts are in the Appendix, including the definition of the notation used in the paper.\n\nThe reviewer believes that the novelty of this work is rather limited. The implementation of the environment itself is rather standard. \n\nThe emerging patterns in terms of strategy are interested, but they appear as a direct consequence of the reward structure defined by the authors. The reviewer believes that the contribution itself is not sufficient for a conference of the level of ICLR.\n\nThe study using curriculum learning is also rather standard for this type of environments. There are limited novel insights that are emerging in this analysis.\n\nThe authors should carefully proofread the paper. There are missing citations and wrong capitalization in the text. The presentation of the evaluation results in terms of description of the experimental settings can also be improved. The discussion of the experimental results is rather concise.\n \nThe authors present an interesting environment, which is made available to the research community as open source software. However, the environment itself is not particularly innovative compared to the state of the art. The evaluation of the platform considering a curriculum algorithm solution is rather standard. The paper is not well structured and impossible to understand without reading the Appendix. Key sections are not in the main paper. \n\nThe reviewer also would like to stress the fact that the paper does not appear to be fully anonymized. There is a direct citation of the GitHub page of the project with the names of the authors of the manuscript.\n\n",
                "reply_to": "iclr_2023_4orJ47he7WV"
            }
        ],
        "generations": {
            "human_reference": "I thank the authors for their submission and active engagement during the discussion period. The reviewers unanimously agree that this paper is not ready for publication. In particular, the reviewers remarked missing clarity around the contributions and a lack of comparison to similar frameworks. Therefore I recommend rejection. ",
            "random_reference": "This paper performs a detailed analysis of a single attention head (L10H7) in GPT2-small that has the effect of suppressing copying behavior. The authors are able to identify how L10H7 is able to achieve this behavior via a weights-based explanation, which is impressive in its level of detail. Reviewer criticism essentially boils down to two main points: (1) how transferable are these findings to more modern models, and (2) what actionable insights do we learn from this analysis? Regarding (1), while the authors do add a brief section on LLaMA models in Appendix A, the writing is too vague to tell if their copy suppression heads behave similarly to L10H7. Regarding (2), I am not as negative on this kind of interpretability research as Reviewer ccpw, as I think it is generally interesting to discover and understand low-level mechanisms in deep networks. However, I do agree that in this specific paper, the analysis is too specific to GPT-2, and the connections to self-repair abilities are not obvious enough to sufficiently excite me about the results.",
            "modular_llama3": "This manuscript proposes a novel massive-agent reinforcement learning environment, Lux, which captures essential elements of learning diverse social behaviors. While the reviewers appreciate the clear writing, reproducibility, and the novelty of the environment, they raise several concerns. The paper's main contribution is unclear, with some attributing innovation to the curriculum training approach, and others finding the algorithmic contributions limited. The work's implementation and novelty are considered rather limited compared to the state of the art, and the paper's structure and self-containment are lacking, with key information relegated to the Appendix. Additionally, the reviewers question the significance of the contributions for a top-tier conference like ICLR, and the paper's non-anonymity raises concerns about the authors' role relative to existing work and the need for fair comparisons with previous frameworks.",
            "llama3_pr_chunk": "This paper proposes Lux, a massive-agent reinforcement learning environment to study collective intelligence through cooperation and competition. The results demonstrate the emergence of collective intelligence without explicit coordination mechanisms. However, reviewers raise concerns about the paper's anonymity, unclear contributions, lack of comparison to prior frameworks, and limited scientific contribution, structure, and impact.",
            "llama3_pr_consolidation": "This manuscript proposes a new massive-agent reinforcement learning environment called Lux, where agents in two teams compete for limited resources. The authors claim that artificial collective intelligence emerges from massive-agent cooperation and competition, and they analyze the emergence of various learned strategies through metrics and ablation studies.\n\nThe reviewers appreciate the introduction of the Lux environment and its potential as a benchmark for multi-agent reinforcement learning. They also acknowledge the clarity of the paper and the appendix, which provides many details aiding reproducibility. However, several reviewers express concerns about the lack of novelty and innovation in the environment itself, comparing it to existing frameworks like MAgent. They also question the scientific contribution of the work, as the implementation of the environment and the curriculum learning solution used are considered standard.\n\nThe reviewers also point out several weaknesses, including the lack of anonymization, which makes it clear that the authors are linked to the existing competition. They also criticize the paper's structure, with key parts and notation definitions in the appendix, making it difficult to understand the dynamics of the game without additional references. The results of the curriculum learning solution are not considered particularly insightful, and the discussion of the experimental results is too concise.\n\nOverall, the reviewers appreciate the potential of the Lux environment but are concerned about the lack of novelty, innovation, and scientific contribution. They suggest that the authors clarify their role relative to the existing competition, improve the paper's structure, and provide more insightful results and discussions.",
            "llama3_pr_knowledge": "**Summary of Review Aspects for the Scientific Article**\n\n**Advancement:** The manuscript presents a new massive-agent reinforcement learning environment, Lux, which has potential to advance the field, but its significance and impact are not yet fully clear. The environment's novelty is limited compared to existing frameworks like MAgent.\n\n**Clarity:** The paper is generally well-written and easy to read, but it lacks self-containment, with key parts and notation definitions relegated to the Appendix. Some minor errors, such as missing citations and incorrect capitalization, were noted.\n\n**Compliance:** The manuscript does not meet anonymity requirements, as it directly cites the authors' GitHub page and associated competition. The submission may be better suited to a single-blind venue.\n\n**Soundness:** The paper's empirical soundness is limited by its lack of comparison to prior frameworks and its reliance on a standard curriculum learning solution. The theoretical soundness is also questionable, as the dynamics of the game and implementation are not particularly insightful.\n\n**Novelty:** The paper's novelty is limited, as the environment and implementation are not particularly innovative compared to existing frameworks. The emerging patterns in terms of strategy are interesting but appear to be a direct consequence of the reward structure defined by the authors.",
            "llama3_pr_decomposed": "The reviewers expressed significant skepticism about the paper's contributions and novelty, questioning the originality of the Lux environment implementation and the curriculum learning solution. They found these aspects to be relatively standard, lacking significant innovations or improvements over existing approaches, thus raising concerns about the paper's overall contributions and novelty.",
            "modular_gpt4": "The manuscript introduces Lux, a new massive-agent reinforcement learning environment designed to study emergent collective intelligence through cooperative and competitive interactions. Key contributions include the Lux environment, emergent behaviors, and successful curriculum training strategies, all provided as open-source. Reviewers appreciate the clear writing, implementation details, and various ablation studies. However, concerns are raised regarding structural issues, including key information being relegated to the Appendix, missing citations, and inappropriate capitalization. The novelty of Lux compared to existing benchmarks is questioned, with reviewers noting inadequate comparisons to prior frameworks and a lack of clear algorithmic contributions. Additionally, anonymity concerns arise due to direct citations and naming authors. Overall, while Lux offers a compelling environment for agent-based research, its contribution may not suffice for high-tier conferences due to the aforementioned issues."
        }
    },
    "index_33": {
        "paper_id": "iclr_2022_Kvbr8NicKq",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "The AutoAttack (AA) has been the most reliable method to evaluate adversarial robustness when considerable computational resources are available. However, the high computational cost (e.g., 100 times more than that of the project gradient descent attack) makes AA infeasible for practitioners with limited computational resources, and also hinders applications of AA in the adversarial training (AT). In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness. Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments. The reliability of our method lies in that we evaluate the quality of adversarial examples using the margin between two targets that can precisely identify the most adversarial example. The computational efficiency of our method lies in an effective Sequential TArget Ranking Selection (STARS) method, ensuring that the cost of the MM attack is independent of the number of classes. The MM attack opens a new way for evaluating adversarial robustness and contributes a feasible and reliable method to generate high-quality adversarial examples in AT.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "JjjvbSLcC1j",
                "content": "The authors propose minimum-margin (MM) attack to provide comparable performance with AutoAttack while significantly decreasing the computational cost. They propose Sequential TArget Ranking Selection (STARS) to make the computational cost independent of the number of classes.  Strengths:\n\n-- The paper is well-written and the preliminaries are described clearly. \n\n-- The proposed method presents significantly low computational complexity. \n\nWeaknesses:\n\n-- The proposed method is only compared against PGD and CW. \n\n-- The authors have mentioned that: \"For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.\". Can you please explain about \"most adversarial example\" ?  \n\n-- $\\beta$ in Equation 9 is not defined. Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "f_xT_PzdYef",
                "content": "The paper proposed a strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. Compared with the SOTA attack, the proposed attack is much faster and thus easier to be applied in practice. The idea is novel and the results are solid.\n Major contributions:\n\nThe main idea has been illustrated in Figure 2. Traditional PGD attack minimizes the probability of the true label (by maximizing the loss), and the proposed minimum margin attack minimizes the margin of the probabilities between the true label and the most confusing label. To the best of my knowledge, the idea is novel in adversarial machine learning.\n\nThe computational efficiency of the MM attack is amazing. Nowadays, researchers are still using mainly PGD for training but AA for evaluation because AA is more than 100 times slower than PGD. The MM attack is 20 or even 30 times faster than the AA attack, making it possible for training with stronger adversarial examples besides faster evaluation of the adversarial robustness of given deep learning models. In my opinion, the results are significant.\n\nConcerns:\n\nThe paper lacks some theoretical analysis, for example, how would the attack converge, how to guarantee the minimum probability margin example is stronger than the minimum probability example (and thus more informative for both evaluating and training), and whether the iterative MM attack algorithm is as stable as the PGD and AA algorithms.\n\nThe experiments mainly focused on the evaluation part and then the training part is quite weak. Although researchers believe stronger adversarial examples lead to more robust models, it is not always the case because adversarial examples can be generated by quite different underlying principles as minimum probability vs. minimum probability margin. It is better to concretely show that MM is almost as fast as PGD and almost as strong as AA for training besides for evaluation. This is quite critical for the significance of the paper.  This is an overall well-executed paper, with good novelty and solid experiments. Some points should be clarified and stregnthened in the revision. ",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "RxbXZXcSIFI",
                "content": "This paper proposes a minimum-margin (MM) attack to evaluate defenses. The authors report detailed results on the effects of different loss functions. Experiments are done on CIFAR-10/100 and SVHN, against the adversarially trained models. Strengths:\n- This paper is well-written, especially with detailed descriptions and empirical results on the effects of different attacking loss functions.\n- The improvements shown in Figure 1 seems promising with significant saving on computation.\n\nWeaknesses:\n- Although several attacking baselines are considered, they are all only evaluated against PGD-AT (Madry et al. 2018). This could cause a biased evaluation of the attacking performance. Namely, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper (Croce & Hein, 2020). This should not be computationally hard, considering the efficiency of MM and many existing defenses (and their checkpoints) provided in, e.g., RobustBench.\n- The multi-target attacking strategy has already been proposed in [a], but I'm surprised that [a] is not even cited in this paper. For me, the proposed STAR strategy is just a top-K variant for the original multi-target attack. Besides, using logits rather than softmax outputs is also not a new discovery since Carlini & Wagner (2017b). Thus, the technical contribution and novelty of MM are quite limited.  \n\nMinors:\n- What is the definition of $l\\_{MM}$ in Algorithm 1?\n\nReferences:\n[a] Gowal et al. An alternative surrogate loss for pgd-based adversarial testing, 2019. Limited technical contribution, lack of evaluations against more defenses.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "ayTXEckd31H",
                "content": "The paper proposes an attack for testing adversarial robustness that is reportedly faster than the state-of-the-art attacks but still produces reliable results. The advantage in speed is obtained by using a sequential target ranking selection method, while reliability is achieved by using a minimum-margin loss.\n ### Comments:\n\n- The threat model is not stated anywhere. There is no definition of adversarial robustness/robust accuracy. It is not clear then if (1) the attack is a minimum-distance or maximum-confidence attack, (2) it is a targeted or untargeted attack in the common sense used in the field (as opposed to the \"targeted\" version of APGD, that is instead just using the \"targets\" for reducing the number of adversarial classes to consider in the optimization), and (3) if the attack is only defined in the $\\ell_\\infty$ norm.\n\n\n### Flaws in the experimental evaluation\n\n- the evaluation does not consider a state-of-the-art attack such as [brendel2020]. This is a pity, as [brendel2020] is presented as a \"fast and reliable method\" for evaluating robustness, and has similar desired characteristics as this attack.\n- The parameters of the attacks used seem sub-optimal. There is no choice of the hyperparameters, and using 10 steps for PGD seems to be limiting the capabilities of the attack. The same can be true for the CW attack, especially as there is no mention on how many binary-search steps are being used. It is OK to test the attacks with limited resources, but a more detailed asymptotic analysis (e.g. with 1k steps) would concretely support the claims of the paper that the attack remains comparable to the other attacks while reducing computational time.\n- The runtimes are computed in an uneven scenario. The total time per-step, or better, per-query to the model should be used instead of the total cumulative time. This makes no sense. As an alternative, one should compare the capabilities of the PGD attack (depicted here as fast but not reliable) with a fixed computational time, i.e., by increasing the number of steps performed by PGD until it spends the same amount of time as the MM attack.\n- the authors did not state if they used some available implementations of the attack, or implemented their own versions. Since the computational time depends on the implementation, this might be a problem when using the runtime as a benchmark.\n\n### Incorrect statements and unsupported claims\n\n**Abstract**\n\n- there is no definition of the \"most adversarial example\", even though there are several references of this in the paper. This is also used in the abstract. Depending on the objective, a stong adversarial example can be seen in different ways. I suggest to expand this with a definition.\n- there is no evidence suggesting that the PGD attack is 100 times slower than AA. The comparison is performed in uneven scenario, where AA uses 100 iterations while PGD uses 10. Moreover, this is stated in the abstract, which makes the statement easy to take and quote, without knowing the context. This statement should be removed.\n\n\n**Introduction**\n\n- \"for practitioners who need real-time evaluation at each epoch of the training process of a robust model\". Is there real cases that require this kind of evaluation? This is missing a reference.\n- \"Unfortunately, PGD fails to reliably evaluate adversarial robustness of a robust DNN\". This sentence is over-generalistic and not true for the majority of the cases. PGD was succesfully used against many defenses, just by making it adaptive to the defense [tramer2020].\n- \"CE loss, which is based on the probability of the true label $p_y$, is not an appropriate measure to the quality of adversarial examples\". There is no definition in the paper for \"quality of adversarial examples\", which makes this statement very confusing.\n- \"Hence, the reliable method is to minimize $z_y - z_t$ for each $t \\neq y$ and take the most adversarial one, which is a widely used solution\". This is not widely-used, as for now it seems only used in [croce2020].\n\n**Preliminary**\n\n- \"$x^{(0)}$ refers to the starting point which corresponds to the natural example (or the natural example perturbed by a small Gaussian or uniformly random noise)\". The statement within parentheses makes the definition of the closed ball in eq. 2 makes the ball centered in $x^{(0)}$. This does not correspond to the adversarial robustness measured in the original clean sample.\n- many equations (see Eqs. 3-7) depend on f, x, y, but they often don't appear inside the equations.\n- \"They showed that using adaptive step size significantly improves the adversarial robustness\". Should be \"improves the adversarial examples\" or \"improves the adversarial evaluation\". The attacks are not improving robustness.\n\n**Realization**\n\n- Eqs. 8 and 9 use variables ($\\alpha$, $\\beta$) never introduced in the text.\n\n\n### Minor issues\n\n- the comparison with targeted-dlr loss in sect. 3 should be clarified. It is very difficult to read, and it does not really capture the advantage of using different methods for rescaling. This might be better supported by some evidence or toy example, and surely by adding some insight on which the hypothesis is based on. Moreover, the authors should then explain what is the difference from the CW loss, as it seems that they are using that one.\n- Figures and tables need descriptive captions that clarify what is being depicted. In particular, tables need improvements in the headers and some highlighting of the results. It is also a good practice to mention them in the text (Figure 1b). Figure 2 is difficult to understand, and contains a legend with unclear definitions (see \"classified area\"). In table 2, it is impossible to understand what are the values presented in the cells. The algorithm needs some hints/comments/description.\n\n\n### References:\n\n- [tramer2020] Tramer, Florian, et al. \"On Adaptive Attacks to Adversarial Example Defenses.\" Advances in Neural Information Processing Systems 33 (2020).\n\n- [croce2020] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n- [brendel2020] Brendel, W., et al. \"Accurate, reliable and fast robustness evaluation.\" Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). Curran, 2020. ### Strengths: \n- Tries to improve efficiency of adversarial attacks\n\n### Weaknesses:\n- the paper is missing some definition that should not be taken for granted\n- the evaluation is not entirely convincing and might be unfair\n- results should be presented better, as they are very difficult to read and understand",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            }
        ],
        "generations": {
            "human_reference": "The paper focuses on the strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. One review gave a score of 8 while the other 3 reviewers gave negative scores. The main issue lies in the limited experiments, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper. The writing of the paper is somehow is not rigorous including many incorrect statements and unsupported claims which should be well addressed in the revision. Thus, it cannot be accepted to ICLR for its current version.",
            "random_reference": "The paper considers the problem of robust novelty detection where the goal is to find novel semantic or content changes while being robust to style shifts. The proposed methods works by ranking pretrained features using Wasserstein distance across environments and removing features with highest Wasserstein distances. The method is motivated by the assumption that feature space is comprehensive enough to capture relevant distribution changes, and the style changes drive larger shifts in the feature space than content changes. Reviewers are of the view that the assumptions underlying the method are overly optimistic, and contribution of the paper is limited in scope and technical merit / novelty. Unfortunately these concerns make the paper unsuitable for publication at ICLR in its current form.",
            "modular_llama3": "This manuscript proposes a novel minimum-margin attack method for evaluating adversarial robustness, achieving comparable performance to state-of-the-art AutoAttack with significantly reduced computational cost. While the reviewers found the paper to be well-written and the method promising, several limitations and areas for improvement were noted. These include a lack of theoretical analysis, unclear definitions and notation, insufficient comparison with other methods, and minor issues such as undefined terms and uneven comparison scenarios. Additionally, the reviewers questioned the technical contribution and novelty of the method, and the experiments were found to have some weaknesses. To improve the paper's quality, revisions are needed to address these issues, provide clearer definitions and notation, and strengthen the evaluations and presentation of results.",
            "llama3_pr_chunk": "The proposed minimum-margin (MM) attack evaluates adversarial robustness efficiently, achieving comparable performance to AutoAttack at 3% of the computational time. The method has strengths in improved computation efficiency and offers a fresh idea in adversarial machine learning. However, it has limitations, including narrow comparisons, unclear definitions, lack of theoretical analysis, and weaknesses in experimental design. The paper also needs to address issues with the threat model, adversarial robustness, and uneven runtime comparisons to improve its validity and readability.",
            "llama3_pr_consolidation": "This manuscript proposes a novel method, the minimum-margin (MM) attack, to evaluate adversarial robustness efficiently. The MM attack achieves comparable performance to the state-of-the-art AutoAttack (AA) while significantly reducing computational time. The method's reliability lies in evaluating the quality of adversarial examples using the margin between two targets, and its efficiency stems from the Sequential TArget Ranking Selection (STARS) method.\n\nThe strengths of the paper include its well-written and clear preliminaries, significant reduction in computational complexity, and solid experimental results. However, the paper also has several weaknesses, including a lack of comparison with other attacks beyond PGD and CW, unclear definitions of key terms such as \"most adversarial example,\" and limited technical contribution and novelty.\n\nThe experimental evaluation is also flawed, with suboptimal parameters for the attacks used, uneven runtime comparisons, and a lack of consideration for state-of-the-art attacks. Additionally, the paper contains several incorrect statements and unsupported claims, including the definition of \"most adversarial example\" and the comparison of PGD and AA.\n\nOverall, the paper has potential but requires significant revisions to address the weaknesses and flaws mentioned above. The authors should provide clearer definitions, more comprehensive comparisons, and stronger experimental evaluations to support their claims. With revisions, this paper could make a valuable contribution to the field of adversarial machine learning.",
            "llama3_pr_knowledge": "**Summary of Reviews**\n\n**Advancement**: The proposed minimum-margin (MM) attack has the potential to significantly advance the field of adversarial machine learning by providing a faster and more reliable method for evaluating adversarial robustness. However, some reviewers raised concerns about the lack of theoretical analysis and the limited comparison with existing attacks.\n\n**Clarity**: The paper is generally well-written, with clear descriptions of the proposed method and empirical results. However, some reviewers pointed out that the paper lacks some definitions, such as the threat model, adversarial robustness, and the quality of adversarial examples.\n\n**Compliance**: The paper meets most of the publication requirements, but some reviewers noted that the evaluation is not entirely convincing and might be unfair. Additionally, the paper lacks some references, such as [Gowal et al., 2019] and [Brendel et al., 2020].\n\n**Soundness**: The empirical soundness of the paper is generally good, with solid experiments and results. However, some reviewers raised concerns about the theoretical soundness, such as the lack of analysis on the convergence of the attack and the stability of the algorithm.\n\n**Novelty**: The proposed MM attack is novel and has the potential to contribute to the field of adversarial machine learning. However, some reviewers noted that the technical contribution and novelty of MM are limited, and that the idea of multi-target attacking has already been proposed in [Gowal et al., 2019].",
            "llama3_pr_decomposed": "The paper on the minimum-margin (MM) attack method is situated within the broader context of adversarial robustness, a critical area of research in machine learning. The MM attack builds upon existing work on adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), which have demonstrated the vulnerability of machine learning models to carefully crafted input perturbations.\n\nThe concept of adversarial robustness is closely related to the idea of robust optimization, which aims to develop models that are resilient to perturbations and uncertainties in the input data. The MM attack can be seen as a contribution to this line of research, providing a more efficient and effective method for generating adversarial examples.\n\nThe paper's comparison with AutoAttack (AA) is also relevant in this context, as AA is a state-of-the-art attack method that has been widely used to evaluate the robustness of machine learning models. The MM attack's advantages over AA, in terms of computational efficiency and effectiveness, highlight its potential as a valuable tool for researchers and practitioners.\n\nFurthermore, the paper's findings have implications for related areas, such as explainability and interpretability in machine learning. The MM attack's ability to generate adversarial examples that are misclassified by machine learning models can provide insights into the models' decision-making processes and vulnerabilities.\n\nKey concepts in this context include:\n\n* Adversarial robustness: the ability of machine learning models to withstand adversarial attacks and perturbations.\n* Robust optimization: the development of models that are resilient to perturbations and uncertainties in the input data.\n* Adversarial examples: input perturbations that are designed to mislead machine learning models.\n* Explainability and interpretability: the ability to understand and interpret the decision-making processes of machine learning models.\n\nOverall, the paper on the MM attack method is situated within a rich and dynamic research landscape, with connections to various areas of machine learning and related fields.",
            "modular_gpt4": "The paper introduces an innovative minimum-margin (MM) attack method for assessing adversarial robustness, notable for its substantial speed advantage and reliability. However, concerns about the novelty and technical contributions were raised, as some employed strategies lack originality. The MM method demonstrates performance on par with AutoAttack (AA) but with significantly reduced computational time. Despite its efficiency, the paper needs to address theoretical analysis, evaluation consistency, hyperparameter optimization, and provide more even runtime comparisons. Additionally, the paper should enhance clarity in definitions, figures, and tables, and improve result presentation. While the method's effectiveness and computational efficiency are applauded, further extensive testing against diverse defenses is required. The reviewers find it technically sound and well-written, recommending minor revisions to address identified issues for the paper to be a significant contribution to the field."
        }
    }
}