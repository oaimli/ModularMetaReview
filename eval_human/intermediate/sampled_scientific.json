[
    {
        "paper_id": "iclr_2024_Hh0Cg4epYY",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems. Utilizing f-divergence bounds to gauge the dissimilarity between distinct distributions, we establish an upper bound for Bayes error. This bound serves as a criterion for neural network training and test data classification. We showcase this technique's applicability to both binary and multi-class cases, examining the network output against a specific threshold for classification. Experimental results substantiate the method's effectiveness in approximating Bayes error. These experiments focus on Gaussian distributions with disparate means but identical variance, comparing the outcomes with theoretical Bayes error. Finally, the paper explores the potential applications of this approach in Generative Adversarial Networks (GANs), offering a promising avenue for future research.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "Wx2gxGMlC4",
                "content": " Dear Reviewers,\n\nThank you for your valuable feedback on our manuscript. We have diligently revised the paper to address your concerns and enhance its overall quality. Here is a summary of the significant changes we have implemented:\n\n1. Enhanced Clarity and Detail: We have comprehensively revised the technical content for improved clarity. Definitions and detailed explanations have been added, especially for mathematical quantities and equations, to make the methodology and objectives of our work more understandable.\n\n2. Terminology Adjustment: In the abstract, we have replaced the term \"groundbreaking\" with \"novel,\" aligning the description more accurately with the nature of our work.\n\n3. Organizational Structure: An \"Organization\" paragraph has been included at the end of the Introduction section, outlining the structure and content of each subsequent section, providing a clear roadmap for readers.\n\n4. Theoretical Results as Theorems: Key theoretical results have been presented as formal \"theorems,\" with precise assumptions and conclusions, to emphasize their significance and align with academic standards.\n\n5. Resolution of Specific Queries: We have addressed all specific questions about our methodology and equations to ensure each aspect of our approach is clear and justified.\n\n6. Goals and Contribution Clarity with Bayes GAN Finding: The goals and contributions of our paper have been further clarified, especially highlighting our findings with the Bayes GAN method. Our work in estimating Bayes error using neural networks is a significant advancement, as Bayes error is the optimal statistical error achievable in classification tasks. This neural estimation of Bayes error underscores the theoretical and practical significance of our method. In the domain of Generative Adversarial Networks (GANs), our Bayes GAN method demonstrates a notable achievement by consistently achieving lower FID (Fr\u00e9chet Inception Distance) scores on the MNIST dataset, signifying a closer resemblance to real image distributions and therefore enhanced realism in generated images. This underscores our method's practical utility in advancing generative models.\n\nWe believe that these revisions effectively address the points raised in your reviews, enhancing both the clarity and the substantive contribution of our paper. We appreciate your guidance and are confident that our responses and the amendments made reflect our dedication to contributing meaningful and high-quality research to the field.\n\nThank you for your time and consideration.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "wIXISWv7ld",
                "content": "The paper is obviously incomplete and more like a research note. I do not see any merit in the current format. None The paper is obviously incomplete and more like a research note. I do not see any merit in the current format.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "N53343EVgK",
                "content": "The paper considers bounding the Bayes error via f-divergence. The paper is incomplete and merely a draft. The problem of upper bounding Bayes error is somewhat interesting, though I am not sure how important it is. The paper does not present sufficient evidence in handling it. The math in the paper is very unclear.\nBayes error is the minimum achievable error. What is the usage of characterizing Bayes error, for model selection? \nUpper bounding generalization error via f-divergence has been previously well-studied.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            },
            {
                "review_id": "mpzI6nvJ1c",
                "content": "In the context of binary and multi-class classification, this paper derives an upper bound onthe Bayes risk in terms of an $f$-divergence (unfortunately, I couldn't quite understand the inputs to the $f$-divergence). The paper then proposes to train neural networks for classification, and GANs for generative modeling, using this $f$-divergence. Experimental results demonstrate that the proposed methods work (a) for classifying two Gaussian classes, (b) for the MNIST digit classification task, and (c) for generating novel MNIST samples. The overall approach seems like an interesting idea, as it suggests a new objective for optimizing complex classifiers, such as neural networks. Overall, the paper was insufficiently detailed for me to understand either its technical content or its high-level goals.\n\nMany quantities are used without being defined (e.g., in Eq. (1), what are $P$, $Q$, and $X$, what conditions is $f$ assumed to satisfy, etc.).\n\nAt a higher level, it is not clear to me what the goal of this paper is, in relation to the larger literature on machine learning (e.g., what gap or limitation of existing methods does this seek to fill?). Providing some quantitative evaluation comparing the proposed method and existing state-of-the-art methods might be one way to help address this.\n\nSome suggestions on improving the presentation:\n1) In the first sentence of the abstract, \"groundbreaking\" is probably too strong of a word; \"novel\" would be more appropriate.\n2) Typically, square brackets (e.g., \"[7]\") are used for numerical literature citations, while parentheses are reserved for referencing equations (e.g., \"Eq. (7)\").\n3) I suggest adding an \"Organization\" paragraph at the end of the Introduction Section explaining the content and goals of each of the subsequent sections of the paper.\n4) The key novel theoretical results (e.g., new upper bounds such as Eqs. (15) or (23)) should be presented as a self-contained \"theorem\" (e.g., using ```\\begin{theorem} ... \\end{theorem}```), including the precise assumptions made and conclusions drawn.",
                "reply_to": "iclr_2024_Hh0Cg4epYY"
            }
        ],
        "human_reference": "This paper studies how to approximate the upper limit of Bayes error in classification tasks. The reviewers raised significant concerns about the technical quality, novelty, evaluations, and completeness of this submission. Specifically:\n\n(1) Multiple reviewers found the mathematical development unclear or insufficiently detailed. \n(2) It is unclear how the proposed method relates to or improves upon existing techniques for bounding generalization error.\n(3) No experiments quantitatively compare to prior state-of-the-art methods.\n(4) Reviewers commented that the submission resembles an incomplete research note more than a full paper. \n\nWhile I appreciate the authors' efforts, given these major issues, I do not believe this work is suitable for presentation at this time.",
        "generation_decomposed": "Unfortunately, I have no text to summarize. Please provide the necessary text, and I will be happy to assist by distilling the summary into a concise and clear paragraph that captures the essence of the paper and its significance.",
        "steps_decomposed": [
            {
                "action": "1. Begin by identifying the main topic and key contributions of the paper as presented in the reviews.",
                "output": "The main topic of the paper is the development of a novel technique for approximating the upper limit of Bayes error in classification tasks, including binary and multi-class problems, using f-divergence bounds. \n\nThe key contributions of the paper include:\n\n1. Deriving an upper bound on the Bayes risk in terms of an f-divergence for binary and multi-class classification tasks.\n2. Proposing the use of this f-divergence as an objective for training neural networks for classification and Generative Adversarial Networks (GANs) for generative modeling.\n3. Experimental results demonstrating the effectiveness of the proposed methods in classifying two Gaussian classes, the MNIST digit classification task, and generating novel MNIST samples.\n4. A novel approach for estimating Bayes error using neural networks, which has significant theoretical and practical implications, particularly in advancing generative models."
            },
            {
                "action": "2. Extract key phrases or sentences that highlight the methodology and objectives of the paper, such as the use of f-divergence bounds to approximate Bayes error.",
                "output": "* Deriving an upper bound on the Bayes risk in terms of an f-divergence for binary and multi-class classification tasks.\n* Proposing the use of this f-divergence as an objective for training neural networks for classification and Generative Adversarial Networks (GANs) for generative modeling.\n* Approximating the upper limit of Bayes error in classification tasks using f-divergence bounds."
            },
            {
                "action": "3. Determine the significance and impact of the research by looking for phrases that emphasize its novelty, potential applications, and relevance to the field, such as the use of the proposed method in Generative Adversarial Networks (GANs).",
                "output": "The significance and impact of the research lie in its potential to derive an upper bound on the Bayes risk in terms of an f-divergence for binary and multi-class classification tasks, as well as its proposed application in training neural networks for classification and Generative Adversarial Networks (GANs) for generative modeling."
            },
            {
                "action": "4. Note the revisions made to the paper in response to reviewer feedback, such as enhanced clarity, added definitions, and improved organizational structure.",
                "output": "In response to the reviewer feedback, the authors have made several revisions to enhance the paper's clarity, including adding clear definitions and improving the overall organizational structure."
            },
            {
                "action": "5. Focus on the results and findings presented in the paper, including experimental results that substantiate the method's effectiveness in approximating Bayes error.",
                "output": "The authors have not provided sufficient experimental results to substantiate the method's effectiveness in approximating Bayes error."
            },
            {
                "action": "6. Identify any limitations or criticisms mentioned in the reviews, such as the paper being incomplete or lacking sufficient evidence to support its claims.",
                "output": "The authors have not provided sufficient experimental results to substantiate the method's effectiveness in approximating Bayes error."
            },
            {
                "action": "7. Summarize the main points and takeaways from the reviews, highlighting the paper's contributions, methodology, and potential impact on the field.",
                "output": "Unfortunately, the provided review does not offer sufficient information to summarize the main points, contributions, methodology, and potential impact of the paper. The review only mentions a limitation of the paper, specifically that the authors have not provided sufficient experimental results to substantiate the method's effectiveness in approximating Bayes error."
            },
            {
                "action": "8. Distill the summary into a concise and clear paragraph that captures the essence of the paper and its significance.",
                "output": "Unfortunately, a concise summary of the paper cannot be provided due to insufficient information."
            }
        ],
        "generation_modular": "The manuscript proposes a novel method for estimating the upper bound of Bayes error in classification tasks using f-divergence bounds and neural networks, offering a significant advancement in approximating optimal statistical error and demonstrating potential applications in Generative Adversarial Networks (GANs). However, despite improvements in clarity and methodology justification, the paper still requires substantial revisions to address several key issues. Specifically, the manuscript needs further quantitative evaluation, clearer mathematical notation, and a more self-contained presentation of novel theoretical results. Additionally, some reviewers found the paper incomplete and unclear, suggesting the need for enhanced presentation and comparison with state-of-the-art methods. To fully realize its potential, the authors should provide more detail and evidence, define key quantities, and formally present key results as theorems.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Utilizing f-divergence bounds to gauge the dissimilarity between distinct distributions, we establish an upper bound for Bayes error.",
                    "Experimental results substantiate the method's effectiveness in approximating Bayes error.",
                    "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems.",
                    "2. Terminology Adjustment: In the abstract, we have replaced the term \"groundbreaking\" with \"novel,\" aligning the description more accurately with the nature of our work.",
                    "6. Goals and Contribution Clarity with Bayes GAN Finding: The goals and contributions of our paper have been further clarified, especially highlighting our findings with the Bayes GAN method. Our work in estimating Bayes error using neural networks is a significant advancement, as Bayes error is the optimal statistical error achievable in classification tasks.",
                    "Upper bounding generalization error via f-divergence has been previously well-studied.",
                    "At a higher level, it is not clear to me what the goal of this paper is, in relation to the larger literature on machine learning (e.g., what gap or limitation of existing methods does this seek to fill?). ",
                    "The overall approach seems like an interesting idea, as it suggests a new objective for optimizing complex classifiers, such as neural networks."
                ],
                "meta_generated": "The proposed approach offers a novel method for estimating the upper bound of Bayes error in classification tasks using f-divergence bounds and neural networks, presenting a significant advancement in approximating optimal statistical error."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "Experimental results substantiate the method's effectiveness in approximating Bayes error.",
                    "We have addressed all specific questions about our methodology and equations to ensure each aspect of our approach is clear and justified.",
                    "Key theoretical results have been presented as formal \"theorems,\" with precise assumptions and conclusions, to emphasize their significance and align with academic standards.",
                    "We have comprehensively revised the technical content for improved clarity. Definitions and detailed explanations have been added, especially for mathematical quantities and equations, to make the methodology and objectives of our work more understandable.",
                    "The math in the paper is very unclear.",
                    "The paper does not present sufficient evidence in handling it.",
                    "Upper bounding generalization error via f-divergence has been previously well-studied.",
                    "Many quantities are used without being defined (e.g., in Eq. (1), what are $P$, $Q$, and $X$, what conditions is $f$ assumed to satisfy, etc.).",
                    "Providing some quantitative evaluation comparing the proposed method and existing state-of-the-art methods might be one way to help address this.",
                    "The key novel theoretical results (e.g., new upper bounds such as Eqs. (15) or (23)) should be presented as a self-contained \"theorem\" (e.g., using ```\\begin{theorem} ... \\end{theorem}```), including the precise assumptions made and conclusions drawn."
                ],
                "meta_generated": "The paper has improved in terms of clarity and methodology justification, but further work is needed to address issues such as insufficient quantitative evaluation, unclear mathematical notation, and presenting novel theoretical results in a self-contained manner."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "The goals and contributions of our paper have been further clarified, especially highlighting our findings with the Bayes GAN method.",
                    "Definitions and detailed explanations have been added, especially for mathematical quantities and equations, to make the methodology and objectives of our work more understandable.",
                    "Enhanced Clarity and Detail: We have comprehensively revised the technical content for improved clarity. ",
                    "An \"Organization\" paragraph has been included at the end of the Introduction section, outlining the structure and content of each subsequent section, providing a clear roadmap for readers.",
                    "Resolution of Specific Queries: We have addressed all specific questions about our methodology and equations to ensure each aspect of our approach is clear and justified.",
                    "The paper is obviously incomplete and more like a research note.",
                    "The math in the paper is very unclear.",
                    "3) I suggest adding an \"Organization\" paragraph at the end of the Introduction Section explaining the content and goals of each of the subsequent sections of the paper.",
                    "Many quantities are used without being defined (e.g., in Eq. (1), what are $P$, $Q$, and $X$, what conditions is $f$ assumed to satisfy, etc.).",
                    "Some suggestions on improving the presentation:",
                    "In the context of binary and multi-class classification, this paper derives an upper bound onthe Bayes risk in terms of an $f$-divergence (unfortunately, I couldn't quite understand the inputs to the $f$-divergence).",
                    "2) Typically, square brackets (e.g., \"[7]\") are used for numerical literature citations, while parentheses are reserved for referencing equations (e.g., \"Eq. (7)\").",
                    "1) In the first sentence of the abstract, \"groundbreaking\" is probably too strong of a word; \"novel\" would be more appropriate.",
                    "4) The key novel theoretical results (e.g., new upper bounds such as Eqs. (15) or (23)) should be presented as a self-contained \"theorem\" (e.g., using ```\\begin{theorem} ... \\end{theorem}```), including the precise assumptions made and conclusions drawn.",
                    "Providing some quantitative evaluation comparing the proposed method and existing state-of-the-art methods might be one way to help address this.",
                    "Overall, the paper was insufficiently detailed for me to understand either its technical content or its high-level goals."
                ],
                "meta_generated": "The paper has been improved with added clarity and detail, addressing specific queries and including an \"Organization\" paragraph, but some reviewers still find it incomplete and unclear, suggesting further revisions to enhance presentation and comparison with state-of-the-art methods."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "Finally, the paper explores the potential applications of this approach in Generative Adversarial Networks (GANs), offering a promising avenue for future research.",
                    "This bound serves as a criterion for neural network training and test data classification.",
                    "Experimental results substantiate the method's effectiveness in approximating Bayes error.",
                    "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems.",
                    "This underscores our method's practical utility in advancing generative models.",
                    "In the domain of Generative Adversarial Networks (GANs), our Bayes GAN method demonstrates a notable achievement by consistently achieving lower FID (Fr\u00e9chet Inception Distance) scores on the MNIST dataset, signifying a closer resemblance to real image distributions and therefore enhanced realism in generated images.",
                    "This neural estimation of Bayes error underscores the theoretical and practical significance of our method.",
                    "Our work in estimating Bayes error using neural networks is a significant advancement, as Bayes error is the optimal statistical error achievable in classification tasks.",
                    "The overall approach seems like an interesting idea, as it suggests a new objective for optimizing complex classifiers, such as neural networks.",
                    "At a higher level, it is not clear to me what the goal of this paper is, in relation to the larger literature on machine learning (e.g., what gap or limitation of existing methods does this seek to fill?)."
                ],
                "meta_generated": "This paper proposes a novel approach for approximating Bayes error using neural networks, demonstrating its effectiveness in classification tasks and potential applications in Generative Adversarial Networks (GANs), offering a promising avenue for future research."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "Definitions and detailed explanations have been added, especially for mathematical quantities and equations, to make the methodology and objectives of our work more understandable.",
                    "Key theoretical results have been presented as formal \"theorems,\" with precise assumptions and conclusions, to emphasize their significance and align with academic standards.",
                    "In the abstract, we have replaced the term \"groundbreaking\" with \"novel,\" aligning the description more accurately with the nature of our work.",
                    "The paper is obviously incomplete and more like a research note. ",
                    "I do not see any merit in the current format.",
                    "Upper bounding generalization error via f-divergence has been previously well-studied.",
                    "The paper is incomplete and merely a draft.",
                    "Many quantities are used without being defined (e.g., in Eq. (1), what are $P$, $Q$, and $X$, what conditions is $f$ assumed to satisfy, etc.).",
                    "Overall, the paper was insufficiently detailed for me to understand either its technical content or its high-level goals.\n\n",
                    "The key novel theoretical results (e.g., new upper bounds such as Eqs. (15) or (23)) should be presented as a self-contained \"theorem\" (e.g., using ```\\begin{theorem} ... \\end{theorem}```), including the precise assumptions made and conclusions drawn."
                ],
                "meta_generated": "The paper is incomplete and lacks sufficient detail to understand its technical content and high-level goals, with many quantities left undefined and key results not formally presented as theorems."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems.",
                    "We believe that these revisions effectively address the points raised in your reviews, enhancing both the clarity and the substantive contribution of our paper.",
                    "I do not see any merit in the current format.",
                    "The paper is obviously incomplete and more like a research note.",
                    "The paper does not present sufficient evidence in handling it.",
                    "The paper is incomplete and merely a draft.",
                    "The problem of upper bounding Bayes error is somewhat interesting, though I am not sure how important it is.",
                    "The overall approach seems like an interesting idea, as it suggests a new objective for optimizing complex classifiers, such as neural networks.",
                    "Overall, the paper was insufficiently detailed for me to understand either its technical content or its high-level goals."
                ],
                "meta_generated": "The paper proposes an innovative approach to approximating the upper limit of Bayes error in classification tasks, but its current format is deemed incomplete and lacking sufficient detail and evidence."
            }
        ]
    },
    {
        "paper_id": "iclr_2020_B1xmOgrFPS",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "rJe_tNnijS",
                "content": "Thanks for the comments! We agree with your concerns, and would like to offer clarifications for a clearer understanding. \n\nTo do a novel few-shot detection task, a prior needs to be acquired from some base data (e.g. meta train data in our case). To acquire this prior, we can follow two approaches: 1) Train a traditional model (e.g. a detector or classifier), and then fine tune on the novel few-shot task; OR 2) Acquire a prior via meta-learning on the base data, and learn a model that is trained to do few-shot learning. \n\nLSTD follows the first paradigm, while our proposed Meta-RCNN follows the second paradigm. Note that both methods have access to the exact same base data, i.e., they have access to the same information. They differ only in the learning algorithm. Then, a novel few-shot task is given to the algorithm, and the algorithm makes the prediction.\n\nSince both models have access to the same information, and make predictions on the same few-shot test task, the comparison is fair.\n\nData Split Difference\nMeta-learning literature (Vinyals et al. 2016, Finn et al. 2017, Snell et al. 2017, etc.) evaluates few-shot performance over multiple tasks drawn from a test task distribution, i.e., the few-shot performance is measured and averaged over multiple tasks. This is a more reliable metric than evaluating performance on only one few-shot task. LSTD data split considers evaluation on only one few-shot task in their data split. We train LSTD on appropriate base data, and then evaluate its performance over multiple tasks, and compare this performance with our method. \n",
                "reply_to": "BygTZ2FdYH"
            },
            {
                "review_id": "Bkeg5r3sir",
                "content": "Thanks for the comments! We do agree with some of your concerns, but do think that most of the suggested issues are addressable. \n\n1. Implementation details with high-res images\n\nThanks for the suggestions, and we apologise for lack details.\n\nDuring meta-training, we train the model using 5-way-1shot tasks, and only 5 query images (1 query image per class). This results in a total of 10 images for one task. With this, implementing the meta-training is not too difficult. Using this trained model, we evaluate performance on various settings (e.g. 5-way 1-shot, and 5-way 5-shot) meta test tasks. We apologize for the lack of clarity in the first RPN - this is not a critical component, and just a minor trick we use to make the prototype more robust (instead of constructing prototypes of the support objects by directly using ground truth bounding boxes and labels, we also use the proposals generated by the RPN, and if it has sufficient overlap with the ground truth, it is used for constructing the prototype). The main contribution in RPN is the one that is trained to generate support(class)-specific proposals.\n\nWe will definitely release the code.\n\n\n2 & 3. Regarding RepMet\n\nThanks for these comments regarding RepMet. \ni) We have improved the presentation to not call it RepMet, but to call it FRCN-PN(baseline), and have changed the written section describing the relation of FRCN-PN with RepMet.\nii) FRCN-PN shares a similar principle as RepMet (traditional detector training + replacing the object classifier with a meta-learner), and thus is a baseline we considered for our work.\niii) We would have liked to reproduce RepMet and compare directly with the original method, however, we were not able to find the code for it. As a result, we decided to implement the method based on this principle ourselves as a baseline. \niv) The code for RepMet: The arxiv version and the published version do not have a working link for the code being available online. We found a not well tested/incomplete version of the code  ( https://github.com/HaydenFaulkner/pytorch.repmet ) done by a third party, which has not yet reproduced the results. \n\n\n4. Comparison with Meta-RCNN in ICCV2019\n\nThanks for suggesting the reference. We believe this work was done in parallel with our work. We would like to highlight that this work was made available on arxiv (28th September) a few days after the ICLR submission deadline (25th September). Moreover, it appeared in ICCV even more recently (27th October).\n\nThis work does share some similarities as our work (principle of class-attentive module), however, there is a fundamental difference in the training approach, specifically for the RPN. In contrast to the reference paper, our RPN is meta-trained and is tailored to generate proposals for the few-shot setting. \n\nWe train the RPN in the meta-learning paradigm (meta-RPN), whereas the RPN training in the ICCV paper is trained using the traditional setting. This difference is extremely crucial for few-shot detection. Traditional RPNs will detect all objects in the image (including objects not of interest, i.e., they will even detect objects that are not available in given support set). Our meta-trained RPN generates proposals for an object from classes only belonging to the support set (i.e., it generates class-specific proposals). \n\nFinally, we would also like to highlight that following the meta-learning literature, we have evaluated the performance of the object detector on \u201cmultiple\u201d few-shot detection tasks. Our reported few-shot performance is average performance over these tasks, in contrast to the existing reference which evaluates result on exactly one few-shot task.\n\nAs regards empirical comparisons, it would be slightly time consuming to do this given different settings (e.g. hyperparameters, backbone, data splits, different approach for using the meta-train dataset, etc.). We do aim to do this in the future. \n",
                "reply_to": "HJeEHAb6tS"
            },
            {
                "review_id": "rygUTX2isH",
                "content": "Thank you for your review! We were delighted with your comments! \n\nAs regards novelty, we would like to highlight that it is not trivial to adapt meta-learning for object detection, and to the best of our knowledge, ours is the first work that trains both the object classifier and the RPN in a meta-learning paradigm, making all the components tailored for  few-shot detection.\n\nThanks for identifying the writing issues, we have fixed them in the current version. \n",
                "reply_to": "ryg6pPtQ5r"
            },
            {
                "review_id": "BygTZ2FdYH",
                "content": "In this paper, authors propose a meta-learning based approach for low-shot object detection. Specifically, they use prototype in the support set as attention guidance, and learn the category-specific representation for each query image. Subsequently, they use the style of Faster RCNN for object detection.\n\nIt is an OK paper with good structure. The idea is somewhat novel, in terms of meta-learning based low-shot detection framework. My main concern is about experiment. First, the data setting is branch new. Why not use the data setting in the literature, e.g., COCO to VOC in LSTD (Chen et al., 2018)? As a result, how to make a fair comparison bothers me a little. Furthermore, LSTD is a non-episodic approach. How to make it in a meta-learning way? Please clarify the implementation details for all other related works in the comparison.  ",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "HJeEHAb6tS",
                "content": "The paper proposes a method for few-shot object detection (FSOD), a variant of few-shot learning (FSL) where using a support set of few training images for novel categories (usually 1 or 5) not only the correct category labels are predicted on the query images, but also the object instances from the novel categories are localized and their bounding boxes are predicted. The method proposes a network architecture where the sliding window features that enter the RPN are first attenuated using support classes prototypes discovered using (a different?) RPN and found as matching to the few provided box annotations on the support images. The attenuation is by channel wise multiplication of the feature map and concatenation of the resulting feature maps (one per support class). After the RPN, ROI-pooling is applied on the concatenated feature map that is reduced using 1x1 convolution and original feature map (before attenuation) being added to the result. Following this a two FC layer classifier is fine-tuned on the support data to form the final \nRCNN head of the few-shot detector. The whole network is claimed to be meta-trained end to end following COCO or ImageNet (LOC? DET?) pre-training. The method is tested on a split of PASCAL VOC07 into two sets of 10 categories, one for meta-training and the other for meta-testing. In addition, experiments are carried out on ImageNet-LOC animals subset. In both cases, the result are compared to some baselines, and some prior work.\n\nAlthough FSOD is an important emerging problem, and advances on it are very important, I believe there are still certain gaps in the current paper that need to be fixed before it is accepted. Specifically:\n\n1. Some important details are missing from the description. For example, detectors are usually trained on high resolution images (e.g. 1000 x 1000) and hence are problematic to train with large batches, yet in the proposed approach it is claimed that the proposed model is meta-trained with batch size 5 on 5 way tasks with 10 queries each, so even in 1-shot case, does it mean that 5 x 15 = 75 high resolution images enter the GPU at each batch? I doubt that even in parallel mode with 5 GPUs and 15 high res image per GPU it is possible for claimed backbone architectures (ResNet-50 and VGG16).\nAs another example, the details of fine-tuning during meta-training seem to be left out, is the model optimized with an inner loop? Details of the RPN that is used to select the support categories prototypes are not specified, where it comes from and how is it trained (clearly as the \"main\" RPN relies on attenuated features, it cannot be it)? Some additional technical details are not very clear and hinder the reproducibility of the paper (no code seem to be promised?), in general I suggest the authors to improve the writing and clarity of the paper.\n\n2. In VOC07 experiment, FRCN-PN is very vaguely described and being claimed that it stands for RepMet (Karlinksy et al., CVPR 2019). It is not clear what it is and its training procedure on VOC07 is not clearly described.\nIt is also claimed in ImageNet experiment that the real RepMet is \"more carefully designed then FRCN-PN\" and has a better backbone, hence it is not clear why FRCN-PN should stand for it.\nI suggest the authors to either do a direct comparison or remove their claim of comparison.\n\n3. RepMet paper has proposed an additional benchmark on ImageNet-LOC with 5-way 1/5/10-shot episodes, and afaik it is reproducible as its code is released, so I am wondering as to why it was not used for \nevaluation given that the authors made the effort of reproducing another ImageNet-LOC test on the same categories? It should be evaluated for a fair comparison.\n\n4. Although they don't strictly have to compare to it, I am wondering if the authors would be willing to relate to a similar approach that was proposed for the upcoming ICCV 19: \n\"Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning\", by Yan et al. Their approach is more similar to RepMet in a sense that the meta-learning is done in the classifier head,\nand better results are reported on VOC07 benchmark (and except for 1-shot, higher results are reported for the 3 and 5 shot FRCNN fine-tuning).",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            },
            {
                "review_id": "ryg6pPtQ5r",
                "content": "This paper is about the task of object detection in the setting of few-shots dataset. The problem is addressed in the learning scheme of meta-learning paradigm: the proposed meta-rcnn trains the popular faster-rcnn on several tasks of few shots object detection while the RPN and the object classification networks are meta-learned among the tasks. Compared to previous work the paper introduces the meta learning framework and several changes to the faster rcnn detector. A prototype representation is derived from the standard RPN network and its proposed bounding box. An attention mechanism choose the object of interest and is used to train the final RPN and classification network. Experiments on the popular Pascal Voc 2007 and ImageNet-FSOD show that the proposed system have state of the art performance.\n\nThe paper is very well written, easy to read and of excellent presentation. The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting. The experiments are solid and show state of the art performance. As a result I recommend this paper to be accepted.\n\nMinor issues:\n- in caption of Fig1: avialable -> available\n- in 4.1: \u201cCompared to other variants...\u201d please add a reference to the specific methods you are comparing to.",
                "reply_to": "iclr_2020_B1xmOgrFPS"
            }
        ],
        "human_reference": "This paper develops a meta-learning approach for few-shot object detection. This paper is borderline and the reviewers are split. The problem is important, albeit somewhat specific to computer vision applications. The main concerns were that it was lacking a head-to-head comparison to RepMet and that it was missing important details (e.g. the image resolution was not clarified, nor was the paper updated to include the details). The authors suggested that the RepMet code was not available, but I was able to find the official code for RepMet via a simple Google search:\nhttps://github.com/jshtok/RepMet\nReviewers also brought up concerns about an ICCV 2019 paper, though this should be considered as concurrent work, as it was not publicly available at the time of submission.\nOverall, I think the paper is borderline. Given that many meta-learning papers compare on rather synthetic benchmarks, the study of a more realistic problem setting is refreshing. That said, it's unclear if the insights from this paper would transfer to other machine learning problem settings of interest to the ICLR community.\nWith all of this in mind, the paper is slightly below the bar for acceptance at ICLR.",
        "generation_decomposed": "Based on the reviewers' comments, the overall assessment of the paper is mixed. \n\nIn terms of writing style, the reviewers found the paper to be well-structured and clear in its presentation of ideas, but some sections were considered too technical and difficult to follow. One reviewer suggested that the authors could improve the paper's readability by providing more explanations and examples to support complex concepts.\n\nThe presentation of the paper was generally considered to be good, with the reviewers praising the authors' use of tables, figures, and headings to organize the content. However, some reviewers noted that the paper could benefit from more visual aids, such as diagrams or flowcharts, to help illustrate key points.\n\nRegarding the potential for acceptance, the reviewers were divided in their opinions. Some reviewers believed that the paper made a significant contribution to the field and was worthy of publication, while others raised concerns about the paper's methodology and the validity of its findings. One reviewer suggested that the authors could strengthen their argument by providing more empirical evidence to support their claims.\n\nOverall, the reviewers' feedback suggests that the paper has some strengths, but also needs some revisions to address the concerns raised. With some revisions to improve the writing style, presentation, and methodology, the paper may be considered for publication.",
        "steps_decomposed": [
            {
                "action": "1. Identify the main theme or topic of the reviews on the academic paper, which is the proposal of a meta-learning framework called Meta-RCNN for few-shot object detection.",
                "output": "The main theme or topic of the reviews on the academic paper is the proposal of a meta-learning framework called Meta-RCNN for few-shot object detection."
            },
            {
                "action": "2. Extract the key points discussed in the reviews, such as the challenges of few-shot object detection, the use of meta-learning, and the comparison with other methods like LSTD and RepMet.",
                "output": "The key points discussed in the reviews of the Meta-RCNN academic paper are:\n\n- The challenges of few-shot object detection, where traditional object detection methods struggle with limited training data and poor generalization to new classes.\n- The proposal of Meta-RCNN, a meta-learning framework designed to address the challenges of few-shot object detection by learning to adapt to new classes with few examples.\n- The use of meta-learning in Meta-RCNN, which enables the model to learn a set of parameters that can be fine-tuned for new classes with few shots, improving its generalization capabilities.\n- Comparison with other meta-learning methods like LSTD (Learning to Learn from Simulation Data) and RepMet (Representation-based Meta-Learning), highlighting the strengths and weaknesses of each approach in the context of few-shot object detection.\n- The potential benefits and limitations of Meta-RCNN, including its ability to learn from a few examples, its robustness to class imbalance, and its computational efficiency."
            },
            {
                "action": "3. Determine the main contributions and novelties of the proposed Meta-RCNN framework, such as its ability to train both the object classifier and the RPN in a meta-learning paradigm.",
                "output": "The main contributions and novelties of the proposed Meta-RCNN framework are:\n\n1. Training the object classifier and the Region Proposal Network (RPN) in a meta-learning paradigm, enabling the model to adapt to new classes with few examples.\n2. Developing a meta-learning framework that can learn to adapt to new classes with limited training data, addressing the challenges of few-shot object detection.\n3. Proposing a framework that can learn a set of parameters that can be fine-tuned for new classes with few shots, improving its generalization capabilities.\n4. Enabling the model to learn from a few examples, be robust to class imbalance, and be computationally efficient."
            },
            {
                "action": "4. Note the experimental results and evaluations presented in the paper, including the performance on Pascal VOC and ImageNet datasets.",
                "output": "4. Experimental results and evaluations were presented, with the Meta-RCNN framework achieving improved performance on Pascal VOC and ImageNet datasets."
            },
            {
                "action": "5. Identify the concerns and criticisms raised by the reviewers, such as the lack of implementation details, the unclear comparison with other methods, and the need for more technical details.",
                "output": "5. Reviewers raised concerns about the lack of implementation details, unclear comparison with other methods, and the need for more technical details."
            },
            {
                "action": "6. Summarize the overall assessment of the paper by the reviewers, including the feedback on the writing style, presentation, and the potential for acceptance.",
                "output": "Reviewers raised several concerns about the paper."
            }
        ],
        "generation_modular": "This manuscript proposes a novel Meta-RCNN framework for few-shot object detection, achieving state-of-the-art performance by leveraging meta-learning. The framework is well-suited for few-shot detection tasks and demonstrates effectiveness in generalizing from small amounts of labelled data. However, the manuscript requires revisions to improve its experimental design, comparisons with existing works, and reproducibility. Specifically, the comparison with RepMet is unclear and lacks direct evaluation, while several technical details need to be clarified. Additionally, the manuscript would benefit from using established data settings, providing implementation details, and increasing reproducibility by providing code and clearer writing. Overall, with minor revisions to address these concerns, the manuscript has the potential to make a valuable contribution to the field of few-shot object detection.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. ",
                    "Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned.",
                    "Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning.",
                    "Since both models have access to the same information, and make predictions on the same few-shot test task, the comparison is fair.",
                    "To do a novel few-shot detection task, a prior needs to be acquired from some base data (e.g. meta train data in our case).",
                    "LSTD follows the first paradigm, while our proposed Meta-RCNN follows the second paradigm.",
                    "As regards novelty, we would like to highlight that it is not trivial to adapt meta-learning for object detection, and to the best of our knowledge, ours is the first work that trains both the object classifier and the RPN in a meta-learning paradigm, making all the components tailored for  few-shot detection.",
                    "The idea is somewhat novel, in terms of meta-learning based low-shot detection framework.",
                    "The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting."
                ],
                "meta_generated": "The proposed Meta-RCNN framework is a novel meta-learning approach to few-shot object detection that learns both the Region Proposal Network and object classification branch via meta-learning, making it well-suited for few-shot detection tasks."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "Since both models have access to the same information, and make predictions on the same few-shot test task, the comparison is fair.",
                    "LSTD data split considers evaluation on only one few-shot task in their data split. We train LSTD on appropriate base data, and then evaluate its performance over multiple tasks, and compare this performance with our method.",
                    "We train the RPN in the meta-learning paradigm (meta-RPN), whereas the RPN training in the ICCV paper is trained using the traditional setting.",
                    "We will definitely release the code.",
                    "As regards empirical comparisons, it would be slightly time consuming to do this given different settings (e.g. hyperparameters, backbone, data splits, different approach for using the meta-train dataset, etc.).",
                    "This work does share some similarities as our work (principle of class-attentive module), however, there is a fundamental difference in the training approach, specifically for the RPN.",
                    "We would have liked to reproduce RepMet and compare directly with the original method, however, we were not able to find the code for it.",
                    "Why not use the data setting in the literature, e.g., COCO to VOC in LSTD (Chen et al., 2018)?",
                    "Furthermore, LSTD is a non-episodic approach.",
                    "As a result, how to make a fair comparison bothers me a little.",
                    "How to make it in a meta-learning way?",
                    "First, the data setting is branch new.",
                    "My main concern is about experiment.",
                    "Please clarify the implementation details for all other related works in the comparison.",
                    "RepMet paper has proposed an additional benchmark on ImageNet-LOC with 5-way 1/5/10-shot episodes, and afaik it is reproducible as its code is released, so I am wondering as to why it was not used for ",
                    "Details of the RPN that is used to select the support categories prototypes are not specified, where it comes from and how is it trained (clearly as the \"main\" RPN relies on attenuated features, it cannot be it)?",
                    "It is also claimed in ImageNet experiment that the real RepMet is \"more carefully designed then FRCN-PN\" and has a better backbone, hence it is not clear why FRCN-PN should stand for it.",
                    "Some important details are missing from the description. For example, detectors are usually trained on high resolution images (e.g. 1000 x 1000) and hence are problematic to train with large batches, yet in the proposed approach it is claimed that the proposed model is meta-trained with batch size 5 on 5 way tasks with 10 queries each, so even in 1-shot case, does it mean that 5 x 15 = 75 high resolution images enter the GPU at each batch?",
                    "evaluation given that the authors made the effort of reproducing another ImageNet-LOC test on the same categories?",
                    "As another example, the details of fine-tuning during meta-training seem to be left out, is the model optimized with an inner loop?",
                    "In VOC07 experiment, FRCN-PN is very vaguely described and being claimed that it stands for RepMet (Karlinksy et al., CVPR 2019). It is not clear what it is and its training procedure on VOC07 is not clearly described.",
                    "Minor issues:\n- in caption of Fig1: avialable -> available\n- in 4.1: \u201cCompared to other variants...\u201d please add a reference to the specific methods you are comparing to.",
                    "The experiments are solid and show state of the art performance."
                ],
                "meta_generated": "The paper presents a new approach to few-shot object detection with state-of-the-art performance, but the experimental design and comparisons with existing works, particularly RepMet and LSTD, could be improved for a fair evaluation."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "To acquire this prior, we can follow two approaches: 1) Train a traditional model (e.g. a detector or classifier), and then fine tune on the novel few-shot task; OR 2) Acquire a prior via meta-learning on the base data, and learn a model that is trained to do few-shot learning.",
                    "i) We have improved the presentation to not call it RepMet, but to call it FRCN-PN(baseline), and have changed the written section describing the relation of FRCN-PN with RepMet.",
                    "Thanks for the suggestions, and we apologise for lack details.",
                    "We would have liked to reproduce RepMet and compare directly with the original method, however, we were not able to find the code for it.",
                    "Thanks for identifying the writing issues, we have fixed them in the current version.",
                    "It is an OK paper with good structure.",
                    "I doubt that even in parallel mode with 5 GPUs and 15 high res image per GPU it is possible for claimed backbone architectures (ResNet-50 and VGG16).",
                    "It is not clear what it is and its training procedure on VOC07 is not clearly described.",
                    "It should be evaluated for a fair comparison.",
                    "I suggest the authors to either do a direct comparison or remove their claim of comparison.",
                    "Some additional technical details are not very clear and hinder the reproducibility of the paper (no code seem to be promised?), in general I suggest the authors to improve the writing and clarity of the paper.",
                    "The paper is very well written, easy to read and of excellent presentation."
                ],
                "meta_generated": "The paper is well-structured, but the comparison with RepMet is unclear and lacks direct evaluation, which hinders the reproducibility of the paper, and several technical details need to be clarified."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data.",
                    "We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results.",
                    "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge.",
                    "Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data.",
                    "Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning.",
                    "To acquire this prior, we can follow two approaches: 1) Train a traditional model (e.g. a detector or classifier), and then fine tune on the novel few-shot task; OR 2) Acquire a prior via meta-learning on the base data, and learn a model that is trained to do few-shot learning. \n\n",
                    "To do a novel few-shot detection task, a prior needs to be acquired from some base data (e.g. meta train data in our case). ",
                    "Meta-learning literature (Vinyals et al. 2016, Finn et al. 2017, Snell et al. 2017, etc.) evaluates few-shot performance over multiple tasks drawn from a test task distribution, i.e., the few-shot performance is measured and averaged over multiple tasks.",
                    "We train the RPN in the meta-learning paradigm (meta-RPN), whereas the RPN training in the ICCV paper is trained using the traditional setting. This difference is extremely crucial for few-shot detection. Traditional RPNs will detect all objects in the image (including objects not of interest, i.e., they will even detect objects that are not available in given support set). Our meta-trained RPN generates proposals for an object from classes only belonging to the support set (i.e., it generates class-specific proposals).",
                    "We have improved the presentation to not call it RepMet, but to call it FRCN-PN(baseline), and have changed the written section describing the relation of FRCN-PN with RepMet.\nii) FRCN-PN shares a similar principle as RepMet (traditional detector training + replacing the object classifier with a meta-learner), and thus is a baseline we considered for our work.\n",
                    "As regards novelty, we would like to highlight that it is not trivial to adapt meta-learning for object detection, and to the best of our knowledge, ours is the first work that trains both the object classifier and the RPN in a meta-learning paradigm, making all the components tailored for  few-shot detection.",
                    "The idea is somewhat novel, in terms of meta-learning based low-shot detection framework.",
                    "It is an OK paper with good structure. ",
                    "The paper proposes a method for few-shot object detection (FSOD), a variant of few-shot learning (FSL) where using a support set of few training images for novel categories (usually 1 or 5) not only the correct category labels are predicted on the query images, but also the object instances from the novel categories are localized and their bounding boxes are predicted.",
                    "Although FSOD is an important emerging problem, and advances on it are very important",
                    "The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting.",
                    "As a result I recommend this paper to be accepted.",
                    "The experiments are solid and show state of the art performance.",
                    "This paper is about the task of object detection in the setting of few-shots dataset."
                ],
                "meta_generated": "The proposed Meta-RCNN model addresses few-shot object detection by leveraging meta-learning, achieving promising results on the Pascal VOC dataset and demonstrating its effectiveness in generalizing from small amounts of labelled data."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "Why not use the data setting in the literature, e.g., COCO to VOC in LSTD (Chen et al., 2018)?",
                    "Please clarify the implementation details for all other related works in the comparison.",
                    "Some additional technical details are not very clear and hinder the reproducibility of the paper (no code seem to be promised?), in general I suggest the authors to improve the writing and clarity of the paper.\n\n"
                ],
                "meta_generated": "The paper needs improvement in several areas, including using established data settings, clarifying implementation details, and increasing reproducibility by providing code and clearer writing."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "The idea is somewhat novel, in terms of meta-learning based low-shot detection framework.",
                    "It is an OK paper with good structure. ",
                    "Although FSOD is an important emerging problem, and advances on it are very important, I believe there are still certain gaps in the current paper that need to be fixed before it is accepted.",
                    "The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting.",
                    "The paper is very well written, easy to read and of excellent presentation.",
                    "The experiments are solid and show state of the art performance.",
                    "As a result I recommend this paper to be accepted."
                ],
                "meta_generated": "This paper proposes a novel meta-learning based low-shot detection framework, with a well-written and easy-to-read presentation, solid experiments, and state-of-the-art performance, making it acceptable with minor revisions."
            }
        ]
    },
    {
        "paper_id": "iclr_2023_4orJ47he7WV",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning. To this end, We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness. In Lux, we build our agents through the standard reinforcement learning algorithm in curriculum learning phases and leverage centralized control via a pixel-to-pixel policy network. As agents co-evolve through self-play, we observe several stages of intelligence, from the acquisition of atomic skills to the development of group strategies. Since these learned group strategies arise from individual decisions without an explicit coordination mechanism, we claim that artificial collective intelligence emerges from massive-agent cooperation and competition. We further analyze the emergence of various learned strategies through metrics and ablation studies, aiming to provide insights for reinforcement learning implementations in massive-agent environments.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "P2shM3giQ6m",
                "content": "The paper seems to be deanonymized. \"To this end, we propose Lux (Doerschuk-Tiberi & Tao, 2021)\" which links to this github repo: \"https://github.com/Lux-AI-Challenge/Lux-Design-2021\". If this is not the case, I will provide a full review.   TBD TBD TBD",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "xXqhXQdw4I",
                "content": "This paper introduces a massively multi-agent (thousands of agents) learning environment, Lux, in which teams compete in a custom Real-Time Strategy (RTS) grid world with limited resources. There are two types of agents on each team, Workers and CityTiles, with distinct actions available. The main contributions of the paper, as indicated by the authors, are (1) the introduction of the Lux learning environment; (2) evidence that there are emergent collective behaviors including local strategies such as regional coordination and global strategies such as sustainable development; (3) the implementation details of a particular algorithm using curriculum training that appears to have a very high success rate against the winner of a competition in developing agents for this environment.  Strengths:\n- This paper demonstrates multi-agent reinforcement learning in a complex domain with limited resources.\n- The paper is written clearly and the appendix has many details aiding reproducibility.\n- The Lux environment is very compelling and could definitely be a source of important new results in multi-agent reinforcement learning, especially in the context of limited resources.\n\n\nWeaknesses:\n- One of the biggest weaknesses is the nature of the first contribution (introducing the environment itself). This paper mentions introducing this environment, but then cites the Doerschuk-Tiberi and Tao (2021) GitHub page and associated competition. If this paper is coming from the authors of the repo, then this is clearly breaking anonymity. Otherwise, it is not clear why the authors are stating this as a contribution.\n- The paper does not adequately explain why the algorithm that is used here has an \"up to 90% win rate\" against the Isaiah et al. (2021) policy; it appears that this is entirely due to the curriculum training, as mentioned in the caption to Figure 10; otherwise the win rate is only 20%. If that is the case, is the contribution about the effectiveness of curriculum training? If so, how would curriculum training benefit another algorithm in this environment, such as the Isaiah et al. (2021) policy?\n- While using a central policy for all of the agents in the solution makes sense from a practical perspective, it primarily makes the action space enormous but removes the individual independence of agents. Would it be possible to train individual agents in this framework, and what would this look like?\n- The particular methodology described in Section 4 for a particular neural architecture does not make it clear how difficult it would be to train different neural architectures or to use different algorithms with this framework.\n- The accompanying video is not sufficiently self-contained; some text in the appendix explaining what the video demonstrates would be useful. As mentioned in the weaknesses above, it is very important that the authors clarify their role relative to the existing competition. If this is not a double-blind submission, and the nature of the competition that has already occurred would make anonymity impossible, it may be better suited to a single-blind venue such as a Datasets and Benchmarks track (regularly at NeurIPS) or similar. By itself, the algorithmic contributions are not clear and limited to the design of the curriculum training approach, which appears to be very successful. Is the main result that curriculum training is very important for massively multi-agent settings? If so, then this should be emphasized, along with a study of how this approach is useful when paired with other algorithms/approaches. Or is the architecture that is proposed here better-suited for curriculum training specifically? What are the limitations of the centralized policy adopted in the proposed solution?\n\nMinor point: there is a missing reference in line 4 of the Introduction. This paper needs to clarify what the contributions are (environment vs. curriculum training). If it is the environment, then this should be either submitted to a different venue or anonymized properly before submission. Also, there are a number of questions about what it would take to train agents in this environment, presumably already described in the competition rules referenced in the Appendix. If it is the curriculum training approach, then there needs to be further study of the effect that this has relative to \"the base algorithm\" it is used with.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "-3eKRfJ1Of",
                "content": "The paper proposes a new benchmark to train a large number of agents in an environment with limited available resources. It also explores the connection of learning atomic / low-level skills with that of learning social strategies such as cooperation, coordination, and competition.  The proposed environment captures the essential elements of learning diverse social behaviors required to sustain in a society with limited resources.\n\nOne concern I have with the work is the lack of comparison to prior frameworks. I'm skeptical as to what additional features Lux provides that are not already there in previous massive-agent frameworks like MAgent. The comparisons done in the paper do not highlight the main advantage of using Lux over previous frameworks. The paper is clearly written and is easy to read. The proposed environment is a novel contribution and there are a number of ablation studies performed to study the various strategies that are evolved throughout the learning. The paper proposes a new massive-agent RL framework that could act as a benchmark for but testing agents' ability to cooperate, coordinate or learn different kinds of social behaviors.",
                "reply_to": "iclr_2023_4orJ47he7WV"
            },
            {
                "review_id": "UvEvnY14xpQ",
                "content": "In this paper, the authors present Lux, an environment for multi-agent reinforcement learning at scale. The environment is evaluated through simulations using a curriculum learning solution. The main contribution of this work resides in the environment itself, which allows for a simulation of multi-agent interactions at scale. However, the dynamics of the game itself does not appear particularly insightful. The implementation of the game itself is also rather standard.  General Note\n\nIt appears that the paper is not anonymized since the authors say \"To this end, we propose Lux (Doerschuk-Tiberi & Tao, 2021), a cooperative and competitive environment where hundreds of agents in two populations scramble for limited resources and fight off the darkness\". It appears that the authors of the work are linked to Doerschuk-Tiberi & Tao.\n\nStrengths\n\n- The release of a platform for the research community is commendable.\n- The emerging patterns are interesting, but somehow expected since they are the direct consequence of the reward structure of the game.\n\nWeaknesses\n\n- The paper is not self-contained. Key parts of the paper are in the Appendix. It is not possible to understand the dynamics of the game without checking the Appendix. Several key experimental results are in the Appendix.\n- It is difficult to see a clear scientific contribution in this work. The reviewer understands that the authors are making available a new environment for the community, but \n- Most of the description of the technical details of the implementation of Lux and its evaluation are in the Appendix. It is worth noting that the some of the notation itself cannot be found in the main paper.\n- The actual contribution of this work is unclear, since it mixes the presentation of the platform and the evaluation with a specific curriculum learning algorithm.\n- The curriculum learning solution used by the authors is rather standard. The results are not particularly insightful.\n \n\nThe paper is not well structured. Key parts are in the Appendix, including the definition of the notation used in the paper.\n\nThe reviewer believes that the novelty of this work is rather limited. The implementation of the environment itself is rather standard. \n\nThe emerging patterns in terms of strategy are interested, but they appear as a direct consequence of the reward structure defined by the authors. The reviewer believes that the contribution itself is not sufficient for a conference of the level of ICLR.\n\nThe study using curriculum learning is also rather standard for this type of environments. There are limited novel insights that are emerging in this analysis.\n\nThe authors should carefully proofread the paper. There are missing citations and wrong capitalization in the text. The presentation of the evaluation results in terms of description of the experimental settings can also be improved. The discussion of the experimental results is rather concise.\n \nThe authors present an interesting environment, which is made available to the research community as open source software. However, the environment itself is not particularly innovative compared to the state of the art. The evaluation of the platform considering a curriculum algorithm solution is rather standard. The paper is not well structured and impossible to understand without reading the Appendix. Key sections are not in the main paper. \n\nThe reviewer also would like to stress the fact that the paper does not appear to be fully anonymized. There is a direct citation of the GitHub page of the project with the names of the authors of the manuscript.\n\n",
                "reply_to": "iclr_2023_4orJ47he7WV"
            }
        ],
        "human_reference": "I thank the authors for their submission and active engagement during the discussion period. The reviewers unanimously agree that this paper is not ready for publication. In particular, the reviewers remarked missing clarity around the contributions and a lack of comparison to similar frameworks. Therefore I recommend rejection. ",
        "generation_decomposed": "The reviewers expressed significant skepticism about the paper's contributions and novelty, questioning the originality of the Lux environment implementation and the curriculum learning solution. They found these aspects to be relatively standard, lacking significant innovations or improvements over existing approaches, thus raising concerns about the paper's overall contributions and novelty.",
        "steps_decomposed": [
            {
                "action": "1. Identify the main topic of the reviews, which is the paper proposing a new massive-agent reinforcement learning environment called Lux.",
                "output": "The main topic of the reviews is the paper proposing a new massive-agent reinforcement learning environment called Lux."
            },
            {
                "action": "2. Determine the key aspects of the paper discussed in the reviews, including the introduction of the Lux environment, the use of curriculum learning, and the emergence of collective behaviors.",
                "output": "The key aspects of the paper discussed in the reviews are:\n\n1. **Introduction of the Lux environment**: The reviewers have discussed the introduction of Lux as a new massive-agent reinforcement learning environment, highlighting its potential and significance in the field.\n2. **Use of curriculum learning**: The reviews mention the use of curriculum learning in the Lux environment, which allows agents to learn and adapt in a more structured and efficient manner.\n3. **Emergence of collective behaviors**: The reviewers have noted the emergence of collective behaviors among agents in the Lux environment, which is a key aspect of the paper and a significant contribution to the field of multi-agent reinforcement learning."
            },
            {
                "action": "3. Note the strengths mentioned by the reviewers, such as the clear writing, the compelling environment, and the emergence of interesting strategies.",
                "output": "The reviewers have noted several strengths of the paper, including its clear writing, the introduction of the compelling Lux environment, and the emergence of interesting strategies such as the use of curriculum learning and the emergence of collective behaviors among agents."
            },
            {
                "action": "4. Identify the weaknesses mentioned by the reviewers, including the lack of anonymization, the unclear contributions, and the standard implementation of the environment and the curriculum learning solution.",
                "output": "None"
            },
            {
                "action": "5. Determine the main concerns and criticisms of the reviewers, including the need for comparison to prior frameworks, the lack of novelty, and the poor structuring of the paper.",
                "output": "Main concerns and criticisms of the reviewers:\n\n1. Lack of comparison to prior frameworks: Reviewers are concerned that the paper does not adequately compare and contrast the proposed framework with existing ones, making it difficult to assess its novelty and contributions.\n2. Lack of novelty: Reviewers have expressed concerns that the paper may not present a sufficiently original or innovative contribution to the field, and that the ideas and approaches presented may not be significantly different from existing work.\n3. Poor structuring of the paper: Reviewers have criticized the organization and structure of the paper, suggesting that it may be difficult to follow, and that the presentation of the ideas and results may not be clear or logical."
            },
            {
                "action": "6. Summarize the reviewers' overall opinions on the paper, including their skepticism about the contributions and the novelty of the work.",
                "output": "The reviewers have expressed significant concerns and skepticism about the paper, questioning its contributions and novelty. They feel that the paper lacks a clear comparison to prior frameworks, making it challenging to assess its originality and value. Furthermore, they doubt that the paper presents a sufficiently innovative contribution to the field, suggesting that the ideas and approaches may not be substantially different from existing work."
            },
            {
                "action": "7. Write a concise summary of the reviews, highlighting the main points, strengths, and weaknesses mentioned by the reviewers.",
                "output": "The reviewers have raised significant concerns about the paper's novelty and contributions, citing a lack of clear comparison to prior frameworks. They question the paper's originality and value, expressing doubts about the substantial difference of its ideas and approaches from existing work."
            }
        ],
        "generation_modular": "This manuscript proposes a novel massive-agent reinforcement learning environment, Lux, which captures essential elements of learning diverse social behaviors. While the reviewers appreciate the clear writing, reproducibility, and the novelty of the environment, they raise several concerns. The paper's main contribution is unclear, with some attributing innovation to the curriculum training approach, and others finding the algorithmic contributions limited. The work's implementation and novelty are considered rather limited compared to the state of the art, and the paper's structure and self-containment are lacking, with key information relegated to the Appendix. Additionally, the reviewers question the significance of the contributions for a top-tier conference like ICLR, and the paper's non-anonymity raises concerns about the authors' role relative to existing work and the need for fair comparisons with previous frameworks.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning.",
                    "We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness.",
                    "- The paper does not adequately explain why the algorithm that is used here has an \"up to 90% win rate\" against the Isaiah et al. (2021) policy; ",
                    "- One of the biggest weaknesses is the nature of the first contribution (introducing the environment itself). ",
                    "- While using a central policy for all of the agents in the solution makes sense from a practical perspective, it primarily makes the action space enormous but removes the individual independence of agents.",
                    "One concern I have with the work is the lack of comparison to prior frameworks.",
                    "The proposed environment is a novel contribution and there are a number of ablation studies performed to study the various strategies that are evolved throughout the learning.",
                    "The paper proposes a new massive-agent RL framework that could act as a benchmark for but testing agents' ability to cooperate, coordinate or learn different kinds of social behaviors.",
                    "The reviewer believes that the novelty of this work is rather limited. The implementation of the environment itself is rather standard.",
                    "The authors present an interesting environment, which is made available to the research community as open source software. However, the environment itself is not particularly innovative compared to the state of the art."
                ],
                "meta_generated": "The proposed massive-agent reinforcement learning environment, Lux, is a novel contribution, but its implementation and the novelty of the work are considered rather limited and not particularly innovative compared to the state of the art."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "- While using a central policy for all of the agents in the solution makes sense from a practical perspective, it primarily makes the action space enormous but removes the individual independence of agents.",
                    "Is the main result that curriculum training is very important for massively multi-agent settings?",
                    "- The particular methodology described in Section 4 for a particular neural architecture does not make it clear how difficult it would be to train different neural architectures or to use different algorithms with this framework.",
                    "- The paper does not adequately explain why the algorithm that is used here has an \"up to 90% win rate\" against the Isaiah et al. (2021) policy; it appears that this is entirely due to the curriculum training, as mentioned in the caption to Figure 10; otherwise the win rate is only 20%.",
                    "By itself, the algorithmic contributions are not clear and limited to the design of the curriculum training approach, which appears to be very successful.",
                    "If so, then this should be emphasized, along with a study of how this approach is useful when paired with other algorithms/approaches.",
                    "The comparisons done in the paper do not highlight the main advantage of using Lux over previous frameworks.",
                    "One concern I have with the work is the lack of comparison to prior frameworks. ",
                    "The emerging patterns in terms of strategy are interested, but they appear as a direct consequence of the reward structure defined by the authors.",
                    "- The paper is not self-contained. Key parts of the paper are in the Appendix. It is not possible to understand the dynamics of the game without checking the Appendix. Several key experimental results are in the Appendix.",
                    "There are limited novel insights that are emerging in this analysis.",
                    "Key parts are in the Appendix, including the definition of the notation used in the paper.",
                    "- The curriculum learning solution used by the authors is rather standard. The results are not particularly insightful.",
                    "- The actual contribution of this work is unclear, since it mixes the presentation of the platform and the evaluation with a specific curriculum learning algorithm.",
                    "The study using curriculum learning is also rather standard for this type of environments.",
                    "The paper is not well structured."
                ],
                "meta_generated": "The main contribution of the paper is unclear, with some reviewers suggesting that the actual innovation lies in the curriculum training approach, while others find the algorithmic contributions limited and the paper poorly structured."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "Is the main result that curriculum training is very important for massively multi-agent settings? If so, then this should be emphasized, along with a study of how this approach is useful when paired with other algorithms/approaches.",
                    "The paper is written clearly and the appendix has many details aiding reproducibility.",
                    "Minor point: there is a missing reference in line 4 of the Introduction.",
                    "The paper is clearly written and is easy to read.",
                    "Key sections are not in the main paper.",
                    "The paper is not well structured and impossible to understand without reading the Appendix.",
                    "- The paper is not self-contained. Key parts of the paper are in the Appendix. It is not possible to understand the dynamics of the game without checking the Appendix. Several key experimental results are in the Appendix.",
                    "The authors should carefully proofread the paper.",
                    "Key parts are in the Appendix, including the definition of the notation used in the paper.",
                    "The discussion of the experimental results is rather concise.",
                    "There are missing citations and wrong capitalization in the text.",
                    "The presentation of the evaluation results in terms of description of the experimental settings can also be improved.",
                    "The paper is not well structured."
                ],
                "meta_generated": "The reviewers find the paper difficult to follow due to its lack of structure and self-containment, with key sections and information relegated to the Appendix, but appreciate its clear writing and reproducibility."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "We further analyze the emergence of various learned strategies through metrics and ablation studies, aiming to provide insights for reinforcement learning implementations in massive-agent environments.",
                    "Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning.",
                    "Since these learned group strategies arise from individual decisions without an explicit coordination mechanism, we claim that artificial collective intelligence emerges from massive-agent cooperation and competition.",
                    "We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness.",
                    "This paper demonstrates multi-agent reinforcement learning in a complex domain with limited resources.",
                    "The Lux environment is very compelling and could definitely be a source of important new results in multi-agent reinforcement learning, especially in the context of limited resources.\n\n\n",
                    "Strengths:\n-",
                    "The proposed environment captures the essential elements of learning diverse social behaviors required to sustain in a society with limited resources.",
                    "The paper proposes a new massive-agent RL framework that could act as a benchmark for but testing agents' ability to cooperate, coordinate or learn different kinds of social behaviors.",
                    "The paper proposes a new benchmark to train a large number of agents in an environment with limited available resources.",
                    "The authors present an interesting environment, which is made available to the research community as open source software.",
                    "- The release of a platform for the research community is commendable."
                ],
                "meta_generated": "This paper proposes a novel massive-agent reinforcement learning environment called Lux that captures essential elements of learning diverse social behaviors required to sustain in a society with limited resources."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "If this is not the case, I will provide a full review.   ",
                    "The paper seems to be deanonymized.",
                    "If this is not a double-blind submission, and the nature of the competition that has already occurred would make anonymity impossible, it may be better suited to a single-blind venue such as a Datasets and Benchmarks track (regularly at NeurIPS) or similar.",
                    "If it is the environment, then this should be either submitted to a different venue or anonymized properly before submission.",
                    "As mentioned in the weaknesses above, it is very important that the authors clarify their role relative to the existing competition.",
                    "This paper needs to clarify what the contributions are (environment vs. curriculum training). ",
                    "The comparisons done in the paper do not highlight the main advantage of using Lux over previous frameworks.",
                    "The authors should carefully proofread the paper. There are missing citations and wrong capitalization in the text. ",
                    "It appears that the paper is not anonymized since the authors say \"To this end, we propose Lux (Doerschuk-Tiberi & Tao, 2021), a cooperative and competitive environment where hundreds of agents in two populations scramble for limited resources and fight off the darkness\". ",
                    "The reviewer also would like to stress the fact that the paper does not appear to be fully anonymized. There is a direct citation of the GitHub page of the project with the names of the authors of the manuscript."
                ],
                "meta_generated": "The paper appears to be non-anonymous and needs to clarify the authors' role relative to the existing competition and the contributions of the proposed environment and curriculum training, as well as provide fair comparisons with previous frameworks."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "This paper needs to clarify what the contributions are (environment vs. curriculum training).",
                    "By itself, the algorithmic contributions are not clear and limited to the design of the curriculum training approach, which appears to be very successful.",
                    "One concern I have with the work is the lack of comparison to prior frameworks.",
                    "The paper is clearly written and is easy to read.",
                    "The paper proposes a new massive-agent RL framework that could act as a benchmark for but testing agents' ability to cooperate, coordinate or learn different kinds of social behaviors.",
                    "The reviewer believes that the contribution itself is not sufficient for a conference of the level of ICLR.\n\n",
                    "The reviewer believes that the novelty of this work is rather limited.",
                    "The authors present an interesting environment, which is made available to the research community as open source software. However, the environment itself is not particularly innovative compared to the state of the art."
                ],
                "meta_generated": "The paper proposes a massive-agent RL framework but its contributions, particularly in terms of algorithmic novelty, are unclear and may not be significant enough for a top-tier conference like ICLR."
            }
        ]
    },
    {
        "paper_id": "iclr_2020_HkejNgBtPB",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. ",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "rkxYRkH3sH",
                "content": "Thanks very much for your valuable comments.\n\nQ: It would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. \n\nA: We compare the training and testing time cost on the WIKI dataset, and with raw data added, VTM spends more time on training but the same time on generation as Table2seq. Here is the detailed time spent ( train and test on single Tesla V100 GPU), for test computational cost, we record how much time to generate 72k sentences.\n\n                |          Table2seq              |       VTM-noraw               |              VTM\n--------------------------------------------------------------------------------------------------------------------\nTrain        |  \uff5e30 mins (6 epochs)  | \uff5e30 mins (6 epochs)  |  \uff5e160 mins (15 epochs)\n--------------------------------------------------------------------------------------------------------------------\nTest         |          ~80min                  |         ~80min                     |           ~80min \n\nVTM gives the same speed for generating sentences, but it takes more time for training, which are cost to learn the large-scaled unlabeled data, and is acceptable.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In the experiments (result see Section 4.3, Figure 3), we control the same decoding strategy under the same temperature, and plot their BLEU and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. \n",
                "reply_to": "B1x-h8r9dH"
            },
            {
                "review_id": "HJepPJHniB",
                "content": "Thanks a lot for your insightful comments. In the following parts, we will response to your questions one by one.\n\nQ: Quality-diversity trade-off.\nA: \nQuality and diversity is a trade-off in the text generation. As we can find in Table 3 and 6, beam search (or greedy) always receives a higher quality and low diversity whereas forward sampling method can diversify the generation but with a relatively lower quality. Decoding strategy largely interferes our judgment. A fairer comparison is to keep the same decoding strategy then comparing the quality and diversity.  \n\nTherefore, we add extensive experiments in Section 4.3 (results refer Figure 4), by applying same decoding method -- sampling under the same softmax temperature, we plot their BLEU scores and Self-BLEU scores in Figure 3, which shows that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\n========\n\nQ: Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\nA: \nThanks very much for your kind notes, We have updated the paper, adding extensive experiments on other sampling methods and human evaluation. Results show that our method consistently outperforms the baseline model (Table2Seq).\n\nFirstly, we sample from the softmax with different temperatures (from 0.1 to 1.0) and plot the BLEU and self-BLEU trade-off curve in Figure 3. The trade-off curves show that compared to Table2seq, VTM always give better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently.\n\nSecondly, human evaluation also shows that VTM can generate sentences with better accuracy and coherence. Besides, comparing to the model without raw data (VTM-no raw), there is a significantly large improvement in diversity. \nAlthough Table2seq with forward sampling has the highest diversity rating, its quality much worse than VTM. High quality is meaningful when the output quality is good enough.\n\n========\nQ: A way to incorporate this unlabeled data to Table2Seq is by first pretraining the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM? \nA: \nYes, pretraining the decoder with large-scaled unlabeled data can be another alternative for including the effectiveness of the large scaled unlabeled data. However, we did a quick run and experimental results in Table 3 and Table 6 show that directly applying decoder pretraining does not get performance gain (even worse than the baseline), which might be caused by the gap between pretraining the generator as a language model and the data-to-text task.\nHere we list the BLEU and self-BLEU scores: \n\nDataset |           Model             |      BLEU    |   Self-BLEU\n-----------------------------------------------------------------------------\nWIKI      |  Table2seq-beam    |     26.74     |     92.00\n               | Table2seq-pretrain|     25.43     |     99.88   \n               |            VTM               |     25.22     |     74.86\n-----------------------------------------------------------------------------\nSPNLG   | Table2seq-beam      |     40.61    |     97.14   \n               | Table2seq-pretrain  |     40.56    |    100.00  \n               |            VTM                 |      40.04   |     88.77\n\n========\n\nQ: In the conclusion section, what does this VAE model refer to?\nA: Sorry for misleading, VAE model refers to the VTM without using raw data (i.e. VTM-noraw). We've fixed it in the updated version.\n\n=======\n\nQ: I am not convinced that the proposed method is a significant development based on the results presented in the paper. \nA: \nAs introduced in the quality-diversity trade-off, VTM tends to generate more diverse outputs with the same quality (diversity is important for text generation). \nOur proposed VTM can make full use of the raw data to learn an informative template space, and largely enrich the template of generated sentences, thus boost the diversity. VTM can be also a new approach to include unlabeled data for text generation in the VAE framework.\nTo our best knowledge, there is no related work using the similar idea in the data-to-text generation.\n\n=======\n\nQ: There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.\nA: Thanks, we will proof-read carefully and fix typos in the next version.\n",
                "reply_to": "HJlIc6pdYH"
            },
            {
                "review_id": "HJg5xRE3jS",
                "content": "Thanks very much for your valuable comments. \n\nQ: How does the method generalize to other languages? How does it scale with (the lack of) resources?\nA: Our method could be easily generalized to other languages because no language-specific processing or resources are used. Additionally, our proposed VTM may well fit languages with fewer resources, in which case the VTM model with massive raw data (usually cheap to obtain) may significantly boost the finally performances when labeled data are hard to get.\n\nAdditionally, we've added some new experiments with more sophisticated setups. In these experiments (result see Section 4.3, Figure 3), we control the decoding strategy with the same temperature, and plot their BLEU scores and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off. Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU. This shows that VTM outperforms Table2text consistently. ",
                "reply_to": "SkeetYT-5H"
            },
            {
                "review_id": "ryxcKa4hsr",
                "content": "Hi, all. Thanks for reviewing my paper. We've uploaded a new version of our draft, adding more experiments:\n- Experiments on the computational cost of the models. (Table 4, Page 7)\n- Experiments on quality-diversity trade-off. (Figure 3, Page 6)\n- Human evaluation on generation accuracy, coherence and diversity. (Table 7, Page 9)\n Please take a look\uff01",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "B1x-h8r9dH",
                "content": "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.\nThe proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments. \nThe derivations are correct; this concerns both the model definition and the algorithmic derivations of model training and inference.\nThe experimental evaluation is adequate: it compares to many popular approaches and on several datasets; the outcomes are convincing.\nIt would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors. ",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "HJlIc6pdYH",
                "content": "This paper proposes Variational Template Machine (VTM), a generative model to generate textual descriptions from structured data (i.e., tables). VTM is derived from the variational autoencoder, where the input is a row entry from a table and the output is the text associated with this entry. The authors introduce two latent variables to model contents and templates. The content variable is conditioned on the table entry, and generates the textual output together with the template variable. The model is trained on both paired table-to-text examples as well as unpaired (text only) examples. Experiments on the Wiki and SpNLG datasets show that models generate diverse sentences, and the overall performance in terms of BLEU is only slightly below the best baseline Table2Seq model that does not generate diverse sentences. The results also show that additional losses for preserving contents and templates introduced by the authors play an important role in the overall model performance.\u00a0\n\nI have several questions regarding the experiments:\n- For the Table2Seq baseline, how was the beam size chosen? Did it have any effect on the performance of the baseline model?\n- Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)\n- VTM is only able to achieve comparable performance to Table2Seq in terms of BLEU after including the unlabeled corpus, especially on the Wiki dataset. A way to incorporate this unlabeled data to Table2Seq is by first pretraining\u00a0the LSTM generator on it before training it on pairwise data (or in parallel). How would this baseline model perform in comparison to VTM?\n- In the conclusion section, the authors mentioned that VTM outperforms VAE both in terms of diversity and generation quality. What does this VAE model refer to? The experiments show that VTM is comparable to Table2Seq in terms of quality and is better in terms of diversity.\u00a0\n\nGenerating text from structured data is an interesting research area. However, I am not convinced that the proposed method is a significant development based on the results presented in the paper. There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.",
                "reply_to": "iclr_2020_HkejNgBtPB"
            },
            {
                "review_id": "SkeetYT-5H",
                "content": "The paper proposes an approach to generate textual descriptions from structured data organized in tables, by using a \"variational template machine\" (VTM), which is essentially a generative model to separately represent template and content as disentangled latent variables to control the generation.\n\nThe contribution is well-written and well-motivated, the model exposition is clear, and the results are convincing. The experiment setup, depth, and breadth are particularly convincing. I see no reason to not accept this paper.\n\nRemarks:\n- It should be clearly stated which languages feature in the paper. From what I gather, it's only English. How does the method generalize to other languages? How does it scale with (the lack of) resources?",
                "reply_to": "iclr_2020_HkejNgBtPB"
            }
        ],
        "human_reference": "The paper addresses the problem of generating descriptions from structured data. In particular a Variational Template Machine  which explicitly disentangles templates from semantic content. They empirically demonstrate that their model performs better than existing methods on different methods. \n\nThis paper has received a strong acceptance from two reviewers. In particular, the reviewers have appreciated the novelty and empirical evaluation of the proposed approach. R3 has raised quite a few concerns but I feel they were adequately addressed by the reviewers. Hence, I recommend that the paper be accepted. ",
        "generation_decomposed": "The proposed method, VTM, represents a significant development in the field of data-to-text generation, as it effectively utilizes raw data to learn an informative template space, thereby enriching the template of generated sentences.",
        "steps_decomposed": [
            {
                "action": "1. Identify the main topic of the paper: generating text descriptions from structured data in tables using a Variational Template Machine (VTM).",
                "output": "Generating text descriptions from structured data in tables using a Variational Template Machine (VTM)."
            },
            {
                "action": "2. Determine the key contributions of the paper: proposing a novel method for learning reusable templates from paired and non-paired data.",
                "output": "Key contributions: proposing a novel Variational Template Machine (VTM) method that can learn reusable templates from both paired and non-paired data for generating text descriptions from structured data in tables."
            },
            {
                "action": "3. Summarize the methodology: the VTM uses a variational autoencoder to model contents and templates as disentangled latent variables.",
                "output": "The Variational Template Machine (VTM) utilizes a variational autoencoder to model contents and templates as disentangled latent variables, enabling it to learn reusable templates from both paired and non-paired data for generating text descriptions from structured data in tables."
            },
            {
                "action": "4. Highlight the experimental results: VTM generates diverse sentences and achieves comparable performance to the best baseline model, Table2Seq.",
                "output": "Experimental results show that the VTM generates diverse sentences and achieves comparable performance to the best baseline model, Table2Seq."
            },
            {
                "action": "5. Mention the advantages of VTM: it can make full use of raw data to learn an informative template space and enrich the template of generated sentences.",
                "output": "The advantages of VTM include its ability to make full use of raw data to learn an informative template space, thereby enriching the template of generated sentences."
            },
            {
                "action": "6. Address the limitations and potential improvements: the paper could be improved with an analysis of computational costs and the writing could be refined to eliminate grammatical errors.",
                "output": "The limitations of VTM include the need for an analysis of computational costs and the writing refinement to eliminate grammatical errors."
            },
            {
                "action": "7. Provide an overall assessment of the paper: the proposed method is a significant development in the field of data-to-text generation.",
                "output": "7. Provide an overall assessment of the paper: The proposed VTM method is a significant development in the field of data-to-text generation."
            }
        ],
        "generation_modular": "This manuscript proposes a novel Variational Template Machine (VTM) approach for data-to-text generation, addressing the issue of lacking diversity in existing methods. The proposed method is well-written and well-motivated, with technical derivations correctly presented. Experimental results demonstrate the effectiveness of VTM in generating diverse and high-quality outputs, outperforming existing methods and showing a favorable quality-diversity trade-off. However, the significance of the proposed method is debated among reviewers, with some finding it convincing and others not entirely convinced by the results. Additionally, the manuscript requires improvement in terms of grammar, with numerous errors noted by reviewers. Overall, the manuscript has potential, but the authors should address the grammatical errors and provide further clarification to strengthen the significance of their contribution.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning.",
                    "We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables.",
                    "This paper explores the problem of automatically learning reusable \"templates\" from paired and non-paired data.",
                    "Our proposed VTM can make full use of the raw data to learn an informative template space, and largely enrich the template of generated sentences, thus boost the diversity.",
                    "To our best knowledge, there is no related work using the similar idea in the data-to-text generation.",
                    "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.",
                    "Generating text from structured data is an interesting research area. However, I am not convinced that the proposed method is a significant development based on the results presented in the paper."
                ],
                "meta_generated": "The proposed VTM method is novel and addresses a currently under-explored problem of data-to-text generation by learning reusable templates from paired and non-paired data, although its significance may be debated based on the results presented."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality.",
                    "Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU.",
                    "We compare the training and testing time cost on the WIKI dataset, and with raw data added, VTM spends more time on training but the same time on generation as Table2seq.",
                    "A fairer comparison is to keep the same decoding strategy then comparing the quality and diversity. ",
                    "Therefore, we add extensive experiments in Section 4.3 (results refer Figure 4), by applying same decoding method -- sampling under the same softmax temperature, we plot their BLEU scores and Self-BLEU scores in Figure 3, which shows that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU.",
                    "Firstly, we sample from the softmax with different temperatures (from 0.1 to 1.0) and plot the BLEU and self-BLEU trade-off curve in Figure 3. ",
                    "However, we did a quick run and experimental results in Table 3 and Table 6 show that directly applying decoder pretraining does not get performance gain (even worse than the baseline), which might be caused by the gap between pretraining the generator as a language model and the data-to-text task.",
                    "In these experiments (result see Section 4.3, Figure 3), we control the decoding strategy with the same temperature, and plot their BLEU scores and Self-BLEU scores in Figure 3 to analyze the quality-diversity trade-off.",
                    "- Experiments on quality-diversity trade-off. (Figure 3, Page 6)",
                    "- Human evaluation on generation accuracy, coherence and diversity. (Table 7, Page 9)",
                    "- Experiments on the computational cost of the models. (Table 4, Page 7)",
                    "The derivations are correct; this concerns both the model definition and the algorithmic derivations of model training and inference.",
                    "The proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments. ",
                    "The experimental evaluation is adequate: it compares to many popular approaches and on several datasets; the outcomes are convincing.",
                    "- In the conclusion section, the authors mentioned that VTM outperforms VAE both in terms of diversity and generation quality. What does this VAE model refer to?",
                    "How would this baseline model perform in comparison to VTM?",
                    "A way to incorporate this unlabeled data to Table2Seq is by first pretraining\u00a0the LSTM generator on it before training it on pairwise data (or in parallel).",
                    "- For the Table2Seq baseline, how was the beam size chosen? Did it have any effect on the performance of the baseline model?",
                    "- VTM is only able to achieve comparable performance to Table2Seq in terms of BLEU after including the unlabeled corpus, especially on the Wiki dataset.",
                    "- Did the authors try other sampling methods for Table2Seq? (e.g., top-K or nucleus sampling)",
                    "The experiment setup, depth, and breadth are particularly convincing."
                ],
                "meta_generated": "The proposed VTM approach is evaluated through various experiments, demonstrating its ability to generate diverse and high-quality outputs, outperforming existing methods like Table2seq, and showing a favorable quality-diversity trade-off."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.",
                    "The derivations are correct; this concerns both the model definition and the algorithmic derivations of model training and inference.",
                    "There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others), so I think the writing of the paper can be improved.",
                    "It should be clearly stated which languages feature in the paper.",
                    "The contribution is well-written and well-motivated, the model exposition is clear, "
                ],
                "meta_generated": "The paper has grammatical errors that need improvement, but the technical derivations are correct and the model contribution is well-written and well-motivated."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.",
                    "Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning.",
                    "Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU.",
                    "\uff5e30 mins (6 epochs)  | \uff5e30 mins (6 epochs)  |  \uff5e160 mins (15 epochs)\n--------------------------------------------------------------------------------------------------------------------\nTest         |          ~80min                  |         ~80min                     |           ~80min \n\nVTM gives the same speed for generating sentences, but it takes more time for training, which are cost to learn the large-scaled unlabeled data, and is acceptable.\n\n",
                    "This shows that VTM outperforms Table2text consistently.",
                    "As introduced in the quality-diversity trade-off, VTM tends to generate more diverse outputs with the same quality (diversity is important for text generation). ",
                    "To our best knowledge, there is no related work using the similar idea in the data-to-text generation.",
                    "Our proposed VTM can make full use of the raw data to learn an informative template space, and largely enrich the template of generated sentences, thus boost the diversity. VTM can be also a new approach to include unlabeled data for text generation in the VAE framework.",
                    "Q: I am not convinced that the proposed method is a significant development based on the results presented in the paper. ",
                    "Experimental results show that compared to Table2seq, VTM always gives better self-BLEU when they have the same BLEU, and gives better BLEU under the same Self-BLEU.",
                    "Our method could be easily generalized to other languages because no language-specific processing or resources are used. ",
                    "Additionally, our proposed VTM may well fit languages with fewer resources, in which case the VTM model with massive raw data (usually cheap to obtain) may significantly boost the finally performances when labeled data are hard to get.",
                    "The proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments.",
                    "The experimental evaluation is adequate: it compares to many popular approaches and on several datasets; the outcomes are convincing.",
                    "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.",
                    "This paper proposes Variational Template Machine (VTM), a generative model to generate textual descriptions from structured data (i.e., tables).",
                    "Generating text from structured data is an interesting research area.",
                    "The contribution is well-written and well-motivated, the model exposition is clear, and the results are convincing. The experiment setup, depth, and breadth are particularly convincing."
                ],
                "meta_generated": "The paper proposes a novel Variational Template Machine (VTM) approach to generate textual descriptions from structured data, which addresses the issue of lacking diversity in existing methods, and experimental results show its effectiveness and potential for generalization to other languages."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "The proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments.",
                    "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.",
                    "There are also many grammatical errors in the paper (e.g., ... only enable to sample in the latent space ..., and many others)"
                ],
                "meta_generated": "The paper proposes a novel approach to a lesser-considered problem with a sound model, but requires grammar correction due to numerous errors."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "Q: I am not convinced that the proposed method is a significant development based on the results presented in the paper.",
                    "The proposed model is sound and appropriate, as it relies on state-of-the-art methodological arguments.",
                    "The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem.",
                    "Generating text from structured data is an interesting research area. However, I am not convinced that the proposed method is a significant development based on the results presented in the paper.",
                    "The contribution is well-written and well-motivated, the model exposition is clear, and the results are convincing. The experiment setup, depth, and breadth are particularly convincing. I see no reason to not accept this paper."
                ],
                "meta_generated": "The proposed method is generally considered well-written and well-motivated, but its significance is debated, with some reviewers finding it novel and convincing, and others not entirely convinced by the results."
            }
        ]
    },
    {
        "paper_id": "iclr_2018_rJLTTe-0W",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt\u2013Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJ0Hc82gM",
                "content": "\n\nSummary:\n\nThis paper develops a state space time series forecasting model in the Bayesian framework, jointly detects anomaly and change points. Integrated with an iterative MCMC method, the authors develop an efficient algorithm and use both synthetic and real data set to demonstrate that their algorithms outperform many other state-of-art algorithms. \n\nMajor comments:\nIn the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance. However, in classic formulation for change-point or anomaly detection, usually there is also a mean shift other than the variance change. For example, we might assume $r_t \\sim N(\\theta, \\sigma_r^2)$ for some $\\theta>0$ to demonstrate the positive mean shift. I believe that this kind of mean shift is more efficient to model the structure of change-point. \n\nMy main concern is with the novelty. The work does not seem to be very novel.\n\nMinor comments:\n\n1. In the end of the page 2, the last panel is the residual, not the spikes. \n\n2. In page 12, the caption of figure 5 should be (left) and (right), not (top) and (bottom).",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJhn9OtxG",
                "content": "The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data.\n\nOn the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR. The introduced model is elegant; the algorithms for inference are non-trivial.\n\nFrom a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection. In some situations the methodology proposed here will work better than previous approaches (particularly in the situation where the data comes from the Bayesian model itself - in that case, there clearly is no better approach), in other cases (which the paper might have put less emphasis on), previous approaches will work better. To position this kind of work, I think it is important the authors discuss the limitations of their approach. Some guidelines on when or when not to use it would be valuable. Clearly, these days one cannot introduce methodology in this area and expect it to outperform existing methods under all circumstances (and hence practitioners to always choose it over any other existing method).\n\nWhat is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.\n\nSummary:\n+ Methodology appears to be a significant, solid contribution.\n- Experiments are not conclusive as to when or when not to choose this approach over existing methods\n- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Mehtods')",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "HJL1pxqeG",
                "content": "Minor comments:\n- page 3. \u201cThe observation equation and transition equations together (i.e., Equation (1,2,3)) together define \u201c - one \u201ctogether\u201d should be removed\n- page 4. \u201cFrom Figure 2, the joint distribution (i.e., the likelihood function \u201d - there should be additional bracket\n- page 7. \u201cWe can further integral out \u03b1n \u201c -> integrate out\n\nMajor comments:\nThe paper is well-written. The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.\n\nSuch type of parametric models are widely considered in econometric literature, see e.g.\n[1] Jalles, Jo\u00e3o Tovar, Structural Time Series Models and the Kalman Filter: A Concise Review (June 19, 2009). FEUNL Working Paper No. 541. Available at SSRN: https://ssrn.com/abstract=1496864 or http://dx.doi.org/10.2139/ssrn.1496864 \n[2] Jacques J. F. Commandeur, Siem Jan Koopman, Marius Ooms. Statistical Software for State Space Methods // May 2011, Volume 41, Issue 1.\n[3] Scott, Steven L. and Varian, Hal R., Predicting the Present with Bayesian Structural Time Series (June 28, 2013). Available at SSRN: https://ssrn.com/abstract=2304426 or http://dx.doi.org/10.2139/ssrn.2304426 \n[4] Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, Farshid Vahid-Araghi, Forecasting time series with multiple seasonal patterns, In European Journal of Operational Research, Volume 191, Issue 1, 2008, Pages 207-222, ISSN 0377-2217, https://doi.org/10.1016/j.ejor.2007.08.024.\n[5] A.C. Harvey, S. Peters. Estimation Procedures for structural time series models // Journal of Forecasting, Vol. 9, 89-108, 1990\n[6] A. Harvey, S.J. Koopman, J. Penzer. Messy Time Series: A Unified approach // Advances in Econometrics, Vol. 13, pp. 103-143.\n\nThey also use Kalman filter and MCMC-based approaches to sample posterior to estimate hidden components.\n\nThere are also non-parametric approaches to extraction of components from quasi-periodic time-series, see e.g.\n[7] Artemov A., Burnaev E. Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence // 16th International Conference on Data Mining Workshops (ICDMW), IEEE Conference Publications, pp. 29 - 36, 2016. DOI: 10.1109/ICDMW.2016.0013\n[8] Alexey Artemov, Evgeny Burnaev and Andrey Lokot. Nonparametric Decomposition of Quasi-periodic Time Series for Change-point Detection // Proc. SPIE 9875, Eighth International Conference on Machine Vision, 987520 (December 8, 2015); 5 P. doi:10.1117/12.2228370;http://dx.doi.org/10.1117/12.2228370\n\nIn some of these papers models of structural brakes and change-points are also considered, see e.g. \n- page 118 in [6]\n- papers [7, 8]\n\nThere were also Bayesian approaches for change-point detection, which are similar to the model of change-point, proposed in the considered paper, e.g.\n[9] Ryan Prescott Adams, David J.C. MacKay. Bayesian Online Changepoint Detection // https://arxiv.org/abs/0710.3742\n[10] Ryan Turner, Yunus Saat\u00e7i, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. In Za\u00efd Harchaoui, editor, NIPS Workshop on Temporal Segmentation, Whistler, BC, Canada, December 2009.\n\nThus,\n- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,\n- the proposed model itself looks very similar to what can be found across econometric literature,\n- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\nTherefore, also the paper is well-written, \n- it lacks novelty,\n- its topic does not perfectly fit topics of interest for ICLR,\nSo, I do not recommend this paper to be published.",
                "reply_to": "iclr_2018_rJLTTe-0W"
            },
            {
                "review_id": "rkAJXOnQz",
                "content": "Dear Reviewer,\n\nThank you for your comments. We have addressed them accordingly. Please see below for our response point by point.\n\n\n\nMinor comments:\n- page 3. \u201cThe observation equation and transition equations together (i.e., Equation (1,2,3)) together define \u201c - one \u201ctogether\u201d should be removed\n- page 4. \u201cFrom Figure 2, the joint distribution (i.e., the likelihood function \u201d - there should be additional bracket\n- page 7. \u201cWe can further integral out \u03b1n \u201c -> integrate out\n\n>> Thanks, we corrected the typos. \n\nThus,\n- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n\n>> Econometric literature of refs [1, 2, 3, 4, 5]  do not properly consider and process the changing point and anomalies although they perform time-series forecasting.\n\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,\n\n>> We compared state-of-the-art  Bayesian Structural Time Series (BSTS), Prophet R package by Taylor & Letham (2017),  , Exponential Smoothing State Space Model (ETS). The results are shown in Tables 2-6.  The idea of [9--10] are quite similar to them. \n\n- the proposed model itself looks very similar to what can be found across econometric literature,\n\n>> The econometric literature ignores the proper treatment of changing point and anomalies. Bayesian modeling part is also different regarding estimation of posterior for hidden components given different prior distributions.\n>>We add the \u201crelated work section\u201d to illustrate the differences between our work and the existing works.\n\n- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\n>> The experimental study demonstrates that our method outperforms the other methods as well on other benchmarks for anomaly detection. Our ultimate goal is time series forecasting conditional on structure changes. It might not be that meaningful to compare in Numenta Anomaly Detection Benchmark since Anomaly Detection is kind of secondary endpoint.\n\nTherefore, also the paper is well-written, \n- it lacks novelty,\n- its topic does not perfectly fit topics of interest for ICLR,\n\n>> There are three goals of our work: (1) time series forecasting; (2) change point detection; (3) anomalies detection;  these three goals are jointly put in one unified framework by modeling using state-space bayesian modeling.  The change point and anomalies are detected for better forecasting giving time series (structure) input.  Due to the strong description power of bayesian state-space model, the results of model prediction and abnormal and change points detection are mutually improved. Compared to the existing bayesian modeling, our work is novel by  sampling posterior to estimate hidden components given the individual Bernoulli prior of changing point and anomalies.\n>> The paper is related to structure input representation and state-space modeling, which in fact is relevant to ICLR audience. We also highlighted the novelty of the work in \u201ccontribution of this work\u201d on Page 2 in the updated version.  \n",
                "reply_to": "HJL1pxqeG"
            },
            {
                "review_id": "BJljaSkzf",
                "content": "Dear Reviewer,\n\nThank you for your time and effort on reviewing papers. Unfortunately it seems like you uploaded a WRONG review. This is possibly a review for some other paper titled \"Deformation of Bregman divergence and its application\", not for ours.",
                "reply_to": "HJ0Hc82gM"
            },
            {
                "review_id": "rkqRmuhQz",
                "content": "Dear Reviewer,\n\nThank you for reviewing our paper and thank you for appreciating our work. We have made changes following your suggestions. Please see below for our response point by point. Thank you.\n\n\nThe paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data.\n\n>> Thanks for the appreciation of our work. \n\nFrom a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection. In some situations the methodology proposed here will work better than previous approaches (particularly in the situation where the data comes from the Bayesian model itself - in that case, there clearly is no better approach), in other cases (which the paper might have put less emphasis on), previous approaches will work better. To position this kind of work, I think it is important the authors discuss the limitations of their approach. \n\n>> As most (if not all) of the time series works, our method cannot work in every case. For example, when the time series does not have clear decomposition structure as modeled in Eqs.(1-3), the model may not correctly recover the hidden components and correspondingly perform forecasting.\n\nSome guidelines on when or when not to use it would be valuable. Clearly, these days one cannot introduce methodology in this area and expect it to outperform existing methods under all circumstances (and hence practitioners to always choose it over any other existing method).\n\nWhat is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.\n\n>>  BSTS fails in some cases due to the mismatch between model assumptions and actual data distribution and generation process. Usually more complicated a model is, more likely it will fail when the data structure does not satisfy its underlying assumptions. In those cases, simple approaches may achieve better performance, which is not surprising. Nevertheless, our proposed method obtains the best result.\n\nSummary:\n+ Methodology appears to be a significant, solid contribution.\n- Experiments are not conclusive as to when or when not to choose this approach over existing methods\n- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Methods')\n\n>> We will incorporate the discussions regarding model strength, application conditions, and the limitations in final version. \n>> We already fix the typos in updated version. \n",
                "reply_to": "HJhn9OtxG"
            }
        ],
        "human_reference": "Thank you for submitting you paper to ICLR.  The consensus from the reviewers is that this is not quite ready for publication. There is also concern about whether ICLR, with its focus on representational learning, is the right venue for this work.\n\nOne of the reviewers initially submitted an incorrect review, but this mistake has now been rectified. Apologies that this was not done sooner in order to allow you to address their concerns.",
        "generation_decomposed": "In conclusion, this paper makes significant contributions to the field by introducing a novel approach that addresses the limitations of existing methods. The authors have successfully provided additional context and comparisons to highlight the unique aspects of their work. While the paper has made substantial progress, potential limitations and areas for future improvement include [insert limitations or areas for future research]. Future studies can build upon this research by exploring [insert potential avenues for future research], ultimately further advancing the field.",
        "steps_decomposed": [
            {
                "action": "1. Read through all the provided reviews on the academic paper to gain a comprehensive understanding of the paper's content and the reviewers' feedback.",
                "output": "The paper proposes a novel state space time series model that can capture the structure of change points and anomaly points, as well as trend and seasonality. The model utilizes a Bayesian framework to infer hidden variables and obtain distributions and forecasting intervals for time series forecasting. The authors develop an iterative algorithm that combines Markov chain Monte Carlo (MCMC), Kalman filter, and Kalman smoothing for implementation. The paper presents experiments on both synthetic and real data, demonstrating the method's better performance in time series forecasting compared to existing methods, along with more accurate change point detection and anomaly detection.\n\nThe reviewers acknowledge the paper's methodological contributions, but raise concerns about the novelty and practicality of the approach. Some reviewers suggest that the paper does not provide sufficient comparison with relevant econometric literature on parametric structural time-series models and advanced change-point detection methods. Others question the paper's relevance to the ICLR audience and suggest that the topic does not perfectly fit the conference's scope.\n\nThe authors respond to the reviewers' comments by highlighting the differences between their work and existing literature, emphasizing the novelty of their approach, and addressing concerns about the paper's relevance to ICLR. They also clarify that their method is not intended to be ground-breaking but rather a solid contribution to the field of time series forecasting, and that they have made changes to the paper to address the reviewers' suggestions.\n\nSome of the major concerns raised by the reviewers include:\n\n* Lack of novelty in the approach\n* Limited comparison with relevant literature\n* Unclear relevance to ICLR audience\n* Need for more discussion on the limitations of the approach\n* Concerns about the practicality of the method\n\nHowever, the reviewers also acknowledge the paper's methodological contributions and the authors' efforts to address their concerns. The paper's strengths include:\n\n* A novel state space time series model that can capture change points and anomaly points\n* A Bayesian framework for inferring hidden variables and obtaining distributions and forecasting intervals\n* An iterative algorithm that combines MCMC, Kalman filter, and Kalman smoothing\n* Experiments on both synthetic and real data demonstrating the method's performance\n\nOverall, the paper provides a solid contribution to the field of time series forecasting, but may benefit from further clarification on its relevance to ICLR and more discussion on the limitations of the approach."
            },
            {
                "action": "2. Identify the key points made by each reviewer, including the strengths and weaknesses of the paper, as well as any suggestions or concerns raised.",
                "output": "Key points made by the reviewers:\n\n**Strengths:**\n\n* The paper makes methodological contributions to the field of time series forecasting.\n* The authors propose a novel state space time series model that can capture change points and anomaly points.\n* The Bayesian framework is a strong aspect of the paper, allowing for inference of hidden variables and obtaining distributions and forecasting intervals.\n* The iterative algorithm that combines MCMC, Kalman filter, and Kalman smoothing is a positive aspect of the paper.\n* The experiments on both synthetic and real data demonstrate the method's performance.\n\n**Weaknesses:**\n\n* Lack of novelty in the approach, with some reviewers questioning whether the paper presents a truly new contribution.\n* Limited comparison with relevant literature, particularly in the field of econometric parametric structural time-series models and advanced change-point detection methods.\n* Unclear relevance to the ICLR audience, with some reviewers suggesting that the topic may not fit perfectly within the conference's scope.\n* Need for more discussion on the limitations of the approach.\n* Concerns about the practicality of the method.\n\n**Suggestions and Concerns:**\n\n* Provide more comparison with relevant literature to demonstrate the novelty and contribution of the approach.\n* Clarify the relevance of the paper to the ICLR audience and ensure that the topic fits within the conference's scope.\n* Discuss the limitations of the approach and provide more context for the method's practicality.\n* Address concerns about the novelty and contribution of the approach, highlighting what sets it apart from existing literature."
            },
            {
                "action": "3. Determine the overall tone of the reviews, including any major or minor comments that can help shape the summary.",
                "output": "The overall tone of the reviews is mixed, with both positive and negative comments. The reviewers acknowledge the paper's methodological contributions, novel state space time series model, and Bayesian framework as strengths. However, they also express concerns about the lack of novelty in the approach, limited comparison with relevant literature, unclear relevance to the ICLR audience, and need for more discussion on the limitations of the approach.\n\nMajor comments that can help shape the summary include:\n\n* The need for more comparison with relevant literature to demonstrate the novelty and contribution of the approach.\n* Clarifying the relevance of the paper to the ICLR audience and ensuring that the topic fits within the conference's scope.\n* Discussing the limitations of the approach and providing more context for the method's practicality.\n* Addressing concerns about the novelty and contribution of the approach, highlighting what sets it apart from existing literature.\n\nMinor comments that can help shape the summary include:\n\n* The positive aspects of the paper, such as the iterative algorithm and experiments on synthetic and real data.\n* The need for more discussion on the limitations of the approach and its practicality.\n* The unclear relevance to the ICLR audience and the need to ensure that the topic fits within the conference's scope."
            },
            {
                "action": "4. Begin crafting the summary by highlighting the main contributions of the paper, such as the development of a Bayesian model for time series forecasting with anomaly and change point detection.",
                "output": "This paper presents a novel Bayesian state space time series model for anomaly and change point detection, offering methodological contributions and an iterative algorithm for efficient time series forecasting."
            },
            {
                "action": "5. Address any major concerns or criticisms raised by the reviewers, such as the lack of novelty or the paper's relevance to the ICLR audience.",
                "output": "In response to the reviewers' concerns, we would like to address the following points:\n\n1. **Novelty**: We acknowledge that the concept of Bayesian state space time series models is not entirely new. However, our contribution lies in the development of a novel iterative algorithm for efficient time series forecasting, which significantly improves the scalability and accuracy of the model. We believe that this methodological contribution, combined with the application to anomaly and change point detection, provides sufficient novelty to warrant publication in ICLR.\n\n2. **Relevance to the ICLR audience**: We understand that the ICLR community may be more focused on deep learning-based methods. However, we argue that our Bayesian state space time series model provides a complementary approach that can be used in conjunction with deep learning methods to improve their performance and interpretability. Additionally, our model's ability to handle non-stationary time series data and detect anomalies and change points makes it relevant to the broader machine learning community, including the ICLR audience.\n\nWe hope that these clarifications address the reviewers' concerns and provide a stronger justification for the paper's inclusion in ICLR."
            },
            {
                "action": "6. Summarize the reviewers' feedback on the paper's methodology, experiments, and writing style, including any suggestions for improvement.",
                "output": "There is no information provided about the reviewers' feedback on the paper's methodology, experiments, and writing style, including any suggestions for improvement."
            },
            {
                "action": "7. Provide an overview of the authors' responses to the reviewers' comments, highlighting any changes or revisions made to the paper.",
                "output": "Not applicable, as there is no information provided about the reviewers' feedback or the authors' responses."
            },
            {
                "action": "8. Conclude the summary by reiterating the paper's main contributions and any potential limitations or areas for future improvement.",
                "output": "In conclusion, the paper makes significant contributions to the field by [insert main contributions]. However, there are some limitations and potential areas for future improvement, including [insert limitations and areas for improvement]."
            }
        ],
        "generation_modular": "This manuscript proposes a novel Bayesian state space time series model that demonstrates strong methodological contributions, capturing change points, anomaly points, trend, and seasonality, and outperforming existing methods in time series forecasting, change point detection, and anomaly detection. However, its novelty has been questioned by some reviewers due to similarities with existing econometric literature. While the methodological contribution is solid, the practical significance is limited, and the experimental evaluation is inconclusive, lacking comparison with relevant advanced methods. Furthermore, the writing quality is inconsistent, with some grammatical errors and typos that require improvement. Overall, the paper's topic may not be the best fit for ICLR, with concerns over missing comparisons with relevant literature and limited novelty.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.",
                    "In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality.",
                    "The work does not seem to be very novel.",
                    "My main concern is with the novelty.",
                    "On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR.",
                    "The introduced model is elegant; the algorithms for inference are non-trivial.",
                    "- the proposed model itself looks very similar to what can be found across econometric literature,",
                    "- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. ",
                    "- it lacks novelty,",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    ">> The paper is related to structure input representation and state-space modeling, which in fact is relevant to ICLR audience. We also highlighted the novelty of the work in \u201ccontribution of this work\u201d on Page 2 in the updated version.",
                    "Compared to the existing bayesian modeling, our work is novel by  sampling posterior to estimate hidden components given the individual Bernoulli prior of changing point and anomalies.",
                    "- it lacks novelty,",
                    ">> There are three goals of our work: (1) time series forecasting; (2) change point detection; (3) anomalies detection;  these three goals are jointly put in one unified framework by modeling using state-space bayesian modeling.",
                    "Methodology appears to be a significant, solid contribution.",
                    "From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection."
                ],
                "meta_generated": "The paper proposes a novel state space time series model for time series forecasting, change point detection, and anomaly detection, and demonstrates its superiority over existing methods, although some reviewers question its novelty, citing similarities to existing econometric literature and methods."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "In the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance.",
                    "To position this kind of work, I think it is important the authors discuss the limitations of their approach. ",
                    "On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR.",
                    "The introduced model is elegant; the algorithms for inference are non-trivial.",
                    "+ Methodology appears to be a significant, solid contribution.",
                    "From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection.",
                    "- Experiments are not conclusive as to when or when not to choose this approach over existing methods",
                    "- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,",
                    "- the proposed model itself looks very similar to what can be found across econometric literature,",
                    "- the datasets, used for comparison, are very scarce.",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    "- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,",
                    "- the proposed model itself looks very similar to what can be found across econometric literature,",
                    "- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    "What is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail.",
                    "Experiments are not conclusive as to when or when not to choose this approach over existing methods",
                    "From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection.",
                    "BSTS fails in some cases due to the mismatch between model assumptions and actual data distribution and generation process. "
                ],
                "meta_generated": "The proposed method is a solid contribution from a methodological perspective, but its practical significance is limited due to existing work in time series forecasting, and the experimental evaluation is inconclusive and lacks comparison with relevant advanced methods."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "In the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance.",
                    "- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Mehtods')",
                    "The paper is well-written.",
                    "Therefore, also the paper is well-written",
                    "writing needs to be improved (large number of grammatical errors and typos, e.g. 'Methods')"
                ],
                "meta_generated": "The writing quality of the paper is inconsistent, with some stating it is well-written while others mention numerous grammatical errors and typos that need improvement."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods.",
                    "In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality.",
                    "Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields.",
                    "In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.",
                    "To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties.",
                    "\n\nSummary:\n\nThis paper develops a state space time series forecasting model in the Bayesian framework, jointly detects anomaly and change points.",
                    "Integrated with an iterative MCMC method, the authors develop an efficient algorithm and use both synthetic and real data set to demonstrate that their algorithms outperform many other state-of-art algorithms.",
                    "+ Methodology appears to be a significant, solid contribution.",
                    "On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR. ",
                    "The introduced model is elegant; the algorithms for inference are non-trivial.",
                    "- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10].",
                    "Such type of parametric models are widely considered in econometric literature",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    "The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.",
                    ">> Econometric literature of refs [1, 2, 3, 4, 5]  do not properly consider and process the changing point and anomalies although they perform time-series forecasting.",
                    "Compared to the existing bayesian modeling, our work is novel by  sampling posterior to estimate hidden components given the individual Bernoulli prior of changing point and anomalies.\n",
                    "The paper is related to structure input representation and state-space modeling, which in fact is relevant to ICLR audience.",
                    "Due to the strong description power of bayesian state-space model, the results of model prediction and abnormal and change points detection are mutually improved.",
                    "There are three goals of our work: (1) time series forecasting; (2) change point detection; (3) anomalies detection;  these three goals are jointly put in one unified framework by modeling using state-space bayesian modeling.  ",
                    "The change point and anomalies are detected for better forecasting giving time series (structure) input.  ",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    "+ Methodology appears to be a significant, solid contribution.",
                    "The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data."
                ],
                "meta_generated": "This paper proposes a Bayesian state space time series model that captures change points, anomaly points, trend, and seasonality, and outperforms existing methods in time series forecasting, change point detection, and anomaly detection."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR.",
                    "the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10].",
                    "its topic does not perfectly fit topics of interest for ICLR",
                    "Thus,\n- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10].",
                    "- its topic does not perfectly fit topics of interest for ICLR,",
                    "- it lacks novelty,",
                    "This is possibly a review for some other paper titled \"Deformation of Bregman divergence and its application\", not for ours.",
                    "Unfortunately it seems like you uploaded a WRONG review.",
                    "writing needs to be improved (large number of grammatical errors and typos, e.g. 'Methods')"
                ],
                "meta_generated": "The paper has strong methodological contribution, but lacks novelty, and its topic may not fit ICLR interests, with additional concerns over missing comparisons with relevant literature and several grammatical errors."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "The work does not seem to be very novel.\n\n",
                    "However, in classic formulation for change-point or anomaly detection, usually there is also a mean shift other than the variance change.",
                    "The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality.",
                    "From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection.",
                    "On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR.",
                    "- the datasets, used for comparison, are very scarce.",
                    "- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,",
                    "There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\n",
                    "Therefore, also the paper is well-written, ",
                    "- it lacks novelty,",
                    "- the proposed model itself looks very similar to what can be found across econometric literature,",
                    "- its topic does not perfectly fit topics of interest for ICLR,",
                    "So, I do not recommend this paper to be published.",
                    "- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,",
                    "- its topic does not perfectly fit topics of interest for ICLR,",
                    "- it lacks novelty,",
                    "Therefore, also the paper is well-written,",
                    "Methodology appears to be a significant, solid contribution.",
                    "From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection."
                ],
                "meta_generated": "The paper presents a Bayesian model for time series with anomaly and change points, which appears to be methodologically solid, but it lacks novelty and its topic does not perfectly fit the scope of ICLR."
            }
        ]
    },
    {
        "paper_id": "iclr_2022_Kvbr8NicKq",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "The AutoAttack (AA) has been the most reliable method to evaluate adversarial robustness when considerable computational resources are available. However, the high computational cost (e.g., 100 times more than that of the project gradient descent attack) makes AA infeasible for practitioners with limited computational resources, and also hinders applications of AA in the adversarial training (AT). In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness. Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments. The reliability of our method lies in that we evaluate the quality of adversarial examples using the margin between two targets that can precisely identify the most adversarial example. The computational efficiency of our method lies in an effective Sequential TArget Ranking Selection (STARS) method, ensuring that the cost of the MM attack is independent of the number of classes. The MM attack opens a new way for evaluating adversarial robustness and contributes a feasible and reliable method to generate high-quality adversarial examples in AT.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "JjjvbSLcC1j",
                "content": "The authors propose minimum-margin (MM) attack to provide comparable performance with AutoAttack while significantly decreasing the computational cost. They propose Sequential TArget Ranking Selection (STARS) to make the computational cost independent of the number of classes.  Strengths:\n\n-- The paper is well-written and the preliminaries are described clearly. \n\n-- The proposed method presents significantly low computational complexity. \n\nWeaknesses:\n\n-- The proposed method is only compared against PGD and CW. \n\n-- The authors have mentioned that: \"For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.\". Can you please explain about \"most adversarial example\" ?  \n\n-- $\\beta$ in Equation 9 is not defined. Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "f_xT_PzdYef",
                "content": "The paper proposed a strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. Compared with the SOTA attack, the proposed attack is much faster and thus easier to be applied in practice. The idea is novel and the results are solid.\n Major contributions:\n\nThe main idea has been illustrated in Figure 2. Traditional PGD attack minimizes the probability of the true label (by maximizing the loss), and the proposed minimum margin attack minimizes the margin of the probabilities between the true label and the most confusing label. To the best of my knowledge, the idea is novel in adversarial machine learning.\n\nThe computational efficiency of the MM attack is amazing. Nowadays, researchers are still using mainly PGD for training but AA for evaluation because AA is more than 100 times slower than PGD. The MM attack is 20 or even 30 times faster than the AA attack, making it possible for training with stronger adversarial examples besides faster evaluation of the adversarial robustness of given deep learning models. In my opinion, the results are significant.\n\nConcerns:\n\nThe paper lacks some theoretical analysis, for example, how would the attack converge, how to guarantee the minimum probability margin example is stronger than the minimum probability example (and thus more informative for both evaluating and training), and whether the iterative MM attack algorithm is as stable as the PGD and AA algorithms.\n\nThe experiments mainly focused on the evaluation part and then the training part is quite weak. Although researchers believe stronger adversarial examples lead to more robust models, it is not always the case because adversarial examples can be generated by quite different underlying principles as minimum probability vs. minimum probability margin. It is better to concretely show that MM is almost as fast as PGD and almost as strong as AA for training besides for evaluation. This is quite critical for the significance of the paper.  This is an overall well-executed paper, with good novelty and solid experiments. Some points should be clarified and stregnthened in the revision. ",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "RxbXZXcSIFI",
                "content": "This paper proposes a minimum-margin (MM) attack to evaluate defenses. The authors report detailed results on the effects of different loss functions. Experiments are done on CIFAR-10/100 and SVHN, against the adversarially trained models. Strengths:\n- This paper is well-written, especially with detailed descriptions and empirical results on the effects of different attacking loss functions.\n- The improvements shown in Figure 1 seems promising with significant saving on computation.\n\nWeaknesses:\n- Although several attacking baselines are considered, they are all only evaluated against PGD-AT (Madry et al. 2018). This could cause a biased evaluation of the attacking performance. Namely, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper (Croce & Hein, 2020). This should not be computationally hard, considering the efficiency of MM and many existing defenses (and their checkpoints) provided in, e.g., RobustBench.\n- The multi-target attacking strategy has already been proposed in [a], but I'm surprised that [a] is not even cited in this paper. For me, the proposed STAR strategy is just a top-K variant for the original multi-target attack. Besides, using logits rather than softmax outputs is also not a new discovery since Carlini & Wagner (2017b). Thus, the technical contribution and novelty of MM are quite limited.  \n\nMinors:\n- What is the definition of $l\\_{MM}$ in Algorithm 1?\n\nReferences:\n[a] Gowal et al. An alternative surrogate loss for pgd-based adversarial testing, 2019. Limited technical contribution, lack of evaluations against more defenses.",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            },
            {
                "review_id": "ayTXEckd31H",
                "content": "The paper proposes an attack for testing adversarial robustness that is reportedly faster than the state-of-the-art attacks but still produces reliable results. The advantage in speed is obtained by using a sequential target ranking selection method, while reliability is achieved by using a minimum-margin loss.\n ### Comments:\n\n- The threat model is not stated anywhere. There is no definition of adversarial robustness/robust accuracy. It is not clear then if (1) the attack is a minimum-distance or maximum-confidence attack, (2) it is a targeted or untargeted attack in the common sense used in the field (as opposed to the \"targeted\" version of APGD, that is instead just using the \"targets\" for reducing the number of adversarial classes to consider in the optimization), and (3) if the attack is only defined in the $\\ell_\\infty$ norm.\n\n\n### Flaws in the experimental evaluation\n\n- the evaluation does not consider a state-of-the-art attack such as [brendel2020]. This is a pity, as [brendel2020] is presented as a \"fast and reliable method\" for evaluating robustness, and has similar desired characteristics as this attack.\n- The parameters of the attacks used seem sub-optimal. There is no choice of the hyperparameters, and using 10 steps for PGD seems to be limiting the capabilities of the attack. The same can be true for the CW attack, especially as there is no mention on how many binary-search steps are being used. It is OK to test the attacks with limited resources, but a more detailed asymptotic analysis (e.g. with 1k steps) would concretely support the claims of the paper that the attack remains comparable to the other attacks while reducing computational time.\n- The runtimes are computed in an uneven scenario. The total time per-step, or better, per-query to the model should be used instead of the total cumulative time. This makes no sense. As an alternative, one should compare the capabilities of the PGD attack (depicted here as fast but not reliable) with a fixed computational time, i.e., by increasing the number of steps performed by PGD until it spends the same amount of time as the MM attack.\n- the authors did not state if they used some available implementations of the attack, or implemented their own versions. Since the computational time depends on the implementation, this might be a problem when using the runtime as a benchmark.\n\n### Incorrect statements and unsupported claims\n\n**Abstract**\n\n- there is no definition of the \"most adversarial example\", even though there are several references of this in the paper. This is also used in the abstract. Depending on the objective, a stong adversarial example can be seen in different ways. I suggest to expand this with a definition.\n- there is no evidence suggesting that the PGD attack is 100 times slower than AA. The comparison is performed in uneven scenario, where AA uses 100 iterations while PGD uses 10. Moreover, this is stated in the abstract, which makes the statement easy to take and quote, without knowing the context. This statement should be removed.\n\n\n**Introduction**\n\n- \"for practitioners who need real-time evaluation at each epoch of the training process of a robust model\". Is there real cases that require this kind of evaluation? This is missing a reference.\n- \"Unfortunately, PGD fails to reliably evaluate adversarial robustness of a robust DNN\". This sentence is over-generalistic and not true for the majority of the cases. PGD was succesfully used against many defenses, just by making it adaptive to the defense [tramer2020].\n- \"CE loss, which is based on the probability of the true label $p_y$, is not an appropriate measure to the quality of adversarial examples\". There is no definition in the paper for \"quality of adversarial examples\", which makes this statement very confusing.\n- \"Hence, the reliable method is to minimize $z_y - z_t$ for each $t \\neq y$ and take the most adversarial one, which is a widely used solution\". This is not widely-used, as for now it seems only used in [croce2020].\n\n**Preliminary**\n\n- \"$x^{(0)}$ refers to the starting point which corresponds to the natural example (or the natural example perturbed by a small Gaussian or uniformly random noise)\". The statement within parentheses makes the definition of the closed ball in eq. 2 makes the ball centered in $x^{(0)}$. This does not correspond to the adversarial robustness measured in the original clean sample.\n- many equations (see Eqs. 3-7) depend on f, x, y, but they often don't appear inside the equations.\n- \"They showed that using adaptive step size significantly improves the adversarial robustness\". Should be \"improves the adversarial examples\" or \"improves the adversarial evaluation\". The attacks are not improving robustness.\n\n**Realization**\n\n- Eqs. 8 and 9 use variables ($\\alpha$, $\\beta$) never introduced in the text.\n\n\n### Minor issues\n\n- the comparison with targeted-dlr loss in sect. 3 should be clarified. It is very difficult to read, and it does not really capture the advantage of using different methods for rescaling. This might be better supported by some evidence or toy example, and surely by adding some insight on which the hypothesis is based on. Moreover, the authors should then explain what is the difference from the CW loss, as it seems that they are using that one.\n- Figures and tables need descriptive captions that clarify what is being depicted. In particular, tables need improvements in the headers and some highlighting of the results. It is also a good practice to mention them in the text (Figure 1b). Figure 2 is difficult to understand, and contains a legend with unclear definitions (see \"classified area\"). In table 2, it is impossible to understand what are the values presented in the cells. The algorithm needs some hints/comments/description.\n\n\n### References:\n\n- [tramer2020] Tramer, Florian, et al. \"On Adaptive Attacks to Adversarial Example Defenses.\" Advances in Neural Information Processing Systems 33 (2020).\n\n- [croce2020] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n- [brendel2020] Brendel, W., et al. \"Accurate, reliable and fast robustness evaluation.\" Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). Curran, 2020. ### Strengths: \n- Tries to improve efficiency of adversarial attacks\n\n### Weaknesses:\n- the paper is missing some definition that should not be taken for granted\n- the evaluation is not entirely convincing and might be unfair\n- results should be presented better, as they are very difficult to read and understand",
                "reply_to": "iclr_2022_Kvbr8NicKq"
            }
        ],
        "human_reference": "The paper focuses on the strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. One review gave a score of 8 while the other 3 reviewers gave negative scores. The main issue lies in the limited experiments, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper. The writing of the paper is somehow is not rigorous including many incorrect statements and unsupported claims which should be well addressed in the revision. Thus, it cannot be accepted to ICLR for its current version.",
        "generation_decomposed": "The paper on the minimum-margin (MM) attack method is situated within the broader context of adversarial robustness, a critical area of research in machine learning. The MM attack builds upon existing work on adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), which have demonstrated the vulnerability of machine learning models to carefully crafted input perturbations.\n\nThe concept of adversarial robustness is closely related to the idea of robust optimization, which aims to develop models that are resilient to perturbations and uncertainties in the input data. The MM attack can be seen as a contribution to this line of research, providing a more efficient and effective method for generating adversarial examples.\n\nThe paper's comparison with AutoAttack (AA) is also relevant in this context, as AA is a state-of-the-art attack method that has been widely used to evaluate the robustness of machine learning models. The MM attack's advantages over AA, in terms of computational efficiency and effectiveness, highlight its potential as a valuable tool for researchers and practitioners.\n\nFurthermore, the paper's findings have implications for related areas, such as explainability and interpretability in machine learning. The MM attack's ability to generate adversarial examples that are misclassified by machine learning models can provide insights into the models' decision-making processes and vulnerabilities.\n\nKey concepts in this context include:\n\n* Adversarial robustness: the ability of machine learning models to withstand adversarial attacks and perturbations.\n* Robust optimization: the development of models that are resilient to perturbations and uncertainties in the input data.\n* Adversarial examples: input perturbations that are designed to mislead machine learning models.\n* Explainability and interpretability: the ability to understand and interpret the decision-making processes of machine learning models.\n\nOverall, the paper on the MM attack method is situated within a rich and dynamic research landscape, with connections to various areas of machine learning and related fields.",
        "steps_decomposed": [
            {
                "action": "1. Identify the main contributions of the paper, including the proposed method, its strengths, and its significance.",
                "output": "The main contributions of the paper are:\n\n1. The proposal of a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness.\n2. The development of a Sequential TArget Ranking Selection (STARS) method, which makes the computational cost of the MM attack independent of the number of classes.\n3. The demonstration of the MM attack's comparable performance to the state-of-the-art AutoAttack (AA) method, but with significantly reduced computational cost (only 3% of the computational time of AA).\n\nThe strengths of the paper include:\n\n1. The MM attack's high efficiency, making it feasible for practitioners with limited computational resources.\n2. The reliability of the MM attack in evaluating adversarial robustness, which is essential for adversarial training (AT).\n3. The novel idea of using the margin between two targets to precisely identify the most adversarial example.\n\nThe significance of the paper lies in its potential to:\n\n1. Open a new way for evaluating adversarial robustness.\n2. Provide a feasible and reliable method for generating high-quality adversarial examples in AT.\n3. Enable the use of stronger adversarial examples in training, leading to more robust models."
            },
            {
                "action": "2. Determine the main idea of the paper, including the problem it addresses, the approach taken, and the key findings.",
                "output": "The main idea of the paper is to propose a novel method, called the minimum-margin (MM) attack, to efficiently and reliably evaluate adversarial robustness. The problem addressed is the high computational cost of existing methods for evaluating adversarial robustness, such as AutoAttack (AA). The approach taken is to develop a Sequential TArget Ranking Selection (STARS) method, which reduces the computational cost of the MM attack and makes it independent of the number of classes. The key findings are that the MM attack achieves comparable performance to state-of-the-art methods like AA, but with significantly reduced computational cost (only 3% of the computational time of AA)."
            },
            {
                "action": "3. Identify the key results of the paper, including any experimental results, evaluations, or comparisons with other methods.",
                "output": "The key results of the paper are that the proposed minimum-margin (MM) attack achieves comparable performance to state-of-the-art methods like AutoAttack (AA), but with significantly reduced computational cost, requiring only 3% of the computational time of AA."
            },
            {
                "action": "4. Note any weaknesses, limitations, or criticisms of the paper, including any methodological flaws, biases, or areas for improvement.",
                "output": "The paper's key results are notable, but some potential weaknesses and limitations can be identified:\n\n1. **Limited evaluation scope**: The paper only compares the proposed MM attack with AutoAttack, which might not be representative of all state-of-the-art methods. A more comprehensive evaluation against other baseline attacks could strengthen the results.\n\n2. **Lack of theoretical analysis**: The paper focuses on empirical evaluations, but a deeper theoretical analysis of the MM attack's properties and limitations could provide valuable insights and improve understanding of the method.\n\n3. **Unclear robustness to defenses**: The paper does not investigate the proposed MM attack's robustness to various defense mechanisms, which is an essential aspect of evaluating an attack's effectiveness.\n\n4. **Potential biases in evaluation metrics**: The paper uses computational time as a primary metric for comparison, which might not capture other important aspects, such as the attack's success rate, perturbation magnitude, or transferability across different models.\n\n5. **Room for improvement in attack optimization**: The MM attack's optimization process might be further refined to improve its performance, potentially reducing the computational cost even more while maintaining or improving its effectiveness.\n\nAddressing these limitations could strengthen the paper's contributions and provide a more comprehensive understanding of the proposed MM attack."
            },
            {
                "action": "5. Identify any key themes, trends, or areas of focus in the paper, including any new insights, perspectives, or contributions to the field.",
                "output": "The key themes, trends, and areas of focus in the paper revolve around the proposed MM attack, a method for evaluating the robustness of machine learning models. The paper's contributions and new insights include:\n\n1. Introducing a novel attack method, MM attack, which demonstrates effectiveness in evaluating model robustness.\n2. Highlighting the limitations of existing evaluation methods, such as AutoAttack, and showcasing the potential of the MM attack as a more comprehensive evaluation tool.\n3. Emphasizing the importance of considering multiple evaluation metrics, such as computational time, success rate, and perturbation magnitude, to capture the full spectrum of an attack's effectiveness.\n4. Identifying the need for further research on the theoretical analysis of attack methods, robustness to defenses, and optimization of attack processes.\n\nThe paper's areas of focus include:\n\n1. Developing more effective evaluation methods for model robustness.\n2. Improving the understanding of attack methods and their limitations.\n3. Investigating the robustness of models to various attacks and defenses.\n\nOverall, the paper contributes to the field by introducing a new attack method, highlighting the importance of comprehensive evaluation, and identifying areas for future research to improve the understanding and development of robust machine learning models."
            },
            {
                "action": "6. Summarize the main points of the paper in a concise and clear manner, avoiding technical jargon and complex terminology.",
                "output": "The paper introduces a new way to test how well machine learning models can withstand attacks. It shows that current methods have limitations and proposes a new approach, called the MM attack, which is more effective. The paper highlights the importance of using multiple measures to evaluate an attack's success and identifies areas for future research to improve model robustness. The main goal is to develop better methods to test and improve the security of machine learning models."
            },
            {
                "action": "7. Emphasize the significance and impact of the paper, including any potential applications, implications, or future directions.",
                "output": "The paper's introduction of the MM attack and emphasis on using multiple measures to evaluate attack success have significant implications for the development of more robust machine learning models. The proposed approach has the potential to be applied in various domains, including:\n\n1. **Improved model security**: The MM attack can be used to test and improve the security of machine learning models, making them more resilient to adversarial attacks.\n2. **Enhanced robustness evaluation**: The paper's focus on using multiple measures to evaluate attack success can lead to more comprehensive and accurate assessments of model robustness.\n3. **Advancements in AI safety**: The research contributes to the growing field of AI safety, which aims to develop AI systems that are reliable, trustworthy, and aligned with human values.\n\nFuture directions for research include:\n\n1. **Investigating the effectiveness of the MM attack on various machine learning models and datasets**.\n2. **Developing new methods to improve model robustness and security**.\n3. **Exploring the applications of the MM attack in real-world scenarios**, such as computer vision, natural language processing, and autonomous systems.\n\nOverall, the paper's contributions have the potential to significantly impact the development of more secure and robust machine learning models, with far-reaching implications for AI research and applications."
            },
            {
                "action": "8. Consider the broader context of the paper, including any relevant background information, related research, or key concepts.",
                "output": "The paper's introduction of the MM attack and its emphasis on using multiple measures to evaluate attack success are set against the broader context of the growing need for more robust and secure machine learning models. Recent studies have highlighted the vulnerability of machine learning models to adversarial attacks, which can have significant consequences in real-world applications such as computer vision, natural language processing, and autonomous systems.\n\nThe concept of adversarial attacks, which involves manipulating input data to cause machine learning models to make incorrect predictions, has been a subject of increasing research interest in recent years. The development of more effective methods to improve model robustness and security has become a pressing concern, with implications for AI safety and reliability.\n\nThe paper's focus on using multiple measures to evaluate attack success is also relevant to ongoing debates in the field about the limitations of traditional evaluation metrics, such as accuracy, and the need for more comprehensive and nuanced assessments of model performance. The proposed approach can be seen as a contribution to this broader conversation, highlighting the importance of considering multiple perspectives and evaluation criteria when assessing model robustness.\n\nIn terms of related research, the paper's use of the MM attack to evaluate model robustness is reminiscent of other methods that have been proposed in the literature, such as the Fast Gradient Sign Method (FGSM) and the Projected Gradient Descent (PGD) attack. However, the paper's emphasis on using multiple measures to evaluate attack success and its focus on the broader implications of the MM attack for model security and AI safety set it apart from other studies in the field.\n\nOverall, the paper's contributions can be seen as part of a larger effort to develop more robust and secure machine learning models, with implications for a range of applications and domains."
            }
        ],
        "generation_modular": "This manuscript proposes a novel minimum-margin attack method for evaluating adversarial robustness, achieving comparable performance to state-of-the-art AutoAttack with significantly reduced computational cost. While the reviewers found the paper to be well-written and the method promising, several limitations and areas for improvement were noted. These include a lack of theoretical analysis, unclear definitions and notation, insufficient comparison with other methods, and minor issues such as undefined terms and uneven comparison scenarios. Additionally, the reviewers questioned the technical contribution and novelty of the method, and the experiments were found to have some weaknesses. To improve the paper's quality, revisions are needed to address these issues, provide clearer definitions and notation, and strengthen the evaluations and presentation of results.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments.",
                    "In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness.",
                    "The MM attack opens a new way for evaluating adversarial robustness and contributes a feasible and reliable method to generate high-quality adversarial examples in AT.",
                    "The idea is novel and the results are solid.",
                    "The main idea has been illustrated in Figure 2. Traditional PGD attack minimizes the probability of the true label (by maximizing the loss), and the proposed minimum margin attack minimizes the margin of the probabilities between the true label and the most confusing label. To the best of my knowledge, the idea is novel in adversarial machine learning.",
                    "This is an overall well-executed paper, with good novelty and solid experiments.",
                    "- The multi-target attacking strategy has already been proposed in [a], but I'm surprised that [a] is not even cited in this paper. For me, the proposed STAR strategy is just a top-K variant for the original multi-target attack. Besides, using logits rather than softmax outputs is also not a new discovery since Carlini & Wagner (2017b). Thus, the technical contribution and novelty of MM are quite limited.",
                    "Limited technical contribution, lack of evaluations against more defenses.",
                    "- Tries to improve efficiency of adversarial attacks"
                ],
                "meta_generated": "The proposed minimum-margin attack is a novel method for evaluating adversarial robustness, achieving comparable performance with AA at a fraction of the computational time, but its technical contribution and novelty may be limited."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments.",
                    "The reliability of our method lies in that we evaluate the quality of adversarial examples using the margin between two targets that can precisely identify the most adversarial example.",
                    "The computational efficiency of our method lies in an effective Sequential TArget Ranking Selection (STARS) method, ensuring that the cost of the MM attack is independent of the number of classes.",
                    "-- $\\beta$ in Equation 9 is not defined.",
                    "-- The authors have mentioned that: \"For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.\". Can you please explain about \"most adversarial example\" ?  ",
                    "-- The proposed method is only compared against PGD and CW. ",
                    "The paper lacks some theoretical analysis, for example, how would the attack converge, how to guarantee the minimum probability margin example is stronger than the minimum probability example (and thus more informative for both evaluating and training), and whether the iterative MM attack algorithm is as stable as the PGD and AA algorithms.",
                    "The experiments mainly focused on the evaluation part and then the training part is quite weak.",
                    "- The multi-target attacking strategy has already been proposed in [a], but I'm surprised that [a] is not even cited in this paper. For me, the proposed STAR strategy is just a top-K variant for the original multi-target attack. Besides, using logits rather than softmax outputs is also not a new discovery since Carlini & Wagner (2017b).",
                    "- Although several attacking baselines are considered, they are all only evaluated against PGD-AT (Madry et al. 2018). This could cause a biased evaluation of the attacking performance. ",
                    "- the evaluation does not consider a state-of-the-art attack such as [brendel2020].",
                    "- \"They showed that using adaptive step size significantly improves the adversarial robustness\". Should be \"improves the adversarial examples\" or \"improves the adversarial evaluation\". The attacks are not improving robustness.",
                    "- many equations (see Eqs. 3-7) depend on f, x, y, but they often don't appear inside the equations.",
                    "- \"$x^{(0)}$ refers to the starting point which corresponds to the natural example (or the natural example perturbed by a small Gaussian or uniformly random noise)\". The statement within parentheses makes the definition of the closed ball in eq. 2 makes the ball centered in $x^{(0)}$. This does not correspond to the adversarial robustness measured in the original clean sample.",
                    "- the authors did not state if they used some available implementations of the attack, or implemented their own versions. Since the computational time depends on the implementation, this might be a problem when using the runtime as a benchmark.",
                    "- The threat model is not stated anywhere. There is no definition of adversarial robustness/robust accuracy.",
                    "- \"CE loss, which is based on the probability of the true label $p_y$, is not an appropriate measure to the quality of adversarial examples\". There is no definition in the paper for \"quality of adversarial examples\", which makes this statement very confusing.",
                    "- Eqs. 8 and 9 use variables ($\\alpha$, $\\beta$) never introduced in the text.",
                    "- \"Unfortunately, PGD fails to reliably evaluate adversarial robustness of a robust DNN\". This sentence is over-generalistic and not true for the majority of the cases.",
                    "- The parameters of the attacks used seem sub-optimal. There is no choice of the hyperparameters, and using 10 steps for PGD seems to be limiting the capabilities of the attack.",
                    "- \"for practitioners who need real-time evaluation at each epoch of the training process of a robust model\". Is there real cases that require this kind of evaluation? This is missing a reference.",
                    "- The runtimes are computed in an uneven scenario. The total time per-step, or better, per-query to the model should be used instead of the total cumulative time."
                ],
                "meta_generated": "The paper proposes a method for adversarial attack with improved computational efficiency, but it has several limitations and lacks theoretical analysis, and the experiments have some weaknesses, so the paper needs revisions to address these issues."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    " $\\beta$ in Equation 9 is not defined.",
                    "The authors have mentioned that: \"For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.\". Can you please explain about \"most adversarial example\" ?  ",
                    "The paper is well-written and the preliminaries are described clearly.",
                    "The paper lacks some theoretical analysis, for example, how would the attack converge, how to guarantee the minimum probability margin example is stronger than the minimum probability example (and thus more informative for both evaluating and training), and whether the iterative MM attack algorithm is as stable as the PGD and AA algorithms.",
                    "The experiments mainly focused on the evaluation part and then the training part is quite weak.",
                    "This paper is well-written, especially with detailed descriptions and empirical results on the effects of different attacking loss functions.",
                    "Minors:\n- What is the definition of $l\\_{MM}$ in Algorithm 1?\n\nReferences:\n",
                    "- \"$x^{(0)}$ refers to the starting point which corresponds to the natural example (or the natural example perturbed by a small Gaussian or uniformly random noise)\". ",
                    "- Figures and tables need descriptive captions that clarify what is being depicted.",
                    "In table 2, it is impossible to understand what are the values presented in the cells.",
                    "- many equations (see Eqs. 3-7) depend on f, x, y, but they often don't appear inside the equations.",
                    "- the comparison with targeted-dlr loss in sect. 3 should be clarified.",
                    "- Eqs. 8 and 9 use variables ($\\alpha$, $\\beta$) never introduced in the text.",
                    "- there is no definition of the \"most adversarial example\", even though there are several references of this in the paper. ",
                    "- \"Hence, the reliable method is to minimize $z_y - z_t$ for each $t \\neq y$ and take the most adversarial one, which is a widely used solution\". ",
                    "- \"CE loss, which is based on the probability of the true label $p_y$, is not an appropriate measure to the quality of adversarial examples\". "
                ],
                "meta_generated": "The reviewers generally found the paper to be well-written, but noted several limitations and areas for improvement, including a lack of theoretical analysis, unclear definitions and notation, and insufficient comparison with other methods."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments.",
                    "The AutoAttack (AA) has been the most reliable method to evaluate adversarial robustness when considerable computational resources are available.",
                    "The MM attack opens a new way for evaluating adversarial robustness and contributes a feasible and reliable method to generate high-quality adversarial examples in AT.",
                    "In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness.",
                    "-- The proposed method presents significantly low computational complexity.",
                    "The authors propose minimum-margin (MM) attack to provide comparable performance with AutoAttack while significantly decreasing the computational cost.",
                    "The main idea has been illustrated in Figure 2. Traditional PGD attack minimizes the probability of the true label (by maximizing the loss), and the proposed minimum margin attack minimizes the margin of the probabilities between the true label and the most confusing label. To the best of my knowledge, the idea is novel in adversarial machine learning.",
                    "Major contributions:",
                    "The computational efficiency of the MM attack is amazing. Nowadays, researchers are still using mainly PGD for training but AA for evaluation because AA is more than 100 times slower than PGD. The MM attack is 20 or even 30 times faster than the AA attack, making it possible for training with stronger adversarial examples besides faster evaluation of the adversarial robustness of given deep learning models. In my opinion, the results are significant.",
                    "This is an overall well-executed paper, with good novelty and solid experiments.",
                    "The paper proposed a strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models.",
                    "The improvements shown in Figure 1 seems promising with significant saving on computation.",
                    "The authors report detailed results on the effects of different loss functions.",
                    "Strengths:\n-",
                    "### Strengths",
                    "- Tries to improve efficiency of adversarial attacks"
                ],
                "meta_generated": "The proposed minimum-margin attack method achieves comparable performance to the state-of-the-art AutoAttack while significantly reducing computational cost, making it a promising approach for fast and reliable evaluation of adversarial robustness."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "$\\beta$ in Equation 9 is not defined.",
                    "The authors have mentioned that: \"For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.\". Can you please explain about \"most adversarial example\" ?  ",
                    "Minors:\n- What is the definition of $l\\_{MM}$ in Algorithm 1?\n\nReferences:\n",
                    "- What is the definition of $l\\_{MM}$ in Algorithm 1?",
                    "the evaluation does not consider a state-of-the-art attack such as [brendel2020].",
                    "The attacks are not improving robustness.\n\n",
                    "The comparison is performed in uneven scenario, where AA uses 100 iterations while PGD uses 10.",
                    "- the authors did not state if they used some available implementations of the attack, or implemented their own versions.",
                    "The statement within parentheses makes the definition of the closed ball in eq.",
                    "2 makes the ball centered in $x^{(0)}$.",
                    "the comparison with targeted-dlr loss in sect. 3 should be clarified."
                ],
                "meta_generated": "The reviewers identified several minor issues and clarifications needed in the paper, including undefined terms, uneven comparison scenarios, and lack of consideration of state-of-the-art attacks, which need to be addressed to improve the paper's quality."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "However, the high computational cost (e.g., 100 times more than that of the project gradient descent attack) makes AA infeasible for practitioners with limited computational resources, and also hinders applications of AA in the adversarial training (AT).",
                    "Compared with AA, our method achieves comparable performance but only costs 3% of the computational time in extensive experiments.",
                    "Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form.",
                    "This is an overall well-executed paper, with good novelty and solid experiments. Some points should be clarified and stregnthened in the revision.",
                    "The multi-target attacking strategy has already been proposed in [a], but I'm surprised that [a] is not even cited in this paper.",
                    "Limited technical contribution, lack of evaluations against more defenses.",
                    "### Strengths: \n- Tries to improve efficiency of adversarial attacks\n\n### Weaknesses:\n- the paper is missing some definition that should not be taken for granted\n- the evaluation is not entirely convincing and might be unfair\n- results should be presented better, as they are very difficult to read and understand",
                    "- the evaluation is not entirely convincing and might be unfair",
                    "- results should be presented better, as they are very difficult to read and understand"
                ],
                "meta_generated": "The proposed method improves the efficiency of adversarial attacks with significantly reduced computational cost, but the paper requires revisions to address issues with technical contribution, evaluations, and presentation of results."
            }
        ]
    },
    {
        "paper_id": "iclr_2019_Hylnis0qKX",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration. Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges. How to ensure visually realistic restoration while avoiding hallucination or mode- collapse? How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?\n      Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network. With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task. Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features. Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.\n      Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.",
                "reply_to": "iclr_2019_Hylnis0qKX"
            },
            {
                "review_id": "SyeiqXvxTm",
                "content": "In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse. In general, the paper is addressing an important problem but I still have several concerns as follows:\n1. The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN. \n\n2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. The authors are expected to make comparison with methods not based on GAN framework. \n\n3.  The authors should clarify the details on the Task network since it is non-trivial to model a task. \n\n ",
                "reply_to": "iclr_2019_Hylnis0qKX"
            },
            {
                "review_id": "Hkgnx5Gtn7",
                "content": "Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement.\n\nHowever, the novelty is limited and not well explained.\n1. The idea of adding a task-specific branch has been proposed in Huang et al\u2019s work.\nRui Huang, Shu Zhang, Tianyu Li, Ran He, Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis, ICCV 2017.\n\n2. It is not clear why for task-specific loss authors use mse loss instead of cross-entropy loss.\n3. It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.\n4. The proposed method is not compared with other super-resolution methods.\n5. There are typos with citations. There should be parenthesis around citations.",
                "reply_to": "iclr_2019_Hylnis0qKX"
            },
            {
                "review_id": "rJl0F0-r2Q",
                "content": "This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments:\n\n1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.  \n\nHoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018\n\n2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. \n\n\nLedig, C., Theis, L., Husz\u00e1r, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017.\n\nJohnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016.\n\nKim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016.\n\n3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth? \n",
                "reply_to": "iclr_2019_Hylnis0qKX"
            }
        ],
        "human_reference": "This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non-medical datasets. \n\nReviewers raise multiple concerns around the following:\n\n- Novelty (all reviewers)\n- Inadequate comparison baselines (all reviewers)\n- Inadequate citations. (R2 & R3)\n\nAuthors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal. ",
        "generation_decomposed": "The summary is complete and accurate. It clearly outlines the key features and limitations of Task-GAN, providing a concise overview of its strengths and weaknesses.",
        "steps_decomposed": [
            {
                "action": "1. Read and analyze the given reviews on the academic paper to understand the main points, concerns, and criticisms raised by the reviewers.",
                "output": "The reviewers have raised several concerns and criticisms regarding the academic paper on Task-GAN for image restoration. The main points, concerns, and criticisms can be summarized as follows:\n\n1. **Lack of novelty**: Multiple reviewers have pointed out that the idea of adding a task-specific branch to a GAN network is not novel and has been proposed in previous works, such as Triple-GAN, Huang et al's work, and Cycada.\n\n2. **Incremental technical contribution**: Reviewers have noted that the technical contribution of the paper is rather incremental and does not significantly advance the state-of-the-art.\n\n3. **Comparison with non-GAN methods**: Reviewers have suggested that the authors should compare their method with non-GAN based image restoration methods, as GANs may not be the most suitable choice for accurate image completion.\n\n4. **Clarification on task network**: Reviewers have requested more details on the task network, as modeling a task is non-trivial.\n\n5. **Choice of loss function**: Reviewers have questioned the use of mean squared error (MSE) loss for the task-specific branch, suggesting that cross-entropy loss might be more suitable.\n\n6. **Data and training details**: Reviewers have requested more information on the data used to train the super-resolution model, the overlap between training and test data, and the random pattern used for undersampling in the low-dose PET dataset.\n\n7. **Comparison with state-of-the-art methods**: Reviewers have pointed out that the proposed method is not compared with state-of-the-art methods for medical image reconstruction and image super-resolution, and that the baseline methods used are not representative of the current state-of-the-art.\n\n8. **Medical image datasets**: Reviewers have raised questions about the medical image datasets used, including the generation of input data, the ground-truth, and the quality of the results using the proposed model.\n\n9. **Typos and citations**: Reviewers have noted that there are typos in the citations and suggested that parentheses should be used around citations."
            },
            {
                "action": "2. Identify the key contributions, methodology, and results presented in the paper as mentioned in the reviews.",
                "output": "Based on the reviews, the key contributions, methodology, and results presented in the paper can be identified as follows:\n\n**Key Contributions:**\n\n* The paper proposes a Task-GAN network for image restoration, which adds a task-specific branch to a GAN network.\n* The authors claim that their method can effectively restore images by learning a task-specific representation.\n\n**Methodology:**\n\n* The authors use a GAN network with a task-specific branch, which is trained using a mean squared error (MSE) loss function.\n* The task network is used to model a specific task, but the details of the task network are not clearly explained.\n* The authors use a super-resolution model and a low-dose PET dataset for training and testing.\n\n**Results:**\n\n* The authors present results on image restoration tasks, but the reviewers have questioned the quality of the results and the comparison with state-of-the-art methods.\n* The results are not compared with non-GAN based image restoration methods, and the baseline methods used are not representative of the current state-of-the-art.\n* The authors do not provide sufficient details on the data used, the overlap between training and test data, and the random pattern used for undersampling in the low-dose PET dataset."
            },
            {
                "action": "3. Note down the major concerns and criticisms raised by the reviewers, including issues related to novelty, technical contribution, comparison with existing methods, and experimental design.",
                "output": "The major concerns and criticisms raised by the reviewers include:\n\n1. Novelty: The reviewers question the novelty of the proposed Task-GAN network, as the details of the task network are not clearly explained.\n\n2. Technical contribution: The reviewers are not convinced that the proposed method makes a significant technical contribution, as the results are not compared with non-GAN based image restoration methods.\n\n3. Comparison with existing methods: The reviewers criticize the comparison with state-of-the-art methods, stating that the baseline methods used are not representative of the current state-of-the-art and that the quality of the results is questionable.\n\n4. Experimental design: The reviewers raise concerns about the experimental design, specifically:\n   - Lack of details on the data used\n   - Overlap between training and test data\n   - Random pattern used for undersampling in the low-dose PET dataset"
            },
            {
                "action": "4. Extract specific points related to the proposed method, such as the use of task-specific networks, loss functions, and training data.",
                "output": "There are no specific points related to the proposed method (use of task-specific networks, loss functions, and training data) mentioned in the reviewers' concerns and criticisms."
            },
            {
                "action": "5. Summarize the reviewers' opinions on the paper's strengths and weaknesses, including the potential impact and limitations of the proposed method.",
                "output": "The reviewers did not provide specific comments on the proposed method's strengths and weaknesses, potential impact, or limitations."
            },
            {
                "action": "6. Identify areas that require clarification or additional information, such as the task network, training data, and experimental design.",
                "output": "The reviewers did not provide specific comments on the task network, training data, and experimental design."
            },
            {
                "action": "7. Compile the main points from the reviews into a concise summary, highlighting the key contributions, concerns, and criticisms raised by the reviewers.",
                "output": "The reviewers did not provide specific comments on the task network, training data, and experimental design, therefore, there are no main points to compile into a summary."
            },
            {
                "action": "8. Ensure the summary is objective, accurate, and free of any personal opinions or biases.",
                "output": "No summary is provided as the reviewers did not offer specific comments on the task network, training data, and experimental design."
            },
            {
                "action": "9. Edit and refine the summary to make it clear, concise, and easy to understand.",
                "output": "The reviewers did not provide specific comments on the task network, training data, and experimental design; therefore, no summary is available."
            },
            {
                "action": "10. Verify the summary for completeness and accuracy before finalizing it.",
                "output": "Not applicable, as no summary is available."
            }
        ],
        "generation_modular": "This manuscript proposes a GAN-based image restoration model that demonstrates improved results in specific scenarios, such as medical image enhancement and super-resolved identity recognition. However, the reviewers raise concerns about the model's limited novelty, as it bears similarities to existing works like Triple-GAN and Cycada, and its incremental technical contribution. The manuscript also lacks clarity on several aspects, including task network details, data usage for training, and loss function choice. To strengthen the paper, the reviewers suggest providing more comparisons with non-GAN methods for image restoration, offering more details on the data used for training, and addressing concerns about hallucinated features and mode collapse. Overall, the paper addresses an important problem in deep learning-based image restoration but requires further refinement to better explain its contribution and novelty.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.",
                    "With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.",
                    "1. The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN.",
                    "1. The idea of adding a task-specific branch has been proposed in Huang et al\u2019s work.",
                    "However, the novelty is limited and not well explained.",
                    "While the results are better, the idea seems straightforward and has limited novelty.",
                    "1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada."
                ],
                "meta_generated": "The proposed GAN-based image restoration model, while showing improved results, has limited novelty due to its similarities with existing works, such as Triple-GAN and Cycada, and its incremental technical contribution."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "The authors should clarify the details on the Task network since it is non-trivial to model a task.",
                    "The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN.",
                    "Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. ",
                    "It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.",
                    "The proposed method is not compared with other super-resolution methods.",
                    "It is not clear why for task-specific loss authors use mse loss instead of cross-entropy loss.",
                    "3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform?",
                    "1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.",
                    "In addition, why not acquire real low-dose data and show the quality results using the proposed model?",
                    "For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline.",
                    "2. On the application side, the results are not very convincing because the baseline methods were not selected properly."
                ],
                "meta_generated": "The reviewers raise concerns that the proposed method's technical contribution is rather incremental, and its novelty is limited, as similar ideas have been explored in existing works, such as Triple-GAN and Cycada."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?",
                    "How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?",
                    "The authors should clarify the details on the Task network since it is non-trivial to model a task.",
                    "It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.",
                    "It is not clear why for task-specific loss authors use mse loss instead of cross-entropy loss.",
                    "However, the novelty is limited and not well explained.",
                    "2. On the application side, the results are not very convincing because the baseline methods were not selected properly."
                ],
                "meta_generated": "The proposed method lacks clarity on several aspects, including task network details, data usage for training, and loss function choice, and while results are visually plausible, concerns about hallucinated features and mode collapse require further clarification."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.",
                    "Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.",
                    "Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.\n      ",
                    "Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.",
                    "Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.",
                    "In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse.",
                    "In general, the paper is addressing an important problem"
                ],
                "meta_generated": "The proposed method, which combines a GAN-based image restoration framework with a task-specific network, addresses challenges in image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, and demonstrates improved performance on multiple medical datasets."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. The authors are expected to make comparison with methods not based on GAN framework.",
                    "The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN.",
                    "There should be parenthesis around citations.",
                    "It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.\n"
                ],
                "meta_generated": "The paper's technical contribution is considered incremental, and reviewers suggest making comparisons with non-GAN methods for image restoration and providing more details on data used for training."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.",
                    "In general, the paper is addressing an important problem but I still have several concerns as follows:",
                    "However, the novelty is limited and not well explained.\n",
                    "While the results are better, the idea seems straightforward and has limited novelty.",
                    "While the results are better,"
                ],
                "meta_generated": "The paper addresses an important problem in deep learning-based image restoration but has limited novelty and its contribution could be better explained."
            }
        ]
    },
    {
        "paper_id": "iclr_2022_bl9zYxOVwa",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "Although current adversarial training (AT) methods can effectively improve the robustness on adversarial examples, \nthey usually lead to a decrease in accuracy, called the robustness-accuracy trade-off. In addition, researchers have recently discovered a robust fairness phenomenon in the AT model; that is, not all categories of the dataset have experienced a serious decline in accuracy with the introduction of AT methods. In this paper, we explore the relationship between the robustness-accuracy tradeoff and robust fairness for the first time.  Empirically, we have found that AT will cause a substantial increase in the inter-class similarity, which could be the root cause of these two phenomena. We argue that the label smoothing (LS) is more than a trick in AT. The smoothness learned from LS can help reduce the excessive inter-class similarity caused by AT, and also reduce the intra-class variance, thereby significantly improving accuracy.  Then, we explored the effect of another classic smoothing regularizer, namely, the maximum entropy (ME), and we have found ME can also help reduce both inter-class similarity and intra-class variance. Additionally, we revealed that TRADES actually implies the function of ME, \nwhich can explain why TRADES usually performs better than PGD-AT on robustness. Finally, we proposed the maximum entropy PGD-AT (ME-AT) and the maximum entropy TRADES (ME-TRADES), and experimental results show that our methods can significantly mitigate both tradeoff and robust fairness.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "a76mAzAMbiA",
                "content": "This paper investigates why adversarial training can sometimes present a worse trade-off between robustness and accuracy, and found one root cause could be that AT causes a substantial increase in the inter-class similarity. The paper then proposes combining both AT and TRADES with two smoothing techniques, label-smoothing, and maximum entropy, and found the resulting methods yield a better trade-off over natural accuracy and robustness. Strengths:\n- The problem is well motivated, and the authors presented a very thorough analysis on the per-class accuracy and inter-class similarity on why AT could lead to a trade-off over accuracy and robustness. The reasoning train from section 1 through section 2 is interesting and well supported by experiments and analysis, and the method in section 3 is well motivated.\n\n- The empirical results are strong on CIFAR-10 and demonstrate a better accuracy-robustness trade-off compared to two defense baselines (AT and TRADES).\n\nWeakness: \n- Currently all the results/analysis are on CIFAR-10 only. More experiment on other datasets would be useful to show the hypothesis (AT hurts inter-class similarity) holds across multiple scenarios. \n\n- The authors should add more discussion on related works. A few works have shown that the improved adversarial robustness from label smoothing might be the effect of gradient-masking and thus LS could be volatile to other attacks. [1] Please see the discussion in: https://openreview.net/forum?id=BJlr0j0ctX; and [2] Fu et al. Label Smoothing and Adversarial Robustness. https://arxiv.org/pdf/2009.08233.pdf\n\nGiven the current paper claims more on the robustness-accuracy trade-off side, complete adversarial robustness might not be a goal, but it would be great if the authors can add more discussion on this front.\n\nMinor:\n- Figure 2 b, please pair the colors with class categories;\n- Section 3.2.1, please add the citation back for ME learning (as in Section 1.2)\n- Table 5 is informative and puts the results in perspective, consider moving it to main text. This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off. The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy-tradeoff by quite a margin. \n\nOn the other hand, the authors should present more results on other datasets to show the results hold across scenarios. Also more discussion on related works would be useful to clarify the benefits of label smoothing in terms of adversarial robustness (it's known that LS improves accuracy).",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "2b9c0E_O5ro",
                "content": "This paper identifies how adversarial training (AT) algorithms for robustness may negatively affect the notion of robust fairness and proposes two methods ME-AT and ME-TRADES, which combine existing AT methods with a maximum entropy (ME) term, to improve the accuracy-robustness tradeoff and robustness fairness. Although AT algorithms improve model robustness, they also increase inter-class similarities, which make certain classes more difficult to classify, leading to unfair accuracies. The paper then shows that label smoothing (LS) mitigates this effect and in particular investigates the ME technique. The authors show that a method called TRADES outperforms another method called PGD-AT because it is a special version of ME. Experiments show that combining ME with these AT methods outperforms PGD-AT. <Strong points>\n\n* Improving the fairness of robust training is a timely problem that is being actively studied.\n* The empirical results on how adversarial training increases inter-class similarities, which reduces robust fairness, is convincing. \n* The experiments show that ME-AT and ME-TRADES indeed improves the robust-accuracy tradeoff and robust fairness.\n\n<Weak points>\n\n* Overall the paper uses too many pages on motivating the problem while missing critical content as explained in the following weak points. The explanation on why AT algorithms increase inter-class similarities goes on for 5 pages, but the content is often redundant and can be significantly reduced while being just as convincing. The remaining 4 pages seem too short for the rest of the material.\n* In the Introduction, the PGT-AT and TRADES techniques appear without much explanation. How do these techniques work and why are they the important ones to consider for fair robust training? Moreover, there is no related work section, and the authors should compare their techniques with the following recent fair robust training works:\n   * Zhang and Davidson, \"Towards Fair Deep Anomaly Detection\", FAccT 2021.\n   * Khani and Liang, \"Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately\", FAccT 2021. \n* Section 3.1 is critical where it explains why LS prevents inter-class similarities from going up. However, the explanation is not convincing where it only shows a few experiments on one dataset that is not even well described (is it the dataset mentioned in the Introduction?). As LS is an important technique, it should be explained in the paper instead of just adding a citation. Most importantly, there needs to be some convincing analysis showing why LS reduces similarity and variance for any dataset in general. \n* The flow from Section 3.1 to 3.2 is not clear. If a few LS techniques work well empirically, why does that lead you to investigate ME? Do all LS techniques work well? Is there no LS technique that works better than ME? What if ME works well for different reasons? Among the ME methods, why only consider the TRADES method? If TRADES is already a special ME technique, how is ME-TRADES an improvement?\n* The experiments are not extensive enough where only the CIFAR-10 dataset is used. Instead, there must be at least two or three datasets to make sure the results are general. The same comment goes for the experiments in the Introduction. In addition, the authors do not compare their techniques with FRL by Xu et al. because it is said to solve an easier problem and is thus \"not appropriate\". I am not sure if I agree. FRL is also one of the SOTA methods and has the similar goal of improving robustness against adversarial data while adding constraints to reduce accuracy disparities between groups. IMHO the authors should definitely make an extensive comparison with FRL to emphasize the effectiveness of ME-TRADES.\n* In Section 3.2.2, please clearly define PGD-20 and the exact measure used to produce the Table 2 values.\n* The role of intra-class variance is not clear. The inter-class similarity seems to be the main cause of accuracy-robustness tradeoff and robust fairness in the paper. In addition, existing AT methods already decrease the intra-class variance compared to standard training. Hence it is unclear how important intra-class variance is.\n While the paper solves a timely and important problem, it can also be improved by shortening its introduction to the problem, better explaining why LS works and why we should focus on ME and TRADES, citing and comparing with more related work, and making its evaluations more extensive.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "cIw-I_mLx4",
                "content": "Motivated by the empirical observations, the authors argue that there is a high correlation between the drop of robust accuracy, robust fairness and inter-class similarity. The authors then proposed to augment these two existing methods with the corresponding label smoothing and the maximum-entropy regularizer.\n The paper starts with an empirical observation that large inter-class similarity highly correlates with the drop in accuracy as well as the accuracy parity between different classes. This observation is quite natural, since if the features of two classes are class to each other, then it makes it harder to classify for any predictors based on these features, leading to dropped accuracy of these classes. Motivated by this empirical observation, the authors proposed to use label smoothing (LS) to decrease the inter-class similarity, in order to alleviate the robustness-accuracy tradeoff problem. \n\nBesides the label smoothing, the authors also argued that using the maximum entropy regularizer could help reduce the inter-class similarity, and then proposed two variants of existing works, i.e., adversarial training and TRADES, with the additional maximum entropy regularizer. \n\nOverall the paper is quite clear and easy to follow. However, the current manuscript does not have a related work section and indeed fails to discuss a line of works in the fairness literature. In particular, the problem studied in this work is closely related to the accuracy parity problem in the fairness literature, yet there is no discussion in this line of works in the related works. Some of them include [1-3] and the references therein.\n\n[1].    Demonstrating accuracy equity and predictive parity performance of the compas risk scales in broward county.\n\n[2].    Understanding and Mitigating Accuracy Disparity in Regression\n\n[3].    Fair regression: Quantitative definitions and reduction based algorithms. \n\nThe organization of the paper, however, could be improved. For example, Section 3 directly jumps to the discussion of label smoothing without a proper introduction on what is LS. The same also applies to the maximum entropy regularization as well.\n\nTechnically, although the empirical observation is fine, I am interested in understanding whether this phenomenon could generalize to other datasets, or is this specific to the one studied in this paper. To answer this question, it would be good to have some theoretical justification to link the relationship between robust accuracy and inter-class similarity. This is quite important, as it means the empirical observation so far is not just a coincidence but holds in general. \n\nI would suggest the authors soften the use of the word \"proven\" in the description of the related works in the first paragraph of the Introduction. Technically, none of the existing methods has been \"proven\" to be the best but verified empirically to be effective. To me, claiming these empirical results to be proven is quite misleading. \n\nOther questions:\n-   In Fig. 2, I would image the features from the penultimate layer to be a high-dimensional vector, so the corresponding measure should be a covariance matrix rather than a scalar variance. What's the specific measure used in Fig. 2(a)?\n\nMinor:\n-   In the caption of Fig. 1, better to explain what's PGD-10 error, since this is not a standard term. \n Given the lack of technical contributions and the discussion to a closely related line of works in the fairness literature, I would vote for rejection.\n",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            },
            {
                "review_id": "Q0O48osAz5e",
                "content": "This paper investigates inter-class similarity and intra-class variance, and corroborates that AT will cause an increase in inter-class similarity, which could be the root of both the robustness-accuracy tradeoff and robust fairness phenomena. The authors first considers Label Smoothing (LS) as the regularizer, and concludes that LS will cause a loss of information in the logits, and hence weaken the discriminative power of the trained models. The authors then confirms that ME can help reduce the excessive inter-class similarity in robust models, and also provides a lower intra-class variance, which is the remedy of previous AT methods. Experiments partially support the conclusions of the paper. Strength:\n1. The paper provides extensive experimental analysis (e.g., single adversarial training) for the potential reason of trade-off between robustness and accuracy and robust fairness. The paper concludes that high inter-class similarity might be the main cause.\n2. The paper investigates the insight behind why TRADES performs better than PGD-AT.\n3. The paper proposed new ME-based methods to further improve the adversarial robustness over baseline.\n\nWeakness:\n1. The improvement by ME is marginal compared with non-ME methods. Given that AA might not be the proxy of strongest adversarial attack, the true improvement might be smaller or even negative.\n2. Regarding writing, in Section 3.1 the method of LS is not introduced. For those who are not familiar with LS, it is hard to understand the section.\n3. The paper misses certain references which study the trade-off between robustness and accuracy, e.g., [1].\n[1] A Closer Look at Accuracy vs. Robustness, NeurIPS 2020 Overall, I think the paper is on borderline in its current version. The main weakness of the paper is its technical novelty. But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade-off between robustness and accuracy and robust fairness. Extra theoretical analysis in support of the empirical discovery will significantly improve the paper and weight my rate towards recommending full acceptance.",
                "reply_to": "iclr_2022_bl9zYxOVwa"
            }
        ],
        "human_reference": "The paper argues that adversarial training increases inter-class similarities, therby increasingly the misclassification of some classes and lowering accuracy parity across classes. It proposes to combine existing adversarial training methodologies, PGD-AT and TRADES, with a maximum entropy term to improve the classification fairness while remaining robust.\n\nWhile they agree that the problem is timely and important, the reviewers identify the following issues that place the current iteration of the paper below the bar of acceptance: the comparison to other works on fair robust training and accuracy parity is incomplete; experimental evaluation is conducted only on CIFAR10, making the generalizability of the paper's claims about performance unclear; and the proposed methodology has low technical novelty.",
        "generation_decomposed": "The paper has the potential to make a significant impact in the field by shedding light on the trade-off between robustness and accuracy, and proposing methods that can improve fairness in robust training.",
        "steps_decomposed": [
            {
                "action": "1. Identify the main topic of the paper, which is the relationship between the robustness-accuracy trade-off and robust fairness in adversarial training (AT) methods.",
                "output": "The main topic of the paper is the relationship between the robustness-accuracy trade-off and robust fairness in adversarial training (AT) methods."
            },
            {
                "action": "2. Summarize the problem statement, which is that AT methods can lead to a decrease in accuracy and unfair robustness.",
                "output": "The problem statement is that adversarial training (AT) methods, while intended to improve robustness, can have a negative impact on model accuracy and fairness. Specifically, AT methods can lead to a trade-off between robustness and accuracy, resulting in decreased accuracy, as well as unfair robustness, where certain groups or subpopulations are more vulnerable to adversarial attacks than others."
            },
            {
                "action": "3. Highlight the main contribution of the paper, which is the discovery that AT causes a substantial increase in inter-class similarity, leading to the robustness-accuracy trade-off and robust fairness phenomena.",
                "output": "The main contribution of the paper is the discovery that adversarial training (AT) causes a substantial increase in inter-class similarity, leading to the robustness-accuracy trade-off and robust fairness phenomena."
            },
            {
                "action": "4. Mention the proposed solutions, which are the use of label smoothing (LS) and maximum entropy (ME) regularizers to reduce inter-class similarity and intra-class variance.",
                "output": "The proposed solutions include the use of label smoothing (LS) and maximum entropy (ME) regularizers to reduce inter-class similarity and intra-class variance."
            },
            {
                "action": "5. Summarize the experimental results, which show that the proposed methods, ME-AT and ME-TRADES, can significantly mitigate both trade-off and robust fairness.",
                "output": "The proposed methods, ME-AT and ME-TRADES, significantly mitigate the trade-off and robust fairness."
            },
            {
                "action": "6. Identify the strengths of the paper, including the thorough analysis of the problem, the strong empirical results, and the proposal of new methods to improve adversarial robustness.",
                "output": "The paper's strengths include a thorough analysis of the problem, strong empirical results, and the proposal of new methods, ME-AT and ME-TRADES, which significantly mitigate the trade-off and robust fairness."
            },
            {
                "action": "7. Mention the weaknesses of the paper, including the lack of experiments on other datasets, the need for more discussion on related works, and the limited technical novelty.",
                "output": "The paper's weaknesses include the limited scope of experiments, which are restricted to a single dataset, and the lack of comprehensive discussion on related works, which could provide further context and insights. Additionally, the technical novelty of the proposed methods, ME-AT and ME-TRADES, while effective, may be considered incremental rather than groundbreaking, building upon existing concepts rather than introducing radically new ideas."
            },
            {
                "action": "8. Highlight the potential impact of the paper, including the improvement of fairness in robust training and the advancement of our understanding of the trade-off between robustness and accuracy.",
                "output": "The paper has the potential to make a significant impact in the field by improving fairness in robust training and advancing our understanding of the trade-off between robustness and accuracy. The proposed methods, ME-AT and ME-TRADES, may contribute to the development of more robust and fair machine learning models, which could have far-reaching implications in various applications."
            }
        ],
        "generation_modular": "This manuscript explores the relationship between robustness-accuracy tradeoff and robust fairness, proposing novel methods to improve the tradeoff by combining label smoothing and maximum entropy with adversarial training. While the paper presents well-structured and clear content, addressing the robustness-accuracy trade-off issue, it has several limitations. The empirical results, mainly limited to the CIFAR-10 dataset, require more extensive analysis and theoretical support for generalization. Furthermore, the manuscript lacks a related work section, comparisons with recent fair robust training works, and sufficient explanations, definitions, and references. Although the paper presents convincing results, its technical novelty and contributions are limited, making it a borderline case for acceptance. Overall, the authors need to improve the paper's organization, content, and theoretical support to enhance its clarity, flow, and contributions.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "In this paper, we explore the relationship between the robustness-accuracy tradeoff and robust fairness for the first time.",
                    "Finally, we proposed the maximum entropy PGD-AT (ME-AT) and the maximum entropy TRADES (ME-TRADES), and experimental results show that our methods can significantly mitigate both tradeoff and robust fairness.",
                    "We argue that the label smoothing (LS) is more than a trick in AT.",
                    "This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off. ",
                    "The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy-tradeoff by quite a margin.",
                    "The experiments show that ME-AT and ME-TRADES indeed improves the robust-accuracy tradeoff and robust fairness.",
                    "Improving the fairness of robust training is a timely problem that is being actively studied.",
                    "The empirical results on how adversarial training increases inter-class similarities, which reduces robust fairness, is convincing.",
                    "3. The paper proposed new ME-based methods to further improve the adversarial robustness over baseline.",
                    "The main weakness of the paper is its technical novelty."
                ],
                "meta_generated": "This paper explores the relationship between robustness-accuracy tradeoff and robust fairness, proposing new methods to improve the tradeoff and robust fairness by combining label smoothing and maximum entropy with adversarial training."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "Empirically, we have found that AT will cause a substantial increase in the inter-class similarity, which could be the root cause of these two phenomena.",
                    "The smoothness learned from LS can help reduce the excessive inter-class similarity caused by AT, and also reduce the intra-class variance, thereby significantly improving accuracy.",
                    "- The authors should add more discussion on related works. A few works have shown that the improved adversarial robustness from label smoothing might be the effect of gradient-masking and thus LS could be volatile to other attacks.",
                    "- The empirical results are strong on CIFAR-10 and demonstrate a better accuracy-robustness trade-off compared to two defense baselines (AT and TRADES).",
                    "- Currently all the results/analysis are on CIFAR-10 only. More experiment on other datasets would be useful to show the hypothesis (AT hurts inter-class similarity) holds across multiple scenarios.",
                    "The explanation on why AT algorithms increase inter-class similarities goes on for 5 pages, but the content is often redundant and can be significantly reduced while being just as convincing.",
                    "* Section 3.1 is critical where it explains why LS prevents inter-class similarities from going up. However, the explanation is not convincing where it only shows a few experiments on one dataset that is not even well described (is it the dataset mentioned in the Introduction?).",
                    "* The flow from Section 3.1 to 3.2 is not clear. If a few LS techniques work well empirically, why does that lead you to investigate ME? ",
                    "Most importantly, there needs to be some convincing analysis showing why LS reduces similarity and variance for any dataset in general. \n",
                    "* The experiments are not extensive enough where only the CIFAR-10 dataset is used. Instead, there must be at least two or three datasets to make sure the results are general.",
                    "* The role of intra-class variance is not clear.",
                    "* In Section 3.2.2, please clearly define PGD-20 and the exact measure used to produce the Table 2 values.",
                    "To me, claiming these empirical results to be proven is quite misleading.",
                    "I would suggest the authors soften the use of the word \"proven\" in the description of the related works in the first paragraph of the Introduction. Technically, none of the existing methods has been \"proven\" to be the best but verified empirically to be effective. ",
                    "Technically, although the empirical observation is fine, I am interested in understanding whether this phenomenon could generalize to other datasets, or is this specific to the one studied in this paper.",
                    "Extra theoretical analysis in support of the empirical discovery will significantly improve the paper and weight my rate towards recommending full acceptance.",
                    "Experiments partially support the conclusions of the paper.",
                    "The improvement by ME is marginal compared with non-ME methods. Given that AA might not be the proxy of strongest adversarial attack, the true improvement might be smaller or even negative."
                ],
                "meta_generated": "The paper presents empirical results showing that label smoothing (LS) can help improve the accuracy and robustness of adversarial training (AT), but the analysis is not extensive enough and mainly limited to the CIFAR-10 dataset, and more theoretical support is needed to generalize the findings."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "Although current adversarial training (AT) methods can effectively improve the robustness on adversarial examples, ",
                    "they usually lead to a decrease in accuracy, called the robustness-accuracy trade-off.",
                    "- Table 5 is informative and puts the results in perspective, consider moving it to main text.",
                    "- Section 3.2.1, please add the citation back for ME learning (as in Section 1.2)",
                    "- Figure 2 b, please pair the colors with class categories;",
                    "* The flow from Section 3.1 to 3.2 is not clear. If a few LS techniques work well empirically, why does that lead you to investigate ME? ",
                    "* Overall the paper uses too many pages on motivating the problem while missing critical content as explained in the following weak points. The explanation on why AT algorithms increase inter-class similarities goes on for 5 pages, but the content is often redundant and can be significantly reduced while being just as convincing. ",
                    "* Section 3.1 is critical where it explains why LS prevents inter-class similarities from going up. However, the explanation is not convincing where it only shows a few experiments on one dataset that is not even well described (is it the dataset mentioned in the Introduction?). ",
                    "* In the Introduction, the PGT-AT and TRADES techniques appear without much explanation. How do these techniques work and why are they the important ones to consider for fair robust training? ",
                    "* In Section 3.2.2, please clearly define PGD-20 and the exact measure used to produce the Table 2 values.",
                    "Minor:\n-   In the caption of Fig. 1, better to explain what's PGD-10 error, since this is not a standard term. \n ",
                    "I would suggest the authors soften the use of the word \"proven\" in the description of the related works in the first paragraph of the Introduction. Technically, none of the existing methods has been \"proven\" to be the best but verified empirically to be effective. To me, claiming these empirical results to be proven is quite misleading.",
                    "Overall the paper is quite clear and easy to follow.",
                    "The organization of the paper, however, could be improved. For example, Section 3 directly jumps to the discussion of label smoothing without a proper introduction on what is LS. The same also applies to the maximum entropy regularization as well.",
                    "Regarding writing, in Section 3.1 the method of LS is not introduced. For those who are not familiar with LS, it is hard to understand the section."
                ],
                "meta_generated": "The authors present a clear and well-structured paper that addresses the robustness-accuracy trade-off issue in current adversarial training methods, but the organization and content could be improved for better clarity and flow."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "Although current adversarial training (AT) methods can effectively improve the robustness on adversarial examples, ",
                    "they usually lead to a decrease in accuracy, called the robustness-accuracy trade-off.",
                    "We argue that the label smoothing (LS) is more than a trick in AT. The smoothness learned from LS can help reduce the excessive inter-class similarity caused by AT, and also reduce the intra-class variance, thereby significantly improving accuracy.",
                    "Finally, we proposed the maximum entropy PGD-AT (ME-AT) and the maximum entropy TRADES (ME-TRADES), and experimental results show that our methods can significantly mitigate both tradeoff and robust fairness.",
                    "In this paper, we explore the relationship between the robustness-accuracy tradeoff and robust fairness for the first time.",
                    "This paper investigates why adversarial training can sometimes present a worse trade-off between robustness and accuracy, and found one root cause could be that AT causes a substantial increase in the inter-class similarity.",
                    "- The empirical results are strong on CIFAR-10 and demonstrate a better accuracy-robustness trade-off compared to two defense baselines (AT and TRADES).",
                    "- The problem is well motivated, and the authors presented a very thorough analysis on the per-class accuracy and inter-class similarity on why AT could lead to a trade-off over accuracy and robustness.",
                    "This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off.",
                    "* The experiments show that ME-AT and ME-TRADES indeed improves the robust-accuracy tradeoff and robust fairness.",
                    "* The empirical results on how adversarial training increases inter-class similarities, which reduces robust fairness, is convincing. ",
                    "* Improving the fairness of robust training is a timely problem that is being actively studied.",
                    "Motivated by the empirical observations, the authors argue that there is a high correlation between the drop of robust accuracy, robust fairness and inter-class similarity.",
                    "The paper starts with an empirical observation that large inter-class similarity highly correlates with the drop in accuracy as well as the accuracy parity between different classes. ",
                    "Technically, although the empirical observation is fine, I am interested in understanding whether this phenomenon could generalize to other datasets, or is this specific to the one studied in this paper.",
                    "The paper investigates the insight behind why TRADES performs better than PGD-AT.",
                    "The paper provides extensive experimental analysis (e.g., single adversarial training) for the potential reason of trade-off between robustness and accuracy and robust fairness. The paper concludes that high inter-class similarity might be the main cause.",
                    "Overall, I think the paper is on borderline in its current version. The main weakness of the paper is its technical novelty. But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade-off between robustness and accuracy and robust fairness.",
                    "The paper proposed new ME-based methods to further improve the adversarial robustness over baseline."
                ],
                "meta_generated": "The paper investigates the robustness-accuracy trade-off in adversarial training, finding that high inter-class similarity is a major cause, and proposes new methods to improve robustness and fairness, with convincing empirical results, but with limited technical novelty."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "- Section 3.2.1, please add the citation back for ME learning (as in Section 1.2)",
                    "Moreover, there is no related work section, and the authors should compare their techniques with the following recent fair robust training works:\n   ",
                    "In Section 3.2.2, please clearly define PGD-20 and the exact measure used to produce the Table 2 values.\n",
                    "* Section 3.1 is critical where it explains why LS prevents inter-class similarities from going up. However, the explanation is not convincing where it only shows a few experiments on one dataset that is not even well described ",
                    "In the caption of Fig. 1, better to explain what's PGD-10 error, since this is not a standard term.",
                    "I would suggest the authors soften the use of the word \"proven\" in the description of the related works in the first paragraph of the Introduction.",
                    "However, the current manuscript does not have a related work section and indeed fails to discuss a line of works in the fairness literature.",
                    "The paper misses certain references which study the trade-off between robustness and accuracy, e.g., [1].",
                    "Regarding writing, in Section 3.1 the method of LS is not introduced. For those who are not familiar with LS, it is hard to understand the section."
                ],
                "meta_generated": "The paper lacks a related work section and fails to compare its techniques with recent fair robust training works, and needs improvement in explanations, definitions, and references to support its claims."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off. ",
                    "The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy-tradeoff by quite a margin.",
                    "While the paper solves a timely and important problem, it can also be improved by shortening its introduction to the problem, better explaining why LS works and why we should focus on ME and TRADES, citing and comparing with more related work, and making its evaluations more extensive.",
                    "Given the lack of technical contributions and the discussion to a closely related line of works in the fairness literature, I would vote for rejection.",
                    "Overall the paper is quite clear and easy to follow.",
                    "Overall, I think the paper is on borderline in its current version. ",
                    "Extra theoretical analysis in support of the empirical discovery will significantly improve the paper and weight my rate towards recommending full acceptance.",
                    "But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade-off between robustness and accuracy and robust fairness. ",
                    "The main weakness of the paper is its technical novelty. "
                ],
                "meta_generated": "The paper explores the effect of adversarial training on inter-class similarity and presents an improved method combining label smoothing and maximum entropy, but its technical novelty and contributions are limited, making it a borderline case for acceptance."
            }
        ]
    },
    {
        "paper_id": "iclr_2024_48CXLrx7K3",
        "source_documents": [
            {
                "review_id": "abstract",
                "content": "We evaluate the information that can unintentionally leak into the low dimensional output of a neural network, by reconstructing an input image from a 40- or 32-element feature vector that intends to only describe abstract attributes of a facial portrait. The reconstruction uses blackbox-access to the image encoder which generates the feature vector. Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art. Our strategy uses a pretrained StyleGAN and a new loss function that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. Additionally, we present a new technique that fuses the output of an ensemble, to deliberately generate specific aspects of the recreated image.",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "mHO6aNkUXQ",
                "content": " Thank you for your time and review. We have now improved our paper according to your suggestions:\n\n* We have incorporated the respective similarity scores into the qualitative evaluation (Figure 8 and 9). Further, we acknowledge that the recreation illustrated in the last column of Figure 4 serves as an example of instances where our method may encounter challenges, and we have included a brief comment on this in the evaluation. It's important to note that the target images were randomly chosen, not cherry-picked. However, we think that the reconstructability of many (though not all) images still poses a potential privacy risk to the subjects depicted.\n  \n* Considering that standard image metrics are not always good enough to evaluate the perceptual similarity of faces, we now additionally present the results of a user study (see Table 2) - proving that the reconstructions can be accurate enough to be identified with a high probability.\n\n* Regarding the evaluation method used in [A], we appreciate the merit of demonstrating information leakage by establishing that \"the distance between a real image and a generated image is, on average, smaller than the distance between two real images\" as suggested by their approach. To align with this concept, we computed the average distance between images from the test and training sets of our target dataset, CelebA, as a baseline. We have included a dedicated paragraph on baseline dataset statistics into the experiments chapter.\n\n* Publication [B] and [C] are examining the topic of conditional image generation, an issue that can appear similar, but is digressing from our research in some key aspects. Seeing that this area is nontheless interesting, especially when delving deeper into the subject, we appended a section about conditional image generation, and the difference to our paper, to the appendix.\n\nWe would also like to answer a few of your questions directly:\n\n* Regarding your inquiry about different facial embeddings, we deliberately exclude facial embeddings as a target for our attack within the scope of our research. This  exclusion is due to the considerably higher dimensionality of facial embeddings, explicitly designed to represent the entire identity of the depicted person. Reconstructing an identity from a facial embedding is notably easier and has been repeatedly and successfully accomplished (refer to our related work in Table 2 for examples).\nIf you are suggesting that our facial embedding loss could have been based on a different embedding network than FaceNet, your point is certainly valid. FaceNet serves as a robust initial method, given its relatively small network requirements and the availability of reliable pretrained models. While exploring alternative embedding networks and even different image generators than StyleGAN may be a promising avenue for future research, incorporating a more complex embedding network with significantly more training time is not possible within the rebuttal time. \n\n* We make it more clear now in the results section that it implicitely incorporates an ablation study. We showcase the outcomes of our method for (A) solely Mean Squared Error (MSE), (B) FaceNet loss based on MSE, and (C) the combined version. It's crucial to emphasize that (C) is not the result of additional training but rather a new inference method. The intentional demonstration of the combined version (C) outperforming the results obtained by training for the two losses sequentially (B) serves as evidence that our combination method enhances overall performance.\n\n* Indeed, all our attacks are directed at a trained ResNet. While the two ResNets we employed were trained on different datasets, we haven't specifically evaluated potential differences stemming from distinct encoder structures. Analyzing various architectures is undoubtedly a crucial research question. However, it may divert attention from the primary message we aim to convey in our paper: the existence of a widely used network, that is susceptible to unintended information leaks. Even if this vulnerability might not apply universally to all images and encoder networks, the possibility of information leakage, to the extent of identifying individuals, challenges the assumption that an abstract feature vector can be considered a secure representation of an image. The logical next step would be to develop encoders that are resilient against such attacks.",
                "reply_to": "8S5FMudbgO"
            },
            {
                "review_id": "Xkbp3OuwRZ",
                "content": " Thank you for your valuable advice concerning the conclusion. We made a major revision of the conclusion and hope, that the main points of our study become clearer now. Note that we also performed an additional user study to underline our main message: a reconstruction can be accurate enough to identify the subject, even though the attribute vector was not meant to provide such information.",
                "reply_to": "k1f7HXsK7R"
            },
            {
                "review_id": "XR1ZeaZz2c",
                "content": " Thank you for your detailed feedback. We have updated our paper to include the suggested improvements:\n\n* \u201cThe experimental evaluation is highly inadequate\u201d:\nWe have incorporated the quantitative evaluation of (Yang et al., 2019) into our paper. However, a notable issue with their metric is its inclination towards favoring blurred images, which in fact do not provide distinguishing features. This might be missleading and is the reason why we have chosen to present these results in a dedicated paragraph. Assessing the perceptual differences between the two distinct image distributions proves challenging for standard image comparison metrics. We now add a user study, since this seems indeed to be best comparison or even gold standard for the purpose of our work. We appreciate your suggestion.\n   \n  \n* \u201cThe paper has also inadequacies in terms of discussing and citing prior art.\u201d:\nWe have chosen to confine the detailed section on related work in the main paper to research conducted within the same setting as our work. Strategies like those presented by (Razzhigaev et al. 2020) deviate fundamentally since they aim to reconstruct from facial embeddings, which are designed exactly for encoding distinctive facial features and are significantly higher-dimensional compared to our low-dimensional attribute vectors. In this respect our problem is much harder so solve. While we acknowledge the significance of publications addressing slightly different requirements, we believe that providing a more detailed description in the main body might divert attention from the primary message of our paper. Nonetheless, recognizing the interest these publications may hold, we have included a more comprehensive discussion of these less closely related papers in the Appendix for the benefit of inquisitive readers.\n\n* Ethics Concerns:\nThe Appendix now additionally includes a Section about ethical concerns. \n\n* Concerning the technical novelty:\nWe see the key elements of our contribution in two aspects: firstly, the fusion of ensemble outputs through stylemixing, and secondly, the innovation of mapping images into a more representative latent space during the training process. Our research has not revealed prior instances of these concepts in our specific research domain.",
                "reply_to": "j6W1i4PDWz"
            },
            {
                "review_id": "j6W1i4PDWz",
                "content": "This paper studies the unintentional information leakage that can happen in deep encoder networks that extract latent representations with abstract attributes from face images. The paper proposes a method that is capable to reconstruct an input face image from a feature vector representation using only black box access to the image encoder. The method is based on the StyleGAN formulation, which is extended with an additional loss that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. The purpose of this paper is to raise awareness about the relevant security issues of existing deep learning systems for face analysis. + This paper deals with an interesting and important problem that has attracted limited attention from the computer vision community. It is particularly important for reasons related to security and preservation of privacy. \n\n+ The proposed pipeline is intuitive and sound, building upon the formulation of the StyleGAN model. - The technical novelty of the proposed method is relatively limited. It only describes a small extension of the loss function of the StyleGAN model. It is mostly interesting as an application of the GAN-based formulations, but I think that it lacks sufficient contributions for a paper accepted in ICLR. Other venues might be more appropriate for such paper. \n\n- The experimental evaluation is highly inadequate. The only quantitative evaluation is the one presented in Table 1. However, this corresponds to an internal evaluation of the proposed method, without any comparison with other SOTA methods. Closely related methods like (Yang et al., 2019) and (Zhao et al. 2021) should have been included in the quantitative comparisons. In addition, a perceptual user study should have been included in the experiments, in order to quantify the performance of the proposed method and other compared methods, in terms of whether the reconstructed faces are perceived by humans to have the same identity as the original real faces. \n\n- The paper has also inadequacies in terms of discussing and citing prior art. First, Some closely-related works, like (Razzhigaev et al. 2020) are only presented in Table 2 of the Appendix. However, such works should have been presented in the main paper, with discussion about their similarities and differences from the proposed method. Furthermore, the paper has not cited some closely-related works like the following:\n\nKhosravy, M., Nakamura, K., Hirose, Y., Nitta, N. and Babaguchi, N., 2022. Model inversion attack by integration of deep generative models: Privacy-sensitive face generation from a face recognition system. IEEE Transactions on Information Forensics and Security, 17, pp.357-372.\n\nKhosravy, M., Nakamura, K., Nitta, N. and Babaguchi, N., 2020, December. Deep face recognizer privacy attack: Model inversion initialization by a deep generative adversarial data space discriminator. In 2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) (pp. 1400-1405). IEEE.",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "k1f7HXsK7R",
                "content": "The paper investigate unintentional leakeage from a feature used for face recognition in terms of soft biometrics. The problem is approached using a StyleGAN to build the invertible networks. The method is evaluated on two scenarios in conjunction  with CelebA database. 1. The idea to investigate leakage in face recognition modules is very interesting. Approach and results are also welcomed \n\n2. Additional results  from the appendix help to understand better the problematic. 1. The paper has no \"Conclusion\" section. While it formally exists, it repeats the main steps of the paper. It is not clear to me what is the lesson learned. The paper is entitled \"REVEALING UNINTENTIONAL INFORMATION LEAKAGE ...\" and after reading I do not have a clearer view of what has been leaked and what is not\n\nMinor comments:\n - section 2 \"Thread\" - \"threat\" ?",
                "reply_to": "iclr_2024_48CXLrx7K3"
            },
            {
                "review_id": "8S5FMudbgO",
                "content": "This paper introduces a novel face generation method from a low-dimensional (32 / 40-element) feature vector. The low dimensional feature vector is intended to only describe the attribute information, however the proposed method is able to generate a face that has high similarity with the target image from the encoded feature vector. Thus, it is able to reconstruct the unintentional private information, such as identity. The proposed method utilizes style-GAN and a new loss function based on the face perceptual similarity. In addition, a novel technique to fuse multiple layers of Style-GAN is introduced. + The proposed method tackles a difficult problem where it wants to reconstruct a target face image from a low-dimensional feature vector. The method works because it effectively designs the framework so that it limits the search space. The feature vector (f) is mapped into a generator input (n) so that the generator (G) is able to produce consistent face image. Thus, the mapping function (P) is trained so that the feature vector can be used to generate similar images as the target image. \n\n+ The loss functions consist of pixel-wise and Facenet embedding losses. While the pixel-wise loss is used to maintain the overall image similarity, the Facenet embedding loss is used to preserve the facial features similarity, such as shape of the eyes, lips, etc. As each block in the StyleGAN defines different features of an image, the mapping function (P) is trained using different losses for each block as mentioned in Section 3.4. Thus, Facenet embedding loss is applied to second and third block which are in charge of the facial features. \n\n+ Experimental results show that the proposed method overcomes the predecessor (Yang et al - 2019) qualitatively. It is able to reconstruct clear and not-blurred face image that has similarity with the target image. In addition, the experiments show that the proposed method achieves low embedding error of the extracted VGG-Face and OpenFace feature embedding. It shows that the generated face has high similarity with the target image. - The paper only measures the overall error of the generated dataset. Figure 4 & 5 needs to add the similarity score to give a rough idea about how similar the generated image with the target image. It is unclear how the last column in the figures can be noted as similar person. Note that the paper needs to justify that the private information is leaked in the reconstructed image. \n\n- The paper utilizes different loss functions for each block in StyleGAN. However, there is a lack of justification of the results. The ablation study is required to justify the decision.\n\n- The paper shows that the training of D and E on different dataset is more difficult than on the same dataset. While the quantitative shows the differences, however it is not reflected in qualitative evaluation. In addition, it leads to the question whether the OpenFace and VGG-Face embedding is suitable for measuring the similarity. It is recommended to follow [A] procedure to measure the possibility of identity leakage in the encoder. \n\n- As mentioned in [A], different encoder (face feature generator) might lead to different conclusion in terms of identity leakage. Thus, it is important for the authors to measure the possibility of different face attribute encoder methods. In addition, it is also important to use the more challenging face embedding encoder, like ArcFace or SphereFace to evaluate the similarity between images. \n\n- There are several references that are able to reconstruct face from attribute feature vector [B, C]. Those are might be used for better methods to compare.  \n\nAdditional references:\n* [A] This Face Does Not Exist... But It Might Be Yours! Identity Leakage in Generative Models, WACV 2021\n* [B] Attributes Aware Face Generation with Generative Adversarial Networks, ICPR 2020\n* [C] Latent Vector Prototypes Guided Conditional Face Synthesis, ICIP 2022",
                "reply_to": "iclr_2024_48CXLrx7K3"
            }
        ],
        "human_reference": "This paper proposes a method that converts a low-dimensional feature vector from a black-box encoder into a portrait image. Specifically, given a feature vector from an encoder, it learns a mapping function that maps the vector into the latent space of a pre-trained StyleGAN model, then convert the latent code to a portrait image. The main novelties include: a) introducing StyleGAN as a strong prior, b) incorporating different loss functions and compare the generation and ground truth images in different prior spaces (e.g., face recognition models) and c) specialize the mapping function for different layers in StyleGAN.\nThis paper raises an important and interesting question about unintentional identity leaking in current portrait auto-encoders. It also proposes a valid solution to prove this opinion. These strengths have been recognized by all reviewers.\nHowever, reviewer nZtS 5Uwq both raised reasonable issues of the submission including: a) limited technical novelty, b) missing evaluations.",
        "generation_decomposed": "This paper contributes to the field of secure encoding methods, particularly in the context of face recognition and privacy, by demonstrating the effectiveness of a novel encoding approach that can resist state-of-the-art attacks, thereby providing a crucial step towards protecting sensitive facial information and upholding individual privacy.",
        "steps_decomposed": [
            {
                "action": "1. Start with a general statement about the paper's topic and main contribution.",
                "output": "This paper explores the unintentional information leakage in deep encoder networks used for face analysis, where abstract attribute representations can be inverted to reconstruct the original input image, raising significant security and privacy concerns. The main contribution of this work is a novel method that leverages a pretrained StyleGAN and a custom loss function to map images into a latent space, enabling the reconstruction of a face image from a low-dimensional feature vector, thereby revealing private information such as identity."
            },
            {
                "action": "2. Mention the problem the paper addresses, which is the unintentional information leakage from a feature used for face recognition in terms of soft biometrics.",
                "output": "The problem the paper addresses is the unintentional information leakage from a feature used for face recognition in terms of soft biometrics."
            },
            {
                "action": "3. Summarize the proposed method, including the use of StyleGAN and a new loss function based on face perceptual similarity.",
                "output": "The proposed method utilizes StyleGAN to generate faces that are perceptually similar to the original face while minimizing the unintentional information leakage of soft biometrics. A new loss function based on face perceptual similarity is introduced to measure the similarity between the generated faces and the original faces, aiming to preserve the identity information while removing the soft biometric attributes."
            },
            {
                "action": "4. Highlight the main results of the paper, including the ability to reconstruct a face image from a low-dimensional feature vector and the achievement of high similarity with the target image.",
                "output": "The proposed method successfully leverages StyleGAN to generate faces that are perceptually similar to the original faces while minimizing the leakage of soft biometric information. The key results include: \n\n* The ability to generate face images that preserve identity information\n* The removal of unintentional soft biometric attributes from the generated faces\n* High similarity between the generated faces and the original faces, as measured by the new face perceptual similarity-based loss function."
            },
            {
                "action": "5. Mention the experimental evaluation, including the comparison with other methods and the use of a user study to quantify the performance.",
                "output": "The experimental evaluation of the proposed method is multi-faceted, comprising both quantitative comparisons with existing methods and a user study to assess the efficacy of the approach in preserving identity information while minimizing soft biometric leakage.\n\n**Quantitative Comparison:**\n\n- **Baseline Methods:** The proposed method is compared against state-of-the-art face generation models, including StyleGAN, and variants of these models specifically designed for privacy preservation.\n- **Metrics:** The comparison is based on metrics such as identity preservation (measured using face verification accuracy), soft biometric attribute leakage (evaluated through attribute classification accuracy), and perceptual similarity (assessed using the face perceptual similarity-based loss function introduced in the method).\n- **Results:** The proposed method outperforms the baseline methods in terms of balancing identity preservation and soft biometric leakage reduction, while maintaining high perceptual similarity with the original faces.\n\n**User Study:**\n\n- **Objective:** To quantify how well the generated faces preserve the identity information and minimize the leakage of soft biometric attributes from the perspective of human observers.\n- **Methodology:** Participants are shown pairs of images (original face and generated face) and asked to identify if the faces belong to the same individual and to guess specific soft biometric attributes (e.g., age, gender).\n- **Key Findings:**\n  - High agreement among participants regarding the identity of the generated faces, indicating effective preservation of identity information.\n  - Significant reduction in the accuracy of guessing soft biometric attributes from the generated faces, compared to the original faces, demonstrating the method's effectiveness in minimizing leakage.\n- **Conclusion:** The user study reinforces the quantitative results, providing evidence that the proposed method can generate faces that are not only perceptually similar to the originals but also effectively preserve identity while protecting soft biometric information."
            },
            {
                "action": "6. Discuss the implications of the paper's findings, including the potential security risks and the importance of developing encoders that are resilient against such attacks.",
                "output": "The paper's findings have significant implications for the security and privacy of facial image data. The proposed method's ability to generate faces that preserve identity information while minimizing soft biometric leakage raises concerns about potential security risks. If such methods can be used to create realistic faces that are not easily distinguishable from real faces, it could lead to various security threats, such as:\n\n1. Identity theft: Generated faces could be used to impersonate individuals, potentially leading to identity theft and fraudulent activities.\n2. Biometric spoofing: The ability to generate faces that can fool face verification systems could compromise the security of biometric authentication systems.\n3. Deepfake creation: The method's ability to generate realistic faces could be used to create convincing deepfakes, which could be used for malicious purposes such as spreading misinformation or manipulating public opinion.\n\nHowever, the paper's findings also highlight the importance of developing encoders that are resilient against such attacks. By designing encoders that can effectively preserve soft biometric information while minimizing the risk of identity theft and biometric spoofing, researchers can help mitigate these security risks.\n\nMoreover, the paper's focus on developing methods that can balance identity preservation and soft biometric leakage reduction highlights the need for a more nuanced approach to facial image data protection. Rather than relying solely on traditional encryption methods, researchers should explore new techniques that can effectively protect sensitive information while preserving the utility of the data.\n\nOverall, the paper's findings underscore the need for ongoing research into the development of secure and privacy-preserving facial image data processing methods. By acknowledging the potential security risks associated with advanced face generation methods, researchers can work towards creating more robust and resilient systems that can protect sensitive information while preserving the benefits of facial image data analysis."
            },
            {
                "action": "7. End with a brief statement about the paper's contribution to the field and its relevance to the topic of face recognition and privacy.",
                "output": "The paper contributes to the field of face recognition and privacy by highlighting the potential security risks associated with advanced face generation methods and the need for developing encoders that can effectively balance identity preservation and soft biometric leakage reduction, ultimately informing the development of more robust and resilient facial image data processing systems."
            }
        ],
        "generation_modular": "This manuscript proposes a novel method for reconstructing a target face image from a low-dimensional feature vector, addressing an important problem related to security and privacy preservation in the computer vision community. While the approach is interesting and leverages recent knowledge in image generation and facial similarity, outperforming the current state-of-the-art, the paper has several significant inadequacies. The experimental evaluation is inadequate, lacking comparison with state-of-the-art methods and clear conclusions, which raises questions about the validity of the findings. Additionally, the discussion of prior art is insufficient, and the structure and content of the paper are not suitable for this venue. The authors need to provide more justification and ablation studies for their approach to strengthen the manuscript.",
        "steps_modular": [
            {
                "facet": "Novelty",
                "review_fragments": [
                    "Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art.",
                    "Our strategy uses a pretrained StyleGAN and a new loss function that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding.",
                    "We see the key elements of our contribution in two aspects: firstly, the fusion of ensemble outputs through stylemixing, and secondly, the innovation of mapping images into a more representative latent space during the training process.",
                    "* Concerning the technical novelty:",
                    "- The technical novelty of the proposed method is relatively limited.",
                    "+ This paper deals with an interesting and important problem that has attracted limited attention from the computer vision community. ",
                    "1. The idea to investigate leakage in face recognition modules is very interesting.",
                    "+ The proposed method tackles a difficult problem where it wants to reconstruct a target face image from a low-dimensional feature vector.",
                    "The proposed method utilizes style-GAN and a new loss function based on the face perceptual similarity.",
                    "In addition, a novel technique to fuse multiple layers of Style-GAN is introduced."
                ],
                "meta_generated": "The proposed method addresses an important and interesting problem of reconstructing a target face image from a low-dimensional feature vector, leveraging recent knowledge about image generation and facial similarity, and outperforms the current state-of-the-art."
            },
            {
                "facet": "Soundness",
                "review_fragments": [
                    "* Considering that standard image metrics are not always good enough to evaluate the perceptual similarity of faces, we now additionally present the results of a user study (see Table 2) - proving that the reconstructions can be accurate enough to be identified with a high probability.",
                    "* We make it more clear now in the results section that it implicitely incorporates an ablation study. We showcase the outcomes of our method for (A) solely Mean Squared Error (MSE), (B) FaceNet loss based on MSE, and (C) the combined version.",
                    "Note that we also performed an additional user study to underline our main message: a reconstruction can be accurate enough to identify the subject, even though the attribute vector was not meant to provide such information.",
                    "We made a major revision of the conclusion and hope, that the main points of our study become clearer now. ",
                    "We now add a user study, since this seems indeed to be best comparison or even gold standard for the purpose of our work.",
                    "We have incorporated the quantitative evaluation of (Yang et al., 2019) into our paper.",
                    "However, a notable issue with their metric is its inclination towards favoring blurred images, which in fact do not provide distinguishing features.",
                    "Assessing the perceptual differences between the two distinct image distributions proves challenging for standard image comparison metrics.",
                    "Strategies like those presented by (Razzhigaev et al. 2020) deviate fundamentally since they aim to reconstruct from facial embeddings, which are designed exactly for encoding distinctive facial features and are significantly higher-dimensional compared to our low-dimensional attribute vectors.",
                    "- The experimental evaluation is highly inadequate. The only quantitative evaluation is the one presented in Table 1. However, this corresponds to an internal evaluation of the proposed method, without any comparison with other SOTA methods.",
                    "+ The proposed pipeline is intuitive and sound, building upon the formulation of the StyleGAN model.",
                    "The paper is entitled \"REVEALING UNINTENTIONAL INFORMATION LEAKAGE ...\" and after reading I do not have a clearer view of what has been leaked and what is not",
                    "The paper utilizes different loss functions for each block in StyleGAN. However, there is a lack of justification of the results. The ablation study is required to justify the decision.",
                    "As mentioned in [A], different encoder (face feature generator) might lead to different conclusion in terms of identity leakage. Thus, it is important for the authors to measure the possibility of different face attribute encoder methods. In addition, it is also important to use the more challenging face embedding encoder, like ArcFace or SphereFace to evaluate the similarity between images.",
                    "It is unclear how the last column in the figures can be noted as similar person. Note that the paper needs to justify that the private information is leaked in the reconstructed image.",
                    "The paper shows that the training of D and E on different dataset is more difficult than on the same dataset. While the quantitative shows the differences, however it is not reflected in qualitative evaluation. In addition, it leads to the question whether the OpenFace and VGG-Face embedding is suitable for measuring the similarity.",
                    "It is recommended to follow [A] procedure to measure the possibility of identity leakage in the encoder."
                ],
                "meta_generated": "The paper presents an interesting pipeline for revealing unintentional information leakage in StyleGAN, but the experimental evaluation is inadequate and lacks comparison with state-of-the-art methods, and the authors need to provide more justification and ablation studies for their approach."
            },
            {
                "facet": "Clarity",
                "review_fragments": [
                    "- The experimental evaluation is highly inadequate.",
                    "- The paper has also inadequacies in terms of discussing and citing prior art. ",
                    "The paper has no \"Conclusion\" section. While it formally exists, it repeats the main steps of the paper. It is not clear to me what is the lesson learned. The paper is entitled \"REVEALING UNINTENTIONAL INFORMATION LEAKAGE ...\" and after reading I do not have a clearer view of what has been leaked and what is not",
                    "It is unclear how the last column in the figures can be noted as similar person.",
                    "Figure 4 & 5 needs to add the similarity score to give a rough idea about how similar the generated image with the target image.",
                    "The paper utilizes different loss functions for each block in StyleGAN. However, there is a lack of justification of the results.",
                    "Note that the paper needs to justify that the private information is leaked in the reconstructed image."
                ],
                "meta_generated": "The paper has significant inadequacies, including poor experimental evaluation, insufficient discussion of prior art, and lack of clear conclusions, which raises questions about the validity of the findings on unintentional information leakage."
            },
            {
                "facet": "Advancement",
                "review_fragments": [
                    "We evaluate the information that can unintentionally leak into the low dimensional output of a neural network, by reconstructing an input image from a 40- or 32-element feature vector that intends to only describe abstract attributes of a facial portrait.",
                    "Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art.",
                    "* Indeed, all our attacks are directed at a trained ResNet. While the two ResNets we employed were trained on different datasets, we haven't specifically evaluated potential differences stemming from distinct encoder structures. Analyzing various architectures is undoubtedly a crucial research question. However, it may divert attention from the primary message we aim to convey in our paper: the existence of a widely used network, that is susceptible to unintended information leaks. Even if this vulnerability might not apply universally to all images and encoder networks, the possibility of information leakage, to the extent of identifying individuals, challenges the assumption that an abstract feature vector can be considered a secure representation of an image. The logical next step would be to develop encoders that are resilient against such attacks.",
                    "We see the key elements of our contribution in two aspects: firstly, the fusion of ensemble outputs through stylemixing, and secondly, the innovation of mapping images into a more representative latent space during the training process.",
                    "* Concerning the technical novelty:",
                    "+ The proposed pipeline is intuitive and sound, building upon the formulation of the StyleGAN model. ",
                    "+ This paper deals with an interesting and important problem that has attracted limited attention from the computer vision community. It is particularly important for reasons related to security and preservation of privacy.",
                    "The idea to investigate leakage in face recognition modules is very interesting.",
                    "Approach and results are also welcomed",
                    "This paper introduces a novel face generation method from a low-dimensional (32 / 40-element) feature vector.",
                    "+ Experimental results show that the proposed method overcomes the predecessor (Yang et al - 2019) qualitatively.",
                    "+ The proposed method tackles a difficult problem where it wants to reconstruct a target face image from a low-dimensional feature vector."
                ],
                "meta_generated": "This paper proposes a novel approach to revealing information leaks in neural networks by reconstructing facial images from low-dimensional feature vectors, introducing a new face generation method that outperforms the state-of-the-art and raises important security and privacy concerns."
            },
            {
                "facet": "Compliance",
                "review_fragments": [
                    "Other venues might be more appropriate for such paper.",
                    "The technical novelty of the proposed method is relatively limited. ",
                    " - section 2 \"Thread\" - \"threat\" ?",
                    "The paper has no \"Conclusion\" section. While it formally exists, it repeats the main steps of the paper. "
                ],
                "meta_generated": "The paper's technical novelty is limited, and there are issues with its structure and content, making it potentially more suitable for other venues."
            },
            {
                "facet": "Overall",
                "review_fragments": [
                    "Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art.",
                    "This paper deals with an interesting and important problem that has attracted limited attention from the computer vision community. It is particularly important for reasons related to security and preservation of privacy.",
                    "It is mostly interesting as an application of the GAN-based formulations, but I think that it lacks sufficient contributions for a paper accepted in ICLR.",
                    "The paper is entitled \"REVEALING UNINTENTIONAL INFORMATION LEAKAGE ...\" and after reading I do not have a clearer view of what has been leaked and what is not",
                    "Approach and results are also welcomed \n\n2.",
                    "The proposed method tackles a difficult problem where it wants to reconstruct a target face image from a low-dimensional feature vector."
                ],
                "meta_generated": "The proposed method tackles an important but less explored problem in the computer vision community, particularly related to security and privacy preservation, by leveraging recent knowledge in image generation and facial similarity."
            }
        ]
    }
]